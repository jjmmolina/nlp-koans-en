> ** Translation Note**: This file is currently in Spanish. English translation coming soon!
> For now, you can use a translator or refer to the code examples which are language-agnostic.
> Want to help translate? See [CONTRIBUTING.md](../../CONTRIBUTING.md)

---

# TeorÃ­a: Semantic Search & Vector Databases

## ğŸ“š Tabla de Contenidos
1. [IntroducciÃ³n a BÃºsqueda SemÃ¡ntica](#introducciÃ³n)
2. [Embeddings: Representaciones Vectoriales](#embeddings)
3. [MÃ©tricas de Similitud](#mÃ©tricas)
4. [Vector Databases](#vector-databases)
5. [Estrategias de BÃºsqueda](#estrategias)
6. [Reranking](#reranking)
7. [OptimizaciÃ³n y Escalado](#optimizaciÃ³n)
8. [Casos de Uso](#casos-uso)

---

## ğŸ” IntroducciÃ³n a BÃºsqueda SemÃ¡ntica {#introducciÃ³n}

### BÃºsqueda Tradicional vs SemÃ¡ntica

**BÃºsqueda Tradicional (Keyword-based):**
```
Query: "python programming"
MÃ©todo: Busca documentos que contengan exactamente "python" y "programming"

Encuentra:
âœ… "Learn Python programming in 10 days"
âŒ "Master the art of coding with Python" (no tiene "programming")
âŒ "Python tutorial for beginners" (no tiene "programming")
```

**BÃºsqueda SemÃ¡ntica (Meaning-based):**
```
Query: "python programming"
MÃ©todo: Entiende el SIGNIFICADO y busca contenido similar semÃ¡nticamente

Encuentra:
âœ… "Learn Python programming in 10 days"
âœ… "Master the art of coding with Python" (coding â‰ˆ programming)
âœ… "Python tutorial for beginners" (tutorial implica programming)
âœ… "Build applications with Python" (similar semÃ¡nticamente)
```

### Â¿Por quÃ© BÃºsqueda SemÃ¡ntica?

**Ventajas:**
- ğŸ¯ Entiende intenciÃ³n, no solo palabras exactas
- ğŸŒ Funciona con diferentes idiomas
- ğŸ“ Maneja sinÃ³nimos y parÃ¡frasis
- ğŸ§  Captura conceptos y relaciones
- âœ¨ Mejores resultados para queries naturales

**Desventajas:**
- ğŸ’° MÃ¡s costoso computacionalmente
- ğŸŒ MÃ¡s lento que bÃºsqueda de keywords
- ğŸ”§ Requiere embeddings pre-calculados
- ğŸ“Š Necesita modelos de embeddings

### EvoluciÃ³n de la BÃºsqueda

```
1990s: TF-IDF, BM25 (estadÃ­stica)
  â†“
2000s: PageRank, Link Analysis
  â†“
2013: Word2Vec (primeros embeddings Ãºtiles)
  â†“
2018: BERT (embeddings contextuales)
  â†“
2019: Sentence Transformers (embeddings de oraciones)
  â†“
2020: Dense Retrieval supera a BM25
  â†“
2023: Embeddings multimodales (texto + imagen)
  â†“
2024: Embeddings de alta dimensiÃ³n (OpenAI, Cohere)
       Vector databases en producciÃ³n
```

---

## ğŸ¯ Embeddings: Representaciones Vectoriales {#embeddings}

### Â¿QuÃ© es un Embedding?

Un **embedding** es una representaciÃ³n numÃ©rica (vector) de texto que captura su significado semÃ¡ntico.

```python
"perro"     â†’ [0.2, -0.5, 0.8, ..., 0.1]  # 384 dimensiones
"gato"      â†’ [0.3, -0.4, 0.7, ..., 0.2]  # Similar a "perro"
"ordenador" â†’ [-0.8, 0.2, -0.3, ..., 0.9] # Muy diferente
```

### Propiedades Clave

**1. Similitud SemÃ¡ntica**
```
Textos similares â†’ Vectores cercanos en el espacio
"rey" - "hombre" + "mujer" â‰ˆ "reina"
```

**2. Dimensionalidad**
- TÃ­picamente: 384, 768, 1536, 3072 dimensiones
- MÃ¡s dimensiones = mÃ¡s precisiÃ³n (pero mÃ¡s costo)

**3. NormalizaciÃ³n**
- Vectores suelen normalizarse (longitud = 1)
- Facilita cÃ¡lculo de similitud coseno

### Tipos de Embeddings

#### Word Embeddings (Nivel Palabra)

**Word2Vec (2013)**
```python
# Predice palabra siguiente o palabra en contexto
"The cat sat on the __"  â†’ probabilidades de palabras
```

**CaracterÃ­sticas:**
- Una representaciÃ³n por palabra
- No captura contexto
- "banco" (dinero) = "banco" (asiento)

**GloVe (2014)**
- Basado en co-ocurrencias globales
- Similar a Word2Vec en prÃ¡ctica

#### Contextualizados (Nivel OraciÃ³n)

**BERT (2018)**
```python
# Embeddings dependen del contexto
"Fui al banco a sacar dinero" â†’ embedding_banco_1
"Me sentÃ© en el banco del parque" â†’ embedding_banco_2
# embedding_banco_1 â‰  embedding_banco_2
```

**Sentence-BERT (2019)**
- Optimizado para embeddings de oraciones completas
- RÃ¡pido y eficiente
- Estado del arte para bÃºsqueda semÃ¡ntica

#### Modernos (2023-2024)

**OpenAI text-embedding-3**
```python
# Dimensiones configurables: 1536 o 3072
# MultilingÃ¼e
# Optimizado para bÃºsqueda
```

**Cohere Embed v3**
```python
# Soporta documentos largos (hasta 512 tokens)
# Embeddings comprimibles
```

### Modelos Populares

| Modelo | Proveedor | Dim | Costo | Calidad | Uso |
|--------|-----------|-----|-------|---------|-----|
| **text-embedding-3-small** | OpenAI | 1536 | ğŸ’° | â­â­â­â­ | ProducciÃ³n |
| **text-embedding-3-large** | OpenAI | 3072 | ğŸ’°ğŸ’° | â­â­â­â­â­ | MÃ¡xima calidad |
| **all-MiniLM-L6-v2** | HuggingFace | 384 | Free | â­â­â­ | Desarrollo |
| **all-mpnet-base-v2** | HuggingFace | 768 | Free | â­â­â­â­ | Balance |
| **multilingual-e5-large** | HuggingFace | 1024 | Free | â­â­â­â­ | MultilingÃ¼e |

### Generando Embeddings

**OpenAI API:**
```python
from openai import OpenAI

client = OpenAI()
response = client.embeddings.create(
    model="text-embedding-3-small",
    input="Texto a convertir en embedding"
)

embedding = response.data[0].embedding  # Lista de 1536 floats
```

**Sentence Transformers (local):**
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode("Texto a convertir")  # Array numpy de 384 floats
```

**Batch Processing:**
```python
# âœ… Eficiente: procesar mÃºltiples textos a la vez
texts = ["texto 1", "texto 2", ..., "texto N"]
embeddings = model.encode(texts, batch_size=32)
```

### Dimensionalidad

**Ventajas de mÃ¡s dimensiones:**
- âœ… Mayor precisiÃ³n
- âœ… Captura mÃ¡s matices semÃ¡nticos
- âœ… Mejor para tareas complejas

**Desventajas:**
- âŒ MÃ¡s memoria
- âŒ BÃºsqueda mÃ¡s lenta
- âŒ Mayor costo (APIs)

**ReducciÃ³n de Dimensionalidad:**
```python
from sklearn.decomposition import PCA

# Reducir de 1536 a 512 dimensiones
pca = PCA(n_components=512)
embeddings_reduced = pca.fit_transform(embeddings)
```

---

## ğŸ“ MÃ©tricas de Similitud {#mÃ©tricas}

### 1. Cosine Similarity (Similitud Coseno)

**Concepto:**
Mide el Ã¡ngulo entre dos vectores.

```
cosine_sim = (A Â· B) / (||A|| Ã— ||B||)

Donde:
A Â· B = producto punto
||A|| = magnitud (norma) de A
```

**Rango:**
- `-1`: Vectores opuestos
- `0`: Vectores perpendiculares (no relacionados)
- `+1`: Vectores idÃ©nticos

**En prÃ¡ctica con embeddings normalizados:**
- Rango tÃ­pico: `0.0` a `1.0`
- `> 0.8`: Muy similar
- `0.5 - 0.8`: Similar
- `< 0.5`: Poco similar

**ImplementaciÃ³n:**
```python
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# Embeddings normalizados
vec_a = np.array([0.5, 0.8, 0.2])
vec_b = np.array([0.6, 0.7, 0.3])

similarity = cosine_similarity([vec_a], [vec_b])[0][0]
# similarity â‰ˆ 0.95
```

**Ventajas:**
- âœ… No afectado por magnitud (solo direcciÃ³n)
- âœ… Funciona bien con embeddings normalizados
- âœ… Interpretable intuitivamente

### 2. Euclidean Distance (Distancia Euclidiana)

**Concepto:**
Distancia en lÃ­nea recta entre dos puntos.

```
euclidean_dist = âˆš(Î£(a_i - b_i)Â²)
```

**Rango:**
- `0`: Vectores idÃ©nticos
- `âˆ`: Sin lÃ­mite superior

**ImplementaciÃ³n:**
```python
from scipy.spatial.distance import euclidean

distance = euclidean(vec_a, vec_b)
```

**Nota:**
- Para embeddings normalizados, cosine similarity y euclidean distance estÃ¡n relacionados:
  `euclidean_dist = âˆš(2 - 2 * cosine_sim)`

### 3. Dot Product (Producto Punto)

**Concepto:**
Suma de productos elemento a elemento.

```
dot_product = Î£(a_i Ã— b_i)
```

**Para vectores normalizados:**
- Equivalente a cosine similarity
- MÃ¡s rÃ¡pido de calcular

**ImplementaciÃ³n:**
```python
dot_prod = np.dot(vec_a, vec_b)
```

### Comparativa

| MÃ©trica | Velocidad | Mejor Para | Sensible a Magnitud |
|---------|-----------|------------|---------------------|
| **Cosine** | âš¡âš¡ | Embeddings generales | No |
| **Euclidean** | âš¡âš¡âš¡ | Clustering | SÃ­ |
| **Dot Product** | âš¡âš¡âš¡âš¡ | Vectores normalizados | SÃ­ |

**RecomendaciÃ³n:**
- Para embeddings normalizados: **Dot Product** (mÃ¡s rÃ¡pido)
- Para embeddings sin normalizar: **Cosine Similarity**
- Para clustering: **Euclidean Distance**

---

## ğŸ—„ï¸ Vector Databases {#vector-databases}

### Â¿QuÃ© es una Vector Database?

Una base de datos optimizada para almacenar y buscar vectores (embeddings) eficientemente.

**Problema a Resolver:**
```python
# âŒ BÃºsqueda ingenua: O(n) - muy lento
def naive_search(query_embedding, all_embeddings):
    similarities = []
    for emb in all_embeddings:  # 1 millÃ³n de embeddings
        sim = cosine_similarity(query_embedding, emb)
        similarities.append(sim)
    return top_k(similarities, k=10)

# Tiempo: ~segundos para millones de vectores
```

```python
# âœ… Vector DB: O(log n) o mejor
def vector_db_search(query_embedding):
    results = vector_db.search(query_embedding, k=10)
    return results

# Tiempo: ~milisegundos para millones de vectores
```

### CaracterÃ­sticas Clave

**1. IndexaciÃ³n Eficiente**
- Algoritmos ANN (Approximate Nearest Neighbors)
- Trade-off: velocidad vs precisiÃ³n

**2. Escalabilidad**
- Manejo de millones/billones de vectores
- DistribuciÃ³n horizontal

**3. Filtrado de Metadata**
```python
# Buscar embeddings + filtrar por metadata
results = db.search(
    query_embedding,
    filter={"category": "technology", "date": ">2024-01-01"}
)
```

**4. Actualizaciones en Tiempo Real**
- AÃ±adir/eliminar vectores dinÃ¡micamente
- Re-indexaciÃ³n incremental

### Vector Databases Populares

#### 1. **ChromaDB**

**CaracterÃ­sticas:**
- ğŸ¯ DiseÃ±ada para simplicidad
- ğŸ’¾ In-memory o persistente
- ğŸ Python-first
- ğŸ†“ Open-source y gratuita

**CuÃ¡ndo Usar:**
- Prototipos y desarrollo
- Aplicaciones pequeÃ±as/medianas (< 1M vectores)
- Embeddings generados localmente

**Ejemplo:**
```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("my_docs")

# AÃ±adir documentos (embeddings automÃ¡ticos)
collection.add(
    documents=["Doc 1", "Doc 2"],
    ids=["id1", "id2"],
    metadatas=[{"source": "web"}, {"source": "pdf"}]
)

# Buscar
results = collection.query(
    query_texts=["consulta"],
    n_results=5
)
```

#### 2. **FAISS (Facebook AI Similarity Search)**

**CaracterÃ­sticas:**
- âš¡ Extremadamente rÃ¡pido
- ğŸ“ Algoritmos de investigaciÃ³n de Facebook
- ğŸ’» CPU y GPU support
- ğŸ†“ Open-source

**CuÃ¡ndo Usar:**
- Necesitas mÃ¡xima velocidad
- Tienes millones de vectores
- Quieres control total sobre Ã­ndices

**Tipos de Ãndices:**
```python
import faiss

# Flat (exacto pero lento para escala)
index = faiss.IndexFlatL2(dimension)

# IVF (particionado - balance velocidad/precisiÃ³n)
index = faiss.IndexIVFFlat(quantizer, dimension, nlist)

# HNSW (graph-based - rÃ¡pido y preciso)
index = faiss.IndexHNSWFlat(dimension, M)
```

#### 3. **Pinecone**

**CaracterÃ­sticas:**
- â˜ï¸ Totalmente cloud/SaaS
- ğŸ“ˆ Escalado automÃ¡tico
- ğŸ”Œ API simple
- ğŸ’° De pago

**CuÃ¡ndo Usar:**
- ProducciÃ³n sin infrastructure management
- Necesitas escalado automÃ¡tico
- Budget disponible

**Ejemplo:**
```python
import pinecone

pinecone.init(api_key="...")
index = pinecone.Index("my-index")

# Upsert vectores
index.upsert(vectors=[
    ("id1", [0.1, 0.2, ...], {"meta": "data"}),
    ("id2", [0.3, 0.4, ...], {"meta": "data"})
])

# Query
results = index.query(
    vector=[0.1, 0.2, ...],
    top_k=10,
    filter={"category": "tech"}
)
```

#### 4. **Weaviate**

**CaracterÃ­sticas:**
- ğŸ¯ GraphQL API
- ğŸ§  MÃ³dulos de AI integrados
- ğŸ”„ VectorizaciÃ³n automÃ¡tica
- ğŸ†“ Open-source + cloud

#### 5. **Qdrant**

**CaracterÃ­sticas:**
- ğŸ¦€ Escrito en Rust (rÃ¡pido)
- ğŸ¯ Filtrado avanzado
- ğŸ“Š Payloads ricos
- ğŸ†“ Open-source

#### 6. **Milvus**

**CaracterÃ­sticas:**
- ğŸ¢ Enterprise-grade
- ğŸ“ˆ Petabyte-scale
- ğŸ”Œ MÃºltiples Ã­ndices
- ğŸ†“ Open-source

### Comparativa

| Database | Facilidad | Velocidad | Escala | Costo | Mejor Para |
|----------|-----------|-----------|--------|-------|------------|
| **ChromaDB** | â­â­â­â­â­ | â­â­â­ | â­â­ | Free | Prototipos |
| **FAISS** | â­â­ | â­â­â­â­â­ | â­â­â­â­ | Free | Alta performance |
| **Pinecone** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | $$$ | ProducciÃ³n managed |
| **Weaviate** | â­â­â­ | â­â­â­â­ | â­â­â­â­ | Free/$ | GraphQL apps |
| **Qdrant** | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | Free/$ | Balance Ã³ptimo |
| **Milvus** | â­â­ | â­â­â­â­ | â­â­â­â­â­ | Free | Enterprise |

---

## ğŸ¯ Estrategias de BÃºsqueda {#estrategias}

### 1. Dense Retrieval (BÃºsqueda Densa)

Solo embeddings semÃ¡nticos.

```python
# 1. Generar embedding de consulta
query_emb = model.encode("Python programming")

# 2. Buscar en vector DB
results = vector_db.search(query_emb, k=10)
```

**Pros:**
- âœ… Entiende semÃ¡ntica
- âœ… Funciona con sinÃ³nimos
- âœ… Queries naturales

**Cons:**
- âŒ Puede fallar con nombres propios
- âŒ Fechas y nÃºmeros exactos problemÃ¡ticos

### 2. Sparse Retrieval (BÃºsqueda Dispersa)

MÃ©todos tradicionales: BM25, TF-IDF.

```python
from rank_bm25 import BM25Okapi

# 1. Tokenizar documentos
tokenized_docs = [doc.split() for doc in documents]

# 2. Crear Ã­ndice BM25
bm25 = BM25Okapi(tokenized_docs)

# 3. Buscar
scores = bm25.get_scores(query.split())
```

**Pros:**
- âœ… RÃ¡pido
- âœ… Bueno para matches exactos
- âœ… Nombres propios, IDs

**Cons:**
- âŒ No entiende semÃ¡ntica
- âŒ SinÃ³nimos son problema

### 3. Hybrid Search (BÃºsqueda HÃ­brida)

Combina dense + sparse.

```python
# 1. BÃºsqueda densa
dense_results = vector_db.search(query_emb, k=20)
dense_scores = {doc_id: score for doc_id, score in dense_results}

# 2. BÃºsqueda dispersa (BM25)
sparse_results = bm25_search(query, k=20)
sparse_scores = {doc_id: score for doc_id, score in sparse_results}

# 3. Combinar scores
final_scores = {}
for doc_id in set(dense_scores) | set(sparse_scores):
    dense = dense_scores.get(doc_id, 0)
    sparse = sparse_scores.get(doc_id, 0)
    
    # Weighted combination
    final_scores[doc_id] = alpha * dense + (1 - alpha) * sparse

# 4. Ordenar y retornar top-k
top_results = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)[:10]
```

**Ventajas:**
- âœ… Lo mejor de ambos mundos
- âœ… Robusto para diferentes tipos de queries

**ParÃ¡metro `alpha`:**
- `alpha = 1.0`: Solo dense (semÃ¡ntica)
- `alpha = 0.5`: Balance 50-50
- `alpha = 0.0`: Solo sparse (keywords)

### 4. MMR (Maximal Marginal Relevance)

Balancea relevancia y diversidad.

```python
def mmr(query_emb, doc_embeddings, lambda_param=0.5, k=10):
    selected = []
    candidates = list(range(len(doc_embeddings)))
    
    while len(selected) < k and candidates:
        mmr_scores = []
        
        for i in candidates:
            # Relevancia a query
            relevance = cosine_sim(query_emb, doc_embeddings[i])
            
            # Similitud a docs ya seleccionados
            if selected:
                max_sim = max([cosine_sim(doc_embeddings[i], doc_embeddings[j]) 
                              for j in selected])
            else:
                max_sim = 0
            
            # MMR = lambda * relevancia - (1-lambda) * redundancia
            mmr_score = lambda_param * relevance - (1 - lambda_param) * max_sim
            mmr_scores.append((i, mmr_score))
        
        # Seleccionar mejor
        best_idx, _ = max(mmr_scores, key=lambda x: x[1])
        selected.append(best_idx)
        candidates.remove(best_idx)
    
    return selected
```

**CuÃ¡ndo usar:**
- Quieres resultados diversos (no todos iguales)
- Exploratory search
- Recomendaciones

---

## ğŸ–ï¸ Reranking {#reranking}

### Concepto

**Two-Stage Retrieval:**

```
Stage 1: Retrieval RÃ¡pido (Bi-Encoder)
         â†“ 1000 documentos candidatos
Stage 2: Reranking Preciso (Cross-Encoder)
         â†“ Top 10 documentos finales
```

### Bi-Encoder vs Cross-Encoder

**Bi-Encoder:**
```python
# Procesa query y docs INDEPENDIENTEMENTE
query_emb = encoder(query)
doc_embs = [encoder(doc) for doc in docs]

# Compara embeddings
scores = [cosine_sim(query_emb, doc_emb) for doc_emb in doc_embs]
```

**Cross-Encoder:**
```python
# Procesa query Y doc JUNTOS
scores = [cross_encoder(query, doc) for doc in docs]
```

**ComparaciÃ³n:**

| Aspecto | Bi-Encoder | Cross-Encoder |
|---------|------------|---------------|
| **Velocidad** | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡ |
| **PrecisiÃ³n** | â­â­â­ | â­â­â­â­â­ |
| **Escalabilidad** | âœ… Millones | âŒ Solo top-K |
| **Uso** | Stage 1 | Stage 2 |

### ImplementaciÃ³n

```python
from sentence_transformers import SentenceTransformer, CrossEncoder

# Stage 1: Bi-encoder retrieval
bi_encoder = SentenceTransformer('all-MiniLM-L6-v2')
query_emb = bi_encoder.encode(query)
doc_embs = bi_encoder.encode(documents)

# Buscar top 100
similarities = cosine_similarity([query_emb], doc_embs)[0]
top_100_indices = np.argsort(similarities)[-100:][::-1]
top_100_docs = [documents[i] for i in top_100_indices]

# Stage 2: Cross-encoder reranking
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-12-v2')
pairs = [[query, doc] for doc in top_100_docs]
scores = cross_encoder.predict(pairs)

# Final top 10
top_10_indices = np.argsort(scores)[-10:][::-1]
final_results = [top_100_docs[i] for i in top_10_indices]
```

### Modelos de Cross-Encoder

| Modelo | TamaÃ±o | Velocidad | Calidad |
|--------|--------|-----------|---------|
| **ms-marco-TinyBERT-L-2-v2** | 17MB | âš¡âš¡âš¡âš¡ | â­â­â­ |
| **ms-marco-MiniLM-L-6-v2** | 80MB | âš¡âš¡âš¡ | â­â­â­â­ |
| **ms-marco-MiniLM-L-12-v2** | 130MB | âš¡âš¡ | â­â­â­â­â­ |

### Mejora de Performance

Reranking tÃ­picamente mejora:
- **NDCG@10**: +10-20%
- **MRR**: +15-25%
- **Precision@10**: +10-15%

---

## âš¡ OptimizaciÃ³n y Escalado {#optimizaciÃ³n}

### 1. IndexaciÃ³n

**HNSW (Hierarchical Navigable Small World)**

Algoritmo de grafos multi-capa.

```python
import faiss

# Crear Ã­ndice HNSW
M = 32  # NÃºmero de conexiones por nodo
index = faiss.IndexHNSWFlat(dimension, M)

# Configurar
index.hnsw.efConstruction = 40  # Calidad de construcciÃ³n
index.hnsw.efSearch = 16        # Calidad de bÃºsqueda

# AÃ±adir vectores
index.add(embeddings)
```

**ParÃ¡metros:**
- `M`: MÃ¡s alto = mÃ¡s preciso pero mÃ¡s memoria
- `efConstruction`: Calidad al construir
- `efSearch`: Trade-off velocidad/precisiÃ³n

**IVF (Inverted File Index)**

Particiona el espacio en clusters.

```python
# Cuantizador
quantizer = faiss.IndexFlatL2(dimension)

# IVF con nlist particiones
nlist = 100  # NÃºmero de clusters
index = faiss.IndexIVFFlat(quantizer, dimension, nlist)

# Entrenar
index.train(embeddings)
index.add(embeddings)

# BÃºsqueda
index.nprobe = 10  # Buscar en 10 clusters mÃ¡s cercanos
```

### 2. CompresiÃ³n

**Product Quantization (PQ)**

Reduce tamaÃ±o de vectores.

```python
# Original: 768 floats Ã— 4 bytes = 3KB por vector
# Con PQ: 96 bytes por vector (32x compresiÃ³n)

m = 96  # NÃºmero de subvectores
nbits = 8  # Bits por subvector

index = faiss.IndexPQ(dimension, m, nbits)
index.train(embeddings)
index.add(embeddings)
```

**Trade-off:**
- âœ… 10-100x menos memoria
- âŒ PÃ©rdida de precisiÃ³n (~5-10%)

### 3. Caching

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_embedding(text):
    return model.encode(text)

# Queries repetidas son instantÃ¡neas
emb1 = get_embedding("python")  # Calcula
emb2 = get_embedding("python")  # Cache hit!
```

### 4. Batch Processing

```python
# âŒ Lento: uno a la vez
for text in texts:
    embedding = model.encode(text)

# âœ… RÃ¡pido: en batch
embeddings = model.encode(texts, batch_size=32)
```

### 5. GPU Acceleration

```python
# FAISS con GPU
import faiss.contrib.torch_utils

# Mover Ã­ndice a GPU
res = faiss.StandardGpuResources()
index_gpu = faiss.index_cpu_to_gpu(res, 0, index_cpu)

# BÃºsquedas 10-100x mÃ¡s rÃ¡pidas
```

### Benchmark: Escalabilidad

| Vectores | FAISS Flat | FAISS HNSW | Pinecone |
|----------|------------|------------|----------|
| **1K** | 10ms | 5ms | 20ms |
| **10K** | 100ms | 10ms | 25ms |
| **100K** | 1s | 15ms | 30ms |
| **1M** | 10s | 20ms | 35ms |
| **10M** | 100s | 30ms | 40ms |

---

## ğŸ’¼ Casos de Uso {#casos-uso}

### 1. **BÃºsqueda de Documentos**

```python
# Empresa con base de conocimiento
docs = ["Manual de usuario...", "FAQ...", "PolÃ­tica..."]
doc_embeddings = model.encode(docs)

# Usuario busca
query = "Â¿CÃ³mo cambiar mi contraseÃ±a?"
query_emb = model.encode(query)

# Encontrar documentos relevantes
similarities = cosine_similarity([query_emb], doc_embeddings)[0]
top_doc = docs[np.argmax(similarities)]
```

### 2. **Recomendaciones**

```python
# Usuario vio producto
product_embedding = get_embedding(product_description)

# Encontrar similares
similar_products = vector_db.search(product_embedding, k=10)
```

### 3. **DetecciÃ³n de Duplicados**

```python
# Comparar nuevo contenido con existente
new_article_emb = model.encode(new_article)
existing_embs = model.encode(existing_articles)

similarities = cosine_similarity([new_article_emb], existing_embs)[0]
if max(similarities) > 0.95:
    print("Posible duplicado detectado")
```

### 4. **Clustering SemÃ¡ntico**

```python
from sklearn.cluster import KMeans

# Agrupar documentos similares
embeddings = model.encode(documents)
kmeans = KMeans(n_clusters=10)
clusters = kmeans.fit_predict(embeddings)
```

### 5. **Q&A Systems**

```python
# Base de Q&A
questions = ["Â¿QuÃ© es Python?", "Â¿CÃ³mo instalo pip?", ...]
answers = ["Python es...", "Pip se instala con...", ...]
q_embeddings = model.encode(questions)

# Usuario pregunta
user_q = "Â¿CÃ³mo usar pip?"
user_q_emb = model.encode(user_q)

# Encontrar pregunta mÃ¡s similar
similarities = cosine_similarity([user_q_emb], q_embeddings)[0]
best_match_idx = np.argmax(similarities)
answer = answers[best_match_idx]
```

---

## ğŸ“š Recursos Adicionales

### Papers Importantes

- [Sentence-BERT](https://arxiv.org/abs/1908.10084)
- [Dense Passage Retrieval](https://arxiv.org/abs/2004.04906)
- [HNSW Algorithm](https://arxiv.org/abs/1603.09320)

### DocumentaciÃ³n

- [Sentence Transformers](https://www.sbert.net/)
- [FAISS Wiki](https://github.com/facebookresearch/faiss/wiki)
- [ChromaDB Docs](https://docs.trychroma.com/)

### Benchmarks

- [MTEB Leaderboard](https://huggingface.co/spaces/mteb/leaderboard) - EvalÃºa modelos de embeddings

---

## ğŸ“ PrÃ³ximos Pasos

- **Koan 13: RAG** - Usa bÃºsqueda semÃ¡ntica para RAG
- **LangChain Retrievers** - Integra con agentes
- **Fine-tuning Embeddings** - Personaliza para tu dominio

Â¡Domina la bÃºsqueda semÃ¡ntica! ğŸš€
