> ** Translation Note**: This file is currently in Spanish. English translation coming soon!
> For now, you can use a translator or refer to the code examples which are language-agnostic.
> Want to help translate? See [CONTRIBUTING.md](../../CONTRIBUTING.md)

---

# TeorÃ­a: RAG (Retrieval-Augmented Generation)

## ğŸ“š Tabla de Contenidos
1. [IntroducciÃ³n a RAG](#introducciÃ³n)
2. [Arquitectura de RAG](#arquitectura)
3. [Document Processing & Chunking](#chunking)
4. [Retrieval Strategies](#retrieval)
5. [Generation & Prompting](#generation)
6. [Advanced RAG Patterns](#advanced)
7. [Evaluation](#evaluation)
8. [Production Best Practices](#production)

---

## ğŸ¯ IntroducciÃ³n a RAG {#introducciÃ³n}

### Â¿QuÃ© es RAG?

**RAG (Retrieval-Augmented Generation)** combina bÃºsqueda de informaciÃ³n con generaciÃ³n de lenguaje para crear respuestas mÃ¡s precisas y actualizadas.

```
Sin RAG:
Usuario: "Â¿CuÃ¡les son las Ãºltimas features de Python 3.12?"
LLM: "No tengo informaciÃ³n sobre Python 3.12..." âŒ

Con RAG:
Usuario: "Â¿CuÃ¡les son las Ãºltimas features de Python 3.12?"
Sistema:
  1. ğŸ” Busca en documentaciÃ³n actualizada
  2. ğŸ“„ Encuentra: "Python 3.12 aÃ±ade type parameter syntax..."
  3. ğŸ¤– LLM genera respuesta basada en docs encontrados
LLM: "Python 3.12 introduce las siguientes features:
      - Type parameter syntax (PEP 695)
      - Improved error messages..." âœ…
```

### Problema que Resuelve

**Limitaciones de LLMs Puros:**

1. **Knowledge Cutoff**
```python
# LLM entrenado hasta enero 2023
pregunta = "Â¿QuÃ© empresas compraron startups de IA en marzo 2024?"
respuesta = llm(pregunta)
# âŒ "No tengo informaciÃ³n mÃ¡s allÃ¡ de enero 2023"
```

2. **Alucinaciones**
```python
pregunta = "Â¿CuÃ¡l es el telÃ©fono de nuestra oficina en Madrid?"
respuesta = llm(pregunta)
# âŒ "+34 912 345 678" (inventado)
```

3. **InformaciÃ³n Privada**
```python
pregunta = "Â¿CuÃ¡l es la polÃ­tica de vacaciones de nuestra empresa?"
respuesta = llm(pregunta)
# âŒ No tiene acceso a documentos internos
```

**SoluciÃ³n con RAG:**

```python
pregunta = "Â¿CuÃ¡l es la polÃ­tica de vacaciones?"

# 1. Recuperar documentos relevantes
docs = retriever.search(pregunta)
# docs = ["Manual de empleados: Las vacaciones son 22 dÃ­as..."]

# 2. Generar respuesta basada en docs
respuesta = llm(f"BasÃ¡ndote en: {docs}\nResponde: {pregunta}")
# âœ… "SegÃºn el manual de empleados, tienes 22 dÃ­as de vacaciones..."
```

### Ventajas de RAG

| Ventaja | DescripciÃ³n | Ejemplo |
|---------|-------------|---------|
| **ğŸ”„ Actualizable** | InformaciÃ³n siempre fresca | AÃ±adir docs nuevos sin reentrenar |
| **ğŸ“š Conocimiento EspecÃ­fico** | Dominio especializado | Manuales tÃ©cnicos, polÃ­ticas internas |
| **ğŸ¯ ReducciÃ³n de Alucinaciones** | Respuestas basadas en hechos | Citas y referencias verificables |
| **ğŸ’° Cost-Effective** | No requiere fine-tuning | Actualizar base de datos vs reentrenar |
| **ğŸ” Trazabilidad** | Citar fuentes | "SegÃºn documento X, pÃ¡gina Y..." |

### RAG vs Alternativas

**RAG vs Fine-Tuning:**

| Aspecto | RAG | Fine-Tuning |
|---------|-----|-------------|
| **Costo** | $ | $$$ |
| **ActualizaciÃ³n** | InstantÃ¡nea | Requiere reentrenamiento |
| **Conocimiento EspecÃ­fico** | âœ… Excelente | âœ… Excelente |
| **Flexibilidad** | âœ… Alta | âŒ Baja |
| **Trazabilidad** | âœ… Citas claras | âŒ Black box |

**RAG vs Prompt Engineering:**

| Aspecto | RAG | Prompt Engineering |
|---------|-----|-------------------|
| **LÃ­mite de Contexto** | âœ… Ilimitado (retrieval) | âŒ Limitado (tokens) |
| **PrecisiÃ³n** | âœ… Alta | âš ï¸ Media |
| **Costo por Query** | $ | $$ |

---

## ğŸ—ï¸ Arquitectura de RAG {#arquitectura}

### Pipeline BÃ¡sico

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Usuario    â”‚
â”‚   Pregunta   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. INGESTION (Offline)                  â”‚
â”‚                                          â”‚
â”‚  Documentos â†’ Chunks â†’ Embeddings       â”‚
â”‚                            â†“             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚                    â”‚ Vector Store â”‚     â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. RETRIEVAL (Query time)               â”‚
â”‚                                          â”‚
â”‚  Query â†’ Embedding â†’ Search Vector DB   â”‚
â”‚                            â†“             â”‚
â”‚                  Top K Documents         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. GENERATION                           â”‚
â”‚                                          â”‚
â”‚  Prompt Template:                       â”‚
â”‚  "Contexto: {docs}"                     â”‚
â”‚  "Pregunta: {query}"                    â”‚
â”‚             â†“                            â”‚
â”‚           LLM                            â”‚
â”‚             â†“                            â”‚
â”‚        Respuesta                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Clave

**1. Document Loaders**
```python
from langchain.document_loaders import (
    TextLoader,
    PyPDFLoader,
    CSVLoader,
    UnstructuredHTMLLoader
)

# Cargar diferentes formatos
loader = PyPDFLoader("manual.pdf")
documents = loader.load()
```

**2. Text Splitters**
```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

chunks = splitter.split_documents(documents)
```

**3. Embeddings**
```python
from langchain.embeddings import OpenAIEmbeddings

embeddings = OpenAIEmbeddings()
```

**4. Vector Store**
```python
from langchain.vectorstores import Chroma

vector_store = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings
)
```

**5. Retriever**
```python
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)
```

**6. LLM**
```python
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(temperature=0)
```

**7. Chain**
```python
from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)
```

---

## ğŸ“„ Document Processing & Chunking {#chunking}

### Â¿Por quÃ© Chunking?

**Problema:** Documentos largos no caben en contexto del LLM.

```python
# Contexto tÃ­pico de LLM: 4k-128k tokens
# Documento: 1 millÃ³n de tokens âŒ
```

**SoluciÃ³n:** Dividir en chunks manejables.

```python
# Documento â†’ [Chunk1, Chunk2, ..., ChunkN]
# Cada chunk: 500-1500 tokens âœ…
```

### Estrategias de Chunking

#### 1. **Fixed Size (TamaÃ±o Fijo)**

```python
def fixed_size_chunks(text, chunk_size=1000):
    return [text[i:i+chunk_size] 
            for i in range(0, len(text), chunk_size)]
```

**Pros:**
- âœ… Simple
- âœ… Predecible

**Cons:**
- âŒ Rompe en medio de oraciones
- âŒ Pierde contexto

#### 2. **Fixed Size con Overlap**

```python
from langchain.text_splitter import CharacterTextSplitter

splitter = CharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200  # 20% overlap
)
```

**Ventaja del Overlap:**
```
Chunk 1: "...al final de este pÃ¡rrafo hablamos de Python..."
Chunk 2: "...Python es un lenguaje..." â† Contexto mantenido
```

**RecomendaciÃ³n:**
- Overlap: 10-20% del chunk size
- Chunk size: 500-1500 tokens

#### 3. **Semantic Chunking (por Significado)**

Divide por cambios de tema o estructura.

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Prioriza splits por:
# 1. PÃ¡rrafos (\n\n)
# 2. LÃ­neas (\n)
# 3. Oraciones (.)
# 4. Palabras ( )

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

#### 4. **Document-Aware Chunking**

Respeta estructura del documento.

```python
# Markdown
from langchain.text_splitter import MarkdownHeaderTextSplitter

splitter = MarkdownHeaderTextSplitter(
    headers_to_split_on=[
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
)

# HTML
from langchain.text_splitter import HTMLHeaderTextSplitter

splitter = HTMLHeaderTextSplitter(
    headers_to_split_on=[
        ("h1", "Header 1"),
        ("h2", "Header 2"),
    ]
)
```

#### 5. **Sentence-Based Chunking**

```python
import nltk

def sentence_chunks(text, max_sentences=5):
    sentences = nltk.sent_tokenize(text)
    chunks = []
    
    for i in range(0, len(sentences), max_sentences):
        chunk = " ".join(sentences[i:i+max_sentences])
        chunks.append(chunk)
    
    return chunks
```

### Metadata Enrichment

AÃ±adir metadatos a chunks para mejor retrieval.

```python
from langchain.schema import Document

chunks = []
for i, text in enumerate(split_texts):
    chunk = Document(
        page_content=text,
        metadata={
            "source": "manual.pdf",
            "page": page_num,
            "chunk_id": i,
            "section": section_name,
            "author": author,
            "date": "2024-01-15"
        }
    )
    chunks.append(chunk)
```

**Beneficios:**
- Filtrado preciso
- Citas con contexto
- Auditabilidad

### Chunking Best Practices

| Factor | RecomendaciÃ³n | RazÃ³n |
|--------|---------------|-------|
| **Chunk Size** | 500-1500 tokens | Balance contexto/precisiÃ³n |
| **Overlap** | 10-20% | Mantiene continuidad |
| **MÃ©todo** | RecursiveCharacter | Respeta estructura |
| **Metadata** | Siempre incluir | Trazabilidad y filtrado |

---

## ğŸ” Retrieval Strategies {#retrieval}

### 1. **Basic Similarity Search**

BÃºsqueda por similitud coseno.

```python
retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 4}
)

docs = retriever.get_relevant_documents("Â¿QuÃ© es Python?")
```

**Pros:**
- âœ… Simple y rÃ¡pido
- âœ… Funciona bien en la mayorÃ­a de casos

**Cons:**
- âŒ Solo considera similitud semÃ¡ntica
- âŒ Puede recuperar documentos redundantes

### 2. **MMR (Maximal Marginal Relevance)**

Balancea relevancia y diversidad.

```python
retriever = vectorstore.as_retriever(
    search_type="mmr",
    search_kwargs={
        "k": 4,
        "fetch_k": 20,  # Candidatos iniciales
        "lambda_mult": 0.5  # Balance relevancia/diversidad
    }
)
```

**ParÃ¡metros:**
- `fetch_k`: Documentos candidatos
- `lambda_mult`:
  - `1.0`: Solo relevancia (como similarity)
  - `0.5`: Balance
  - `0.0`: MÃ¡xima diversidad

**Ventaja:**
```
Similarity: ["Intro Python", "Python basics", "Python tutorial"]
            â†‘ Todos muy similares entre sÃ­

MMR: ["Intro Python", "Python advanced", "Python vs Java"]
     â†‘ Relevantes pero diversos
```

### 3. **Similarity Score Threshold**

Solo documentos con score mÃ­nimo.

```python
retriever = vectorstore.as_retriever(
    search_type="similarity_score_threshold",
    search_kwargs={
        "score_threshold": 0.8,  # MÃ­nimo 0.8 similitud
        "k": 4
    }
)
```

**Ventaja:**
- âœ… Filtra resultados irrelevantes
- âœ… Respuestas mÃ¡s precisas

**PrecauciÃ³n:**
- âš ï¸ Puede retornar 0 documentos si threshold muy alto

### 4. **Metadata Filtering**

Filtrar por metadatos antes de buscar.

```python
retriever = vectorstore.as_retriever(
    search_kwargs={
        "k": 4,
        "filter": {
            "source": "manual_2024.pdf",
            "section": "installation"
        }
    }
)
```

**Casos de uso:**
```python
# Por fecha
filter={"date": {"$gte": "2024-01-01"}}

# Por categorÃ­a
filter={"category": {"$in": ["python", "programming"]}}

# Combinado
filter={
    "author": "John Doe",
    "verified": True,
    "date": {"$gte": "2023-01-01"}
}
```

### 5. **Compression (Contextual Compression)**

Comprime documentos recuperados para extraer partes relevantes.

```python
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

# Base retriever
base_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

# Compressor (extrae partes relevantes con LLM)
compressor = LLMChainExtractor.from_llm(llm)

# Compression retriever
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)
```

**Ventaja:**
```
Antes: [Doc completo 2000 tokens]
DespuÃ©s: [Solo pÃ¡rrafo relevante 200 tokens]
```

**Beneficio:**
- âœ… MÃ¡s contexto Ãºtil en menos tokens
- âœ… Respuestas mÃ¡s precisas

### 6. **Multi-Query Retrieval**

Genera mÃºltiples variantes de la query.

```python
from langchain.retrievers.multi_query import MultiQueryRetriever

retriever = MultiQueryRetriever.from_llm(
    retriever=vectorstore.as_retriever(),
    llm=llm
)

# Usuario: "Â¿CÃ³mo instalar Python?"
# Sistema genera:
#   - "Â¿CÃ³mo instalar Python?"
#   - "Pasos para instalaciÃ³n de Python"
#   - "GuÃ­a de instalaciÃ³n Python"
#   - "Configurar Python en mi sistema"
```

**Ventaja:**
- âœ… Captura diferentes formulaciones
- âœ… MÃ¡s robusto a queries ambiguas

### 7. **Ensemble Retriever**

Combina mÃºltiples retrievers.

```python
from langchain.retrievers import EnsembleRetriever
from langchain.retrievers import BM25Retriever

# Dense retriever (embeddings)
dense_retriever = vectorstore.as_retriever()

# Sparse retriever (BM25)
bm25_retriever = BM25Retriever.from_documents(documents)

# Combinar
ensemble_retriever = EnsembleRetriever(
    retrievers=[dense_retriever, bm25_retriever],
    weights=[0.5, 0.5]  # 50% cada uno
)
```

**Ventaja:**
- âœ… Mejor de ambos mundos (semantic + keyword)

---

## ğŸ¤– Generation & Prompting {#generation}

### Prompt Templates para RAG

#### Template BÃ¡sico

```python
from langchain.prompts import PromptTemplate

template = """Usa el siguiente contexto para responder la pregunta.
Si no sabes la respuesta, di que no lo sabes.

Contexto: {context}

Pregunta: {question}

Respuesta:"""

prompt = PromptTemplate(
    template=template,
    input_variables=["context", "question"]
)
```

#### Template con Instrucciones EspecÃ­ficas

```python
template = """Eres un asistente experto en Python. Usa el contexto proporcionado 
para responder la pregunta del usuario.

REGLAS:
1. Solo usa informaciÃ³n del contexto
2. Si la respuesta no estÃ¡ en el contexto, di "No tengo esa informaciÃ³n"
3. Incluye ejemplos de cÃ³digo cuando sea relevante
4. Cita la fuente del contexto

Contexto:
{context}

Pregunta: {question}

Respuesta (siguiendo las reglas):"""
```

#### Template con Few-Shot Examples

```python
template = """Responde basÃ¡ndote en el contexto proporcionado.

EJEMPLOS:
Pregunta: Â¿QuÃ© es Python?
Respuesta: SegÃºn el documento "Intro.pdf", Python es un lenguaje...

Pregunta: Â¿CuÃ¡nto cuesta la licencia?
Respuesta: No encuentro informaciÃ³n sobre precios en los documentos proporcionados.

---

Contexto actual:
{context}

Pregunta: {question}

Respuesta:"""
```

### Chain Types

#### 1. **Stuff Chain**

Concatena todos los docs en un solo prompt.

```python
from langchain.chains import RetrievalQA

qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever
)
```

**Ventajas:**
- âœ… Simple
- âœ… Una sola llamada al LLM
- âœ… Respuesta coherente

**Desventajas:**
- âŒ Limitado por contexto del LLM
- âŒ No escala con muchos docs

**CuÃ¡ndo usar:**
- Pocos documentos (< 4)
- Documentos pequeÃ±os

#### 2. **Map-Reduce Chain**

Procesa cada doc independientemente, luego combina.

```python
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="map_reduce",
    retriever=retriever
)
```

**Proceso:**
```
Doc1 â†’ LLM â†’ Summary1 â”
Doc2 â†’ LLM â†’ Summary2 â”œâ†’ Combinar â†’ Respuesta Final
Doc3 â†’ LLM â†’ Summary3 â”˜
```

**Ventajas:**
- âœ… Escala a muchos docs
- âœ… Paralelizable

**Desventajas:**
- âŒ MÃºltiples llamadas al LLM (costoso)
- âŒ Puede perder contexto entre docs

#### 3. **Refine Chain**

Refina la respuesta iterativamente.

```python
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="refine",
    retriever=retriever
)
```

**Proceso:**
```
Doc1 â†’ LLM â†’ Answer1
Answer1 + Doc2 â†’ LLM â†’ Answer2
Answer2 + Doc3 â†’ LLM â†’ Answer3 (final)
```

**Ventajas:**
- âœ… Respuesta mejorada iterativamente
- âœ… Mantiene contexto

**Desventajas:**
- âŒ Secuencial (no paralelizable)
- âŒ Costoso (N llamadas)

#### 4. **Map-Rerank Chain**

LLM asigna score de confianza a cada respuesta.

```python
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="map_rerank",
    retriever=retriever
)
```

**Proceso:**
```
Doc1 â†’ LLM â†’ (Answer1, Score: 0.8)
Doc2 â†’ LLM â†’ (Answer2, Score: 0.95) â† Seleccionada
Doc3 â†’ LLM â†’ (Answer3, Score: 0.6)
```

**Ventaja:**
- âœ… Selecciona mejor respuesta automÃ¡ticamente

### Citations (Citas)

Incluir fuentes en la respuesta.

```python
template = """Usa el contexto para responder. SIEMPRE cita la fuente.

Contexto:
{context}

Pregunta: {question}

Respuesta con formato:
[Tu respuesta]

Fuentes:
- Documento X, pÃ¡gina Y
"""

# O usando return_source_documents
qa = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

result = qa({"query": "Â¿QuÃ© es Python?"})
answer = result["result"]
sources = result["source_documents"]
```

---

## ğŸš€ Advanced RAG Patterns {#advanced}

### 1. **Multi-Query RAG**

Ya visto en retrieval. Genera mÃºltiples queries.

### 2. **RAG Fusion**

Combina resultados de mÃºltiples queries con Reciprocal Rank Fusion.

```python
def reciprocal_rank_fusion(results_lists, k=60):
    """
    results_lists: [[doc1, doc2, ...], [doc3, doc1, ...], ...]
    """
    doc_scores = {}
    
    for results in results_lists:
        for rank, doc in enumerate(results):
            doc_id = doc.metadata.get("id", str(doc))
            
            # RRF score
            score = 1 / (k + rank + 1)
            
            if doc_id in doc_scores:
                doc_scores[doc_id] += score
            else:
                doc_scores[doc_id] = score
    
    # Ordenar por score
    sorted_docs = sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)
    return sorted_docs
```

**Ventaja:**
- âœ… MÃ¡s robusto que single query
- âœ… Combina perspectivas diferentes

### 3. **Conversational RAG**

Mantiene historial de conversaciÃ³n.

```python
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=True,
    output_key="answer"
)

qa = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory
)

# Primera pregunta
response1 = qa({"question": "Â¿QuÃ© es Python?"})

# Pregunta de seguimiento (usa contexto)
response2 = qa({"question": "Â¿CuÃ¡les son sus ventajas?"})
# Entiende que "sus" se refiere a Python
```

**TÃ©cnicas:**

**Query Rewriting:**
```python
# Usuario: "Â¿Y sus desventajas?" (ambiguo)
# Sistema reescribe: "Â¿CuÃ¡les son las desventajas de Python?"
```

**Condensed Question:**
```python
# Historial:
# User: "Â¿QuÃ© es Python?"
# AI: "Python es un lenguaje..."
# User: "Dame ejemplos"

# Sistema condensa:
# "Dame ejemplos de Python" o "Dame ejemplos de cÃ³digo Python"
```

### 4. **Self-Query Retriever**

LLM extrae filtros de la query en lenguaje natural.

```python
from langchain.retrievers.self_query.base import SelfQueryRetriever
from langchain.chains.query_constructor.base import AttributeInfo

metadata_field_info = [
    AttributeInfo(
        name="source",
        description="El origen del documento",
        type="string"
    ),
    AttributeInfo(
        name="date",
        description="Fecha de publicaciÃ³n",
        type="date"
    ),
]

retriever = SelfQueryRetriever.from_llm(
    llm=llm,
    vectorstore=vectorstore,
    document_contents="DocumentaciÃ³n de Python",
    metadata_field_info=metadata_field_info
)

# Query: "documentos sobre funciones publicados despuÃ©s de 2023"
# Sistema extrae:
#   - Query semÃ¡ntica: "funciones"
#   - Filtro: date > 2023-01-01
```

### 5. **Parent Document Retriever**

Busca en chunks pequeÃ±os, retorna documentos completos.

```python
from langchain.retrievers import ParentDocumentRetriever
from langchain.storage import InMemoryStore

# Store para documentos completos
docstore = InMemoryStore()

# Retriever que busca chunks pero retorna parents
retriever = ParentDocumentRetriever(
    vectorstore=vectorstore,
    docstore=docstore,
    child_splitter=child_splitter,  # Chunks pequeÃ±os
    parent_splitter=parent_splitter,  # Docs completos (opcional)
)
```

**Ventaja:**
```
BÃºsqueda: chunk de 200 tokens (preciso) âœ…
Contexto al LLM: documento completo de 2000 tokens (contexto rico) âœ…
```

### 6. **Hypothetical Document Embeddings (HyDE)**

LLM genera documento hipotÃ©tico, se usa para bÃºsqueda.

```python
# 1. Usuario pregunta
query = "Â¿CÃ³mo optimizar queries SQL?"

# 2. LLM genera respuesta hipotÃ©tica (sin contexto)
hypothetical_doc = llm.predict(
    f"Escribe un artÃ­culo que responda: {query}"
)

# 3. Embedding del documento hipotÃ©tico
hyde_embedding = embeddings.embed_query(hypothetical_doc)

# 4. Buscar con ese embedding
docs = vectorstore.similarity_search_by_vector(hyde_embedding)
```

**Ventaja:**
- âœ… Mejor que buscar con query corta
- âœ… Captura intenciÃ³n mÃ¡s completa

---

## ğŸ“Š Evaluation {#evaluation}

### MÃ©tricas Clave

#### 1. **Retrieval Metrics**

**Precision@K:**
```python
def precision_at_k(relevant_docs, retrieved_docs, k):
    retrieved_k = retrieved_docs[:k]
    relevant_retrieved = len(set(relevant_docs) & set(retrieved_k))
    return relevant_retrieved / k
```

**Recall@K:**
```python
def recall_at_k(relevant_docs, retrieved_docs, k):
    retrieved_k = retrieved_docs[:k]
    relevant_retrieved = len(set(relevant_docs) & set(retrieved_k))
    return relevant_retrieved / len(relevant_docs)
```

**MRR (Mean Reciprocal Rank):**
```python
def mrr(relevant_docs, retrieved_docs):
    for i, doc in enumerate(retrieved_docs):
        if doc in relevant_docs:
            return 1 / (i + 1)
    return 0
```

**NDCG (Normalized Discounted Cumulative Gain):**
```python
from sklearn.metrics import ndcg_score

# relevance_scores: [3, 2, 3, 0, 1] (graded relevance)
# retrieved_scores: scores de retrieval
ndcg = ndcg_score([relevance_scores], [retrieved_scores])
```

#### 2. **Generation Metrics**

**Faithfulness:**
Â¿La respuesta estÃ¡ basada en el contexto?

```python
from ragas.metrics import faithfulness

score = faithfulness.score(
    question=question,
    answer=answer,
    contexts=[doc.page_content for doc in retrieved_docs]
)
```

**Answer Relevancy:**
Â¿La respuesta responde la pregunta?

```python
from ragas.metrics import answer_relevancy

score = answer_relevancy.score(
    question=question,
    answer=answer
)
```

**Context Relevancy:**
Â¿El contexto recuperado es relevante?

```python
from ragas.metrics import context_relevancy

score = context_relevancy.score(
    question=question,
    contexts=[doc.page_content for doc in retrieved_docs]
)
```

#### 3. **End-to-End Metrics**

**Correctness:**
ComparaciÃ³n con ground truth.

```python
from ragas.metrics import answer_correctness

score = answer_correctness.score(
    question=question,
    answer=answer,
    ground_truth=expected_answer
)
```

### Frameworks de EvaluaciÃ³n

#### RAGAS (RAG Assessment)

```python
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
)

# Dataset de evaluaciÃ³n
eval_dataset = {
    "question": [...],
    "answer": [...],
    "contexts": [...],
    "ground_truths": [...]
}

# Evaluar
result = evaluate(
    eval_dataset,
    metrics=[
        faithfulness,
        answer_relevancy,
        context_recall,
        context_precision,
    ],
)

print(result)
# {
#   "faithfulness": 0.92,
#   "answer_relevancy": 0.88,
#   "context_recall": 0.85,
#   "context_precision": 0.90
# }
```

#### TruLens

```python
from trulens_eval import TruChain, Feedback, Tru

# Crear feedback functions
f_qa_relevance = Feedback(relevance_scorer).on_input_output()
f_groundedness = Feedback(groundedness_scorer).on(contexts).on_output()

# Wrap chain
tru_chain = TruChain(
    qa_chain,
    app_id="my_rag_app",
    feedbacks=[f_qa_relevance, f_groundedness]
)

# Usar normalmente
result = tru_chain.query("Â¿QuÃ© es Python?")

# Ver dashboard
tru = Tru()
tru.run_dashboard()
```

### A/B Testing

```python
def compare_rag_systems(systemA, systemB, test_queries):
    results = {"A": [], "B": []}
    
    for query in test_queries:
        # Sistema A
        responseA = systemA.query(query)
        scoreA = evaluate_response(query, responseA)
        results["A"].append(scoreA)
        
        # Sistema B
        responseB = systemB.query(query)
        scoreB = evaluate_response(query, responseB)
        results["B"].append(scoreB)
    
    # Comparar
    import numpy as np
    print(f"A: {np.mean(results['A']):.2f}")
    print(f"B: {np.mean(results['B']):.2f}")
```

---

## ğŸ­ Production Best Practices {#production}

### 1. **Chunking Strategy**

```python
# âœ… Recomendado para producciÃ³n
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

**Experimentar con:**
- Chunk sizes: 500, 1000, 1500 tokens
- Overlaps: 10%, 20%, 30%
- Medir impacto en mÃ©tricas

### 2. **Indexing Pipeline**

```python
# Pipeline robusto con error handling
def index_documents(documents):
    try:
        # 1. Validar documentos
        validated = validate_documents(documents)
        
        # 2. Chunking
        chunks = splitter.split_documents(validated)
        
        # 3. Enriquecer metadata
        enriched = enrich_metadata(chunks)
        
        # 4. Generar embeddings (batch)
        embeddings = embedding_model.embed_documents(
            [c.page_content for c in enriched],
            batch_size=32
        )
        
        # 5. AÃ±adir a vector store
        vectorstore.add_documents(enriched)
        
        # 6. Log success
        logger.info(f"Indexed {len(chunks)} chunks")
        
    except Exception as e:
        logger.error(f"Indexing failed: {e}")
        raise
```

### 3. **Caching**

```python
from functools import lru_cache
import hashlib

@lru_cache(maxsize=1000)
def get_embedding(text):
    return embedding_model.embed_query(text)

# Cache de respuestas completas
class RAGCache:
    def __init__(self):
        self.cache = {}
    
    def get(self, query):
        query_hash = hashlib.md5(query.encode()).hexdigest()
        return self.cache.get(query_hash)
    
    def set(self, query, response):
        query_hash = hashlib.md5(query.encode()).hexdigest()
        self.cache[query_hash] = response
```

### 4. **Monitoring**

```python
import logging
from datetime import datetime

class RAGMonitor:
    def log_query(self, query, response, latency, docs_retrieved):
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "query": query,
            "response_length": len(response),
            "latency_ms": latency,
            "docs_retrieved": len(docs_retrieved),
            "avg_doc_score": np.mean([d.score for d in docs_retrieved])
        }
        
        logging.info(log_entry)
        
        # Alertas
        if latency > 5000:  # > 5 segundos
            self.alert("High latency detected")
        
        if len(docs_retrieved) == 0:
            self.alert("No documents retrieved")
```

### 5. **Rate Limiting**

```python
from ratelimit import limits, sleep_and_retry

@sleep_and_retry
@limits(calls=50, period=60)  # 50 llamadas por minuto
def query_rag_system(query):
    return rag_chain.run(query)
```

### 6. **Error Handling**

```python
def robust_rag_query(query, max_retries=3):
    for attempt in range(max_retries):
        try:
            # Validar query
            if not query or len(query) > 1000:
                raise ValueError("Invalid query")
            
            # Retrieval
            docs = retriever.get_relevant_documents(query)
            
            if not docs:
                return "No encontrÃ© informaciÃ³n relevante."
            
            # Generation
            response = llm_chain.run(
                context=docs,
                question=query
            )
            
            return response
            
        except RateLimitError:
            if attempt < max_retries - 1:
                sleep(2 ** attempt)  # Exponential backoff
            else:
                return "Sistema temporalmente ocupado. Intenta mÃ¡s tarde."
        
        except Exception as e:
            logger.error(f"Error: {e}")
            return "OcurriÃ³ un error. Por favor intenta de nuevo."
```

### 7. **Incremental Updates**

```python
# AÃ±adir nuevos documentos sin reindexar todo
def add_new_documents(new_docs):
    # 1. Procesar
    chunks = splitter.split_documents(new_docs)
    
    # 2. Check duplicados
    existing_ids = vectorstore.get_all_ids()
    chunks = [c for c in chunks if c.metadata["id"] not in existing_ids]
    
    # 3. AÃ±adir
    if chunks:
        vectorstore.add_documents(chunks)
        logger.info(f"Added {len(chunks)} new chunks")
```

### 8. **Vector Store Optimization**

```python
# Para FAISS
import faiss

# Usar GPU si disponible
if faiss.get_num_gpus() > 0:
    index = faiss.index_cpu_to_gpu(
        faiss.StandardGpuResources(),
        0,
        index
    )

# Product Quantization para reducir memoria
index = faiss.IndexIVFPQ(
    quantizer,
    dimension,
    nlist=100,
    M=8,
    nbits=8
)
```

### 9. **Testing**

```python
import pytest

def test_rag_pipeline():
    # Test retrieval
    docs = retriever.get_relevant_documents("test query")
    assert len(docs) > 0
    assert all(hasattr(d, "page_content") for d in docs)
    
    # Test generation
    response = qa_chain.run("test query")
    assert len(response) > 0
    assert "no sÃ©" not in response.lower() or len(docs) == 0

def test_edge_cases():
    # Query vacÃ­a
    response = rag_system.query("")
    assert "invalid" in response.lower()
    
    # Query muy larga
    long_query = "test " * 1000
    response = rag_system.query(long_query)
    assert response is not None
```

### 10. **Deployment Checklist**

- [ ] Embeddings cacheados
- [ ] Vector store optimizado
- [ ] Rate limiting configurado
- [ ] Monitoring activo
- [ ] Error handling robusto
- [ ] Tests automatizados
- [ ] DocumentaciÃ³n completa
- [ ] Backup de vector store
- [ ] Plan de escalado
- [ ] Security (API keys, authentication)

---

## ğŸ¯ Casos de Uso Reales

### 1. **Customer Support**

```python
# Base de conocimiento de FAQs
qa_system = RAGSystem(
    documents=load_faq_documents(),
    chunk_size=500,
    retriever_k=3
)

# Usuario pregunta
answer = qa_system.query("Â¿CÃ³mo reseteo mi contraseÃ±a?")
```

### 2. **Code Documentation Assistant**

```python
# Indexar cÃ³digo + docs
code_qa = RAGSystem(
    documents=load_code_and_docs(),
    chunk_size=1500,  # Mayor para cÃ³digo
    metadata_filters={"type": "code"}
)

# Developer pregunta
answer = code_qa.query("Â¿CÃ³mo usar la API de autenticaciÃ³n?")
```

### 3. **Legal Document Analysis**

```python
# Documentos legales
legal_qa = RAGSystem(
    documents=load_legal_docs(),
    chunk_size=1000,
    return_source_documents=True  # Citas esenciales
)

# Abogado busca precedentes
result = legal_qa.query("Casos similares a fraude corporativo 2020-2024")
answer = result["answer"]
sources = result["sources"]  # Para verificaciÃ³n
```

### 4. **Research Assistant**

```python
# Papers cientÃ­ficos
research_qa = ConversationalRAG(
    documents=load_research_papers(),
    memory=ConversationBufferMemory()
)

# Investigador hace preguntas iterativas
research_qa.query("Â¿QuÃ© dice la literatura sobre transformers?")
research_qa.query("Â¿Y sobre sus limitaciones?")
research_qa.query("Dame papers especÃ­ficos")
```

---

## ğŸ“š Recursos Adicionales

### Papers Fundamentales

- [RAG (Lewis et al., 2020)](https://arxiv.org/abs/2005.11401) - Paper original
- [Self-RAG (Asai et al., 2023)](https://arxiv.org/abs/2310.11511)
- [Corrective RAG (Yan et al., 2024)](https://arxiv.org/abs/2401.15884)

### Frameworks

- [LangChain](https://python.langchain.com/) - Framework completo para RAG
- [LlamaIndex](https://www.llamaindex.ai/) - Alternativa especializada en RAG
- [Haystack](https://haystack.deepset.ai/) - Pipeline de NLP

### Herramientas

- [RAGAS](https://github.com/explodinggradients/ragas) - EvaluaciÃ³n de RAG
- [TruLens](https://www.trulens.org/) - Monitoring y debugging
- [Weights & Biases](https://wandb.ai/) - Experiment tracking

---

## ğŸ“ PrÃ³ximos Pasos

DespuÃ©s de dominar RAG:

1. **Advanced RAG Architectures**
   - Self-RAG con reflexiÃ³n
   - Corrective RAG
   - Adaptive RAG

2. **Multi-Modal RAG**
   - Texto + imÃ¡genes
   - Texto + tablas
   - Texto + cÃ³digo

3. **Agent-Based RAG**
   - Agentes que deciden cuÃ¡ndo usar RAG
   - RAG como tool en sistemas multi-agente

4. **Fine-Tuning para RAG**
   - Embeddings especÃ­ficos de dominio
   - LLMs optimizados para RAG

Â¡Domina RAG y construye aplicaciones inteligentes! ğŸš€
