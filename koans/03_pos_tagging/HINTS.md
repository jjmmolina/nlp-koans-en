# üí° Pistas para Koan 03: POS Tagging

## üéØ Objetivo del Koan

Aprender a **etiquetar categor√≠as gramaticales** (Part-of-Speech):
- Identificar verbos, sustantivos, adjetivos, etc.
- Usar etiquetas universales (Universal Dependencies)
- Extraer palabras por categor√≠a

---

## üìù Funci√≥n 1: `pos_tag_nltk()`

### Nivel 1: Concepto
NLTK tiene un tagger entrenado para ingl√©s que asigna etiquetas POS a cada palabra.

### Nivel 2: Implementaci√≥n
```python
from nltk import pos_tag, word_tokenize
tokens = word_tokenize(text)
# pos_tag(tokens) retorna lista de tuplas (palabra, etiqueta)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def pos_tag_nltk(text: str) -> List[Tuple[str, str]]:
    from nltk import pos_tag, word_tokenize
    tokens = word_tokenize(text)
    return pos_tag(tokens)
```
</details>

---

## üìù Funci√≥n 2: `pos_tag_spacy()`

### Nivel 1: Concepto
spaCy hace POS tagging autom√°ticamente. Cada token tiene:
- `token.pos_`: Etiqueta universal (NOUN, VERB, ADJ, etc.)
- `token.tag_`: Etiqueta espec√≠fica del idioma

### Nivel 2: Implementaci√≥n
```python
import spacy
nlp = spacy.load("es_core_news_sm")
doc = nlp(text)
# Retorna lista de (texto, pos_, tag_)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def pos_tag_spacy(text: str, lang: str = "es") -> List[Tuple[str, str, str]]:
    import spacy
    model = "es_core_news_sm" if lang == "es" else "en_core_web_sm"
    nlp = spacy.load(model)
    doc = nlp(text)
    return [(token.text, token.pos_, token.tag_) for token in doc]
```
</details>

---

## üìù Funci√≥n 3: `get_nouns()`

### Nivel 1: Concepto
Extrae solo los sustantivos de un texto usando spaCy.

### Nivel 2: Pasos
1. Procesa texto con spaCy
2. Filtra tokens donde `token.pos_ == "NOUN"`
3. Retorna lista de textos

### Nivel 3: Casi la soluci√≥n
```python
doc = nlp(text)
return [token.text for token in doc if token.pos_ == "NOUN"]
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def get_nouns(text: str, lang: str = "es") -> List[str]:
    import spacy
    model = "es_core_news_sm" if lang == "es" else "en_core_web_sm"
    nlp = spacy.load(model)
    doc = nlp(text)
    return [token.text for token in doc if token.pos_ == "NOUN"]
```
</details>

---

## üìù Funci√≥n 4: `get_verbs()`

### Nivel 1: Concepto
Similar a `get_nouns()`, pero filtra por `token.pos_ == "VERB"`.

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def get_verbs(text: str, lang: str = "es") -> List[str]:
    import spacy
    model = "es_core_news_sm" if lang == "es" else "en_core_web_sm"
    nlp = spacy.load(model)
    doc = nlp(text)
    return [token.text for token in doc if token.pos_ == "VERB"]
```
</details>

---

## üìù Funci√≥n 5: `get_adjectives()`

### Nivel 1: Concepto
Filtra por `token.pos_ == "ADJ"` para obtener adjetivos.

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def get_adjectives(text: str, lang: str = "es") -> List[str]:
    import spacy
    model = "es_core_news_sm" if lang == "es" else "en_core_web_sm"
    nlp = spacy.load(model)
    doc = nlp(text)
    return [token.text for token in doc if token.pos_ == "ADJ"]
```
</details>

---

## üìù Funci√≥n 6: `filter_by_pos()`

### Nivel 1: Concepto
Versi√≥n gen√©rica que filtra por **cualquier** etiqueta POS.

### Nivel 2: Pasos
```python
doc = nlp(text)
# Convierte pos_tags a set para b√∫squeda r√°pida
pos_set = set(pos_tags)
return [token.text for token in doc if token.pos_ in pos_set]
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def filter_by_pos(text: str, pos_tags: List[str], lang: str = "es") -> List[str]:
    import spacy
    model = "es_core_news_sm" if lang == "es" else "en_core_web_sm"
    nlp = spacy.load(model)
    doc = nlp(text)
    pos_set = set(pos_tags)
    return [token.text for token in doc if token.pos_ in pos_set]
```
</details>

---

## üìù Funci√≥n 7: `analyze_sentence_structure()`

### Nivel 1: Concepto
Cuenta cu√°ntas palabras de cada categor√≠a hay en el texto.

### Nivel 2: Implementaci√≥n
```python
from collections import Counter
doc = nlp(text)
pos_counts = Counter([token.pos_ for token in doc])
return dict(pos_counts)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def analyze_sentence_structure(text: str, lang: str = "es") -> dict:
    import spacy
    from collections import Counter
    model = "es_core_news_sm" if lang == "es" else "en_core_web_sm"
    nlp = spacy.load(model)
    doc = nlp(text)
    pos_counts = Counter([token.pos_ for token in doc])
    return dict(pos_counts)
```
</details>

---

## üéØ Etiquetas POS Universales

### Principales etiquetas (Universal Dependencies)

| Etiqueta | Nombre | Ejemplo ES | Ejemplo EN |
|----------|--------|------------|------------|
| **NOUN** | Sustantivo | casa, perro | house, dog |
| **VERB** | Verbo | comer, correr | eat, run |
| **ADJ** | Adjetivo | grande, azul | big, blue |
| **ADV** | Adverbio | r√°pidamente | quickly |
| **PRON** | Pronombre | √©l, ella | he, she |
| **DET** | Determinante | el, la, un | the, a |
| **ADP** | Preposici√≥n | de, en, por | of, in, by |
| **CONJ** | Conjunci√≥n | y, o, pero | and, or, but |
| **PUNCT** | Puntuaci√≥n | . , ; | . , ; |
| **NUM** | N√∫mero | uno, 42 | one, 42 |

### NLTK vs spaCy Tags

**NLTK (Penn Treebank)**:
- Solo ingl√©s
- Etiquetas espec√≠ficas: NN, NNS, VB, VBD, JJ, etc.
- M√°s de 36 etiquetas

**spaCy (Universal Dependencies)**:
- Multiidioma
- Etiquetas universales: 17 categor√≠as principales
- Consistente entre idiomas

## üí° Tips

1. **spaCy es mejor para producci√≥n** (m√°s r√°pido y preciso)
2. **Usa `token.pos_` para etiquetas universales** (NOUN, VERB)
3. **Usa `token.tag_` para etiquetas espec√≠ficas** (NN, VB, etc.)
4. **Filtra por m√∫ltiples POS** con `filter_by_pos()`

## üöÄ Casos de Uso

### Extracci√≥n de informaci√≥n
```python
# Obtener todos los nombres propios
nouns = get_nouns("Mar√≠a vive en Madrid")
# ['Mar√≠a', 'Madrid']
```

### An√°lisis de estilo
```python
# Verificar densidad de adjetivos
structure = analyze_sentence_structure(text)
adj_ratio = structure.get('ADJ', 0) / sum(structure.values())
```

### Simplificaci√≥n de texto
```python
# Mantener solo contenido importante
important = filter_by_pos(text, ['NOUN', 'VERB', 'ADJ'])
```

## üöÄ Siguiente Paso

Una vez completo, ve al **Koan 04: Named Entity Recognition**!
