> ** Translation Note**: This file is currently in Spanish. English translation coming soon!
> For now, you can use a translator or refer to the code examples which are language-agnostic.
> Want to help translate? See [CONTRIBUTING.md](../../CONTRIBUTING.md)

---

# üí° Pistas para Koan 05: Text Classification

## üéØ Objetivo del Koan

Aprender a **clasificar textos** usando Machine Learning:
- Convertir texto a n√∫meros (vectorizaci√≥n)
- Entrenar clasificadores
- Evaluar modelos
- Predecir categor√≠as de nuevos textos

---

## üìù Funci√≥n 1: `vectorize_text_tfidf()`

### Nivel 1: Concepto
TF-IDF (Term Frequency-Inverse Document Frequency) convierte texto en vectores num√©ricos.

### Nivel 2: Implementaci√≥n
```python
from sklearn.feature_extraction.text import TfidfVectorizer
vectorizer = TfidfVectorizer(max_features=max_features)
# fit_transform para entrenar y transformar
# transform para solo transformar
```

### Nivel 3: Casi la soluci√≥n
```python
if train:
    return vectorizer.fit_transform(texts), vectorizer
else:
    return vectorizer.transform(texts)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def vectorize_text_tfidf(texts: List[str], max_features: int = 1000, 
                         vectorizer=None, train: bool = True):
    from sklearn.feature_extraction.text import TfidfVectorizer
    
    if train:
        vectorizer = TfidfVectorizer(max_features=max_features)
        X = vectorizer.fit_transform(texts)
        return X, vectorizer
    else:
        if vectorizer is None:
            raise ValueError("Vectorizer is required when train=False")
        return vectorizer.transform(texts)
```
</details>

---

## üìù Funci√≥n 2: `train_naive_bayes()`

### Nivel 1: Concepto
Naive Bayes es un clasificador probabil√≠stico simple pero efectivo para texto.

### Nivel 2: Implementaci√≥n
```python
from sklearn.naive_bayes import MultinomialNB
classifier = MultinomialNB()
# Usa fit(X, y) para entrenar
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def train_naive_bayes(X_train, y_train):
    from sklearn.naive_bayes import MultinomialNB
    classifier = MultinomialNB()
    classifier.fit(X_train, y_train)
    return classifier
```
</details>

---

## üìù Funci√≥n 3: `train_logistic_regression()`

### Nivel 1: Concepto
Logistic Regression es un clasificador lineal que funciona bien para texto.

### Nivel 2: Implementaci√≥n
```python
from sklearn.linear_model import LogisticRegression
classifier = LogisticRegression(max_iter=max_iter)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def train_logistic_regression(X_train, y_train, max_iter: int = 1000):
    from sklearn.linear_model import LogisticRegression
    classifier = LogisticRegression(max_iter=max_iter)
    classifier.fit(X_train, y_train)
    return classifier
```
</details>

---

## üìù Funci√≥n 4: `predict_text_class()`

### Nivel 1: Concepto
Usa el modelo entrenado para predecir la clase de nuevos textos.

### Nivel 2: Pasos
1. Vectoriza los textos con el vectorizer
2. Usa classifier.predict() para obtener predicciones

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def predict_text_class(texts: List[str], vectorizer, classifier) -> List[str]:
    X = vectorizer.transform(texts)
    return classifier.predict(X).tolist()
```
</details>

---

## üìù Funci√≥n 5: `evaluate_classifier()`

### Nivel 1: Concepto
Calcula m√©tricas de rendimiento: accuracy, precision, recall, f1-score.

### Nivel 2: Implementaci√≥n
```python
from sklearn.metrics import accuracy_score, classification_report
accuracy = accuracy_score(y_true, y_pred)
report = classification_report(y_true, y_pred, output_dict=True)
```

### Nivel 3: Casi la soluci√≥n
```python
accuracy = accuracy_score(y_true, y_pred)
report_dict = classification_report(y_true, y_pred, output_dict=True)
return {
    "accuracy": accuracy,
    "precision": report_dict["weighted avg"]["precision"],
    "recall": report_dict["weighted avg"]["recall"],
    "f1_score": report_dict["weighted avg"]["f1-score"]
}
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def evaluate_classifier(y_true: List[str], y_pred: List[str]) -> dict:
    from sklearn.metrics import accuracy_score, classification_report
    
    accuracy = accuracy_score(y_true, y_pred)
    report = classification_report(y_true, y_pred, output_dict=True)
    
    return {
        "accuracy": accuracy,
        "precision": report["weighted avg"]["precision"],
        "recall": report["weighted avg"]["recall"],
        "f1_score": report["weighted avg"]["f1-score"]
    }
```
</details>

---

## üìù Funci√≥n 6: `create_text_classifier_pipeline()`

### Nivel 1: Concepto
Pipeline combina vectorizaci√≥n y clasificaci√≥n en un solo objeto.

### Nivel 2: Implementaci√≥n
```python
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=1000)),
    ('classifier', MultinomialNB())
])
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def create_text_classifier_pipeline(classifier_type: str = "naive_bayes", 
                                     max_features: int = 1000):
    from sklearn.pipeline import Pipeline
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.linear_model import LogisticRegression
    
    if classifier_type == "naive_bayes":
        classifier = MultinomialNB()
    elif classifier_type == "logistic_regression":
        classifier = LogisticRegression(max_iter=1000)
    else:
        raise ValueError(f"Unknown classifier type: {classifier_type}")
    
    pipeline = Pipeline([
        ('vectorizer', TfidfVectorizer(max_features=max_features)),
        ('classifier', classifier)
    ])
    
    return pipeline
```
</details>

---

## üìù Funci√≥n 7: `get_feature_importance()`

### Nivel 1: Concepto
Identifica las palabras m√°s importantes para cada clase.

### Nivel 2: Pasos
1. Obt√©n los nombres de features del vectorizer
2. Obt√©n los coeficientes del clasificador
3. Encuentra los top N features con mayor/menor coeficiente

### Nivel 3: Casi la soluci√≥n
```python
import numpy as np
feature_names = vectorizer.get_feature_names_out()
coef = classifier.coef_[class_idx]
top_indices = np.argsort(coef)[-top_n:][::-1]
return [feature_names[i] for i in top_indices]
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def get_feature_importance(vectorizer, classifier, class_idx: int = 0, 
                           top_n: int = 10) -> List[str]:
    import numpy as np
    
    feature_names = vectorizer.get_feature_names_out()
    
    if hasattr(classifier, 'coef_'):
        coef = classifier.coef_[class_idx]
        top_indices = np.argsort(coef)[-top_n:][::-1]
        return [feature_names[i] for i in top_indices]
    else:
        raise ValueError("Classifier does not have coefficients (coef_)")
```
</details>

---

## üéØ Conceptos Clave

### Pipeline de Clasificaci√≥n de Texto

```
Texto crudo ‚Üí Vectorizaci√≥n ‚Üí Modelo ML ‚Üí Predicci√≥n
```

1. **Vectorizaci√≥n**: Convertir texto a n√∫meros
   - TF-IDF: Frecuencia de t√©rminos con peso por rareza
   - Count Vectorizer: Solo frecuencias
   - Word Embeddings: Vectores sem√°nticos

2. **Clasificadores**:
   - Naive Bayes: R√°pido, simple, bueno para texto
   - Logistic Regression: Interpretable, robusto
   - SVM: Bueno para alta dimensionalidad
   - Random Forest: Ensemble, no lineal

3. **M√©tricas de Evaluaci√≥n**:
   - **Accuracy**: % de predicciones correctas
   - **Precision**: % de positivos correctos
   - **Recall**: % de positivos encontrados
   - **F1-Score**: Media arm√≥nica de precision y recall

### TF-IDF Explicado

**TF (Term Frequency)**: Cu√°ntas veces aparece una palabra en un documento

**IDF (Inverse Document Frequency)**: Qu√© tan rara es una palabra en el corpus

```
TF-IDF = TF √ó IDF
```

**Palabras importantes**: Alta frecuencia en pocos documentos
**Palabras sin importancia**: Alta frecuencia en todos los documentos (stop words)

## üí° Tips Pr√°cticos

### 1. Preprocesamiento es clave
```python
# Mejora la clasificaci√≥n
texts = [text.lower().strip() for text in texts]
```

### 2. Ajusta max_features
```python
# Pocos features = r√°pido pero menos preciso
vectorizer = TfidfVectorizer(max_features=100)

# Muchos features = lento pero m√°s preciso
vectorizer = TfidfVectorizer(max_features=10000)
```

### 3. Usa Pipeline para evitar errores
```python
# Malo: f√°cil equivocarse
X_train, vectorizer = vectorize_text_tfidf(train_texts)
X_test = vectorizer.transform(test_texts)
classifier.fit(X_train, y_train)

# Bueno: pipeline maneja todo
pipeline = create_text_classifier_pipeline()
pipeline.fit(train_texts, y_train)
pipeline.predict(test_texts)
```

### 4. Split train/test correctamente
```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(
    texts, labels, test_size=0.2, random_state=42
)
```

## üöÄ Casos de Uso

### Clasificaci√≥n de emails (spam/no spam)
```python
pipeline = create_text_classifier_pipeline("naive_bayes")
pipeline.fit(emails, labels)  # labels = ["spam", "ham", ...]
prediction = pipeline.predict(["Click here to win $1000!!!"])
```

### An√°lisis de sentimientos b√°sico
```python
texts = ["Me encanta", "Es horrible"]
labels = ["positivo", "negativo"]
pipeline.fit(texts, labels)
```

### Categorizaci√≥n de noticias
```python
categories = ["deportes", "pol√≠tica", "tecnolog√≠a"]
pipeline = create_text_classifier_pipeline("logistic_regression")
```

## üîß Troubleshooting

### Problema: Accuracy muy baja
**Soluci√≥n**: 
- M√°s datos de entrenamiento
- Mejor preprocesamiento
- Probar otros clasificadores

### Problema: Overfitting
**Soluci√≥n**:
- Reducir max_features
- Regularizaci√≥n en LogisticRegression
- M√°s datos de entrenamiento

### Problema: Predicciones sesgadas
**Soluci√≥n**:
- Balancear clases en entrenamiento
- Usar class_weight='balanced'

## üöÄ Siguiente Paso

Una vez completo, ve al **Koan 06: Sentiment Analysis**!
