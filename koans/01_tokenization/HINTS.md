> ** Translation Note**: This file is currently in Spanish. English translation coming soon!
> For now, you can use a translator or refer to the code examples which are language-agnostic.
> Want to help translate? See [CONTRIBUTING.md](../../CONTRIBUTING.md)

---

# üí° Pistas para Koan 01: Tokenizaci√≥n

## üìù Funci√≥n 1: `tokenize_words_nltk()`

### Nivel 1: Pista General
Necesitas importar y usar la funci√≥n `word_tokenize` de NLTK.

### Nivel 2: Pista Espec√≠fica
```python
from nltk.tokenize import word_tokenize
# Usa word_tokenize(text) y ret√≥rnalo
```

### Nivel 3: Casi la Soluci√≥n
```python
def tokenize_words_nltk(text: str) -> List[str]:
    from nltk.tokenize import word_tokenize
    tokens = word_tokenize(text)
    return tokens
```

### ‚úÖ Soluci√≥n Completa
<details>
<summary>Click para ver la soluci√≥n (¬°intenta resolverlo primero!)</summary>

```python
def tokenize_words_nltk(text: str) -> List[str]:
    from nltk.tokenize import word_tokenize
    return word_tokenize(text)
```
</details>

---

## üìù Funci√≥n 2: `tokenize_sentences_nltk()`

### Nivel 1: Pista General
Similar a `word_tokenize`, pero existe `sent_tokenize` para oraciones.

### Nivel 2: Pista Espec√≠fica
```python
from nltk.tokenize import sent_tokenize
# √ösalo igual que word_tokenize pero para oraciones
```

### ‚úÖ Soluci√≥n Completa
<details>
<summary>Click para ver la soluci√≥n</summary>

```python
def tokenize_sentences_nltk(text: str) -> List[str]:
    from nltk.tokenize import sent_tokenize
    return sent_tokenize(text)
```
</details>

---

## üìù Funci√≥n 3: `tokenize_words_spacy()`

### Nivel 1: Pista General
spaCy requiere:
1. Cargar un modelo (espa√±ol o ingl√©s)
2. Procesar el texto
3. Extraer tokens

### Nivel 2: Pista Espec√≠fica
```python
import spacy
# Carga modelo: "es_core_news_sm" para espa√±ol, "en_core_web_sm" para ingl√©s
nlp = spacy.load(modelo)
doc = nlp(text)
# Extrae tokens: [token.text for token in doc]
```

### Nivel 3: Casi la Soluci√≥n
```python
def tokenize_words_spacy(text: str, lang: str = "es") -> List[str]:
    import spacy
    model = "es_core_news_sm" if lang == "es" else "en_core_web_sm"
    nlp = spacy.load(model)
    doc = nlp(text)
    return [token.text for token in doc]
```

### ‚úÖ Soluci√≥n Completa
<details>
<summary>Click para ver la soluci√≥n</summary>

```python
def tokenize_words_spacy(text: str, lang: str = "es") -> List[str]:
    import spacy
    model_name = "es_core_news_sm" if lang == "es" else "en_core_web_sm"
    nlp = spacy.load(model_name)
    doc = nlp(text)
    return [token.text for token in doc]
```
</details>

---

## üìù Funci√≥n 4: `custom_tokenize()`

### Nivel 1: Pista General
Usa el m√©todo `.split()` de strings de Python.

### Nivel 2: Pista Espec√≠fica
```python
# text.split(delimitador) separa por el delimitador
return text.split(delimiter)
```

### ‚úÖ Soluci√≥n Completa
<details>
<summary>Click para ver la soluci√≥n</summary>

```python
def custom_tokenize(text: str, delimiter: str = " ") -> List[str]:
    return text.split(delimiter)
```
</details>

---

## üìù Funci√≥n 5: `count_tokens()`

### Nivel 1: Pista General
Necesitas:
1. Tokenizar el texto
2. Convertir a min√∫sculas
3. Contar frecuencias

### Nivel 2: Pista Espec√≠fica
```python
from collections import Counter
tokens = tokenize_words_nltk(text)
# Convierte a min√∫sculas: [t.lower() for t in tokens]
# Usa Counter para contar
```

### ‚úÖ Soluci√≥n Completa
<details>
<summary>Click para ver la soluci√≥n</summary>

```python
def count_tokens(text: str) -> dict:
    from collections import Counter
    tokens = tokenize_words_nltk(text)
    tokens_lower = [token.lower() for token in tokens]
    return dict(Counter(tokens_lower))
```
</details>

---

## üìù Funci√≥n 6: `remove_punctuation_tokens()`

### Nivel 1: Pista General
Usa `string.punctuation` que contiene todos los signos de puntuaci√≥n.

### Nivel 2: Pista Espec√≠fica
```python
import string
# string.punctuation = '!"#$%&\'()*+,-./:;<=>?@[\\]^_`{|}~'
# Filtra tokens que NO est√©n en string.punctuation
```

### Nivel 3: Casi la Soluci√≥n
```python
import string
return [token for token in tokens if token not in string.punctuation]
```

### ‚úÖ Soluci√≥n Completa
<details>
<summary>Click para ver la soluci√≥n</summary>

```python
def remove_punctuation_tokens(tokens: List[str]) -> List[str]:
    import string
    return [token for token in tokens if token not in string.punctuation]
```
</details>

---

## üéØ Consejos Generales

1. **Ejecuta los tests frecuentemente**: `pytest test_tokenization.py -v`
2. **Lee los mensajes de error**: Te dicen exactamente qu√© falta
3. **Usa print() para debug**: Imprime resultados intermedios
4. **Consulta la documentaci√≥n**:
   - NLTK: https://www.nltk.org/
   - spaCy: https://spacy.io/

## üöÄ Siguiente Paso

Una vez que todos los tests pasen, ve al **Koan 02: Stemming y Lemmatization**!
