> ** Translation Note**: This file is currently in Spanish. English translation coming soon!
> For now, you can use a translator or refer to the code examples which are language-agnostic.
> Want to help translate? See [CONTRIBUTING.md](../../CONTRIBUTING.md)

---

# TeorÃ­a: TokenizaciÃ³n

## ğŸ“š Tabla de Contenidos
1. [IntroducciÃ³n a la TokenizaciÃ³n](#introducciÃ³n)
2. [Tipos de TokenizaciÃ³n](#tipos)
3. [TokenizaciÃ³n en Diferentes Idiomas](#idiomas)
4. [Herramientas y Bibliotecas](#herramientas)
5. [Casos Especiales](#casos-especiales)
6. [TokenizaciÃ³n Moderna](#moderna)
7. [Casos de Uso](#casos-uso)

---

## ğŸ¯ IntroducciÃ³n a la TokenizaciÃ³n {#introducciÃ³n}

### Â¿QuÃ© es la TokenizaciÃ³n?

La **tokenizaciÃ³n** es el proceso de dividir texto en unidades mÃ¡s pequeÃ±as llamadas **tokens**. Es el primer paso fundamental en prÃ¡cticamente cualquier pipeline de procesamiento de lenguaje natural (NLP).

```
Texto: "Hello, world! How are you?"

Tokens: ["Hello", ",", "world", "!", "How", "are", "you", "?"]
```

### Â¿Por quÃ© es Importante?

**1. Unidad BÃ¡sica de Procesamiento**
```python
# Las computadoras necesitan unidades discretas para trabajar
texto = "I love Python"

# âŒ DifÃ­cil de procesar como string completo
# âœ… FÃ¡cil de procesar como lista de palabras
tokens = ["I", "love", "Python"]
```

**2. Base para AnÃ¡lisis Posterior**
```
TokenizaciÃ³n â†’ POS Tagging â†’ NER â†’ Parsing â†’ ...
     â†‘
  Primer paso esencial
```

**3. Impacto en Calidad**
```python
# Mala tokenizaciÃ³n
"don't" â†’ ["don", "'", "t"]  # âŒ Pierde significado

# Buena tokenizaciÃ³n
"don't" â†’ ["do", "n't"]  # âœ… Preserva estructura gramatical
# O alternativamente:
"don't" â†’ ["don't"]  # âœ… Mantiene como unidad
```

### DesafÃ­os

**AmbigÃ¼edad de LÃ­mites:**
```
"New York" â†’ Â¿["New", "York"] o ["New York"]?
"Ph.D." â†’ Â¿["Ph", ".", "D", "."] o ["Ph.D."]?
"rock'n'roll" â†’ Â¿["rock", "'", "n", "'", "roll"] o ["rock'n'roll"]?
```

**Diferencias Entre Idiomas:**
```
InglÃ©s: "I love NLP" â†’ ["I", "love", "NLP"] âœ… (separados por espacios)
Chino: "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†" â†’ Â¿? (sin espacios explÃ­citos)
AlemÃ¡n: "Donaudampfschifffahrtsgesellschaft" â†’ Â¿una palabra o varias?
```

---

## ğŸ“ Tipos de TokenizaciÃ³n {#tipos}

### 1. Word Tokenization (TokenizaciÃ³n por Palabras)

La forma mÃ¡s comÃºn: dividir texto en palabras.

**MÃ©todo Ingenuo:**
```python
# âŒ Demasiado simple
text = "Hello, world!"
tokens = text.split()  # ["Hello,", "world!"]
# Problema: puntuaciÃ³n pegada a palabras
```

**MÃ©todo con Regex:**
```python
import re

text = "Hello, world! How are you?"
tokens = re.findall(r'\w+|[^\w\s]', text)
# ["Hello", ",", "world", "!", "How", "are", "you", "?"]
```

**NLTK Word Tokenizer:**
```python
from nltk.tokenize import word_tokenize

text = "Hello, world! Don't worry."
tokens = word_tokenize(text)
# ["Hello", ",", "world", "!", "Do", "n't", "worry", "."]
```

**spaCy Tokenizer:**
```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Hello, world! Don't worry.")
tokens = [token.text for token in doc]
# ["Hello", ",", "world", "!", "Do", "n't", "worry", "."]
```

### 2. Sentence Tokenization (TokenizaciÃ³n por Oraciones)

Dividir texto en oraciones.

**DesafÃ­o:**
```python
text = "Dr. Smith works at U.S.A. Inc. He loves NLP."
# Â¿DÃ³nde terminan las oraciones?
# "Dr." no es fin de oraciÃ³n
# "U.S.A." tampoco
# "Inc." tampoco
# Solo despuÃ©s de "NLP." es fin de oraciÃ³n
```

**NLTK Sentence Tokenizer:**
```python
from nltk.tokenize import sent_tokenize

text = "Dr. Smith works at U.S.A. Inc. He loves NLP."
sentences = sent_tokenize(text)
# ["Dr. Smith works at U.S.A. Inc.", "He loves NLP."]
```

**spaCy Sentence Segmentation:**
```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Dr. Smith works at U.S.A. Inc. He loves NLP.")
sentences = [sent.text for sent in doc.sents]
# ["Dr. Smith works at U.S.A. Inc.", "He loves NLP."]
```

### 3. Character Tokenization

Dividir en caracteres individuales.

```python
text = "Hello"
tokens = list(text)
# ["H", "e", "l", "l", "o"]
```

**CuÃ¡ndo usar:**
- Modelos de generaciÃ³n de texto
- OCR (reconocimiento Ã³ptico de caracteres)
- AnÃ¡lisis morfolÃ³gico detallado

### 4. Subword Tokenization

Dividir en subpalabras (entre caracteres y palabras completas).

**Problema que Resuelve:**
```python
# Vocabulario limitado con palabras completas
vocab = {"cat", "dog", "run", "running"}
# Â¿QuÃ© hacer con "cats", "dogs", "runner"? âŒ No estÃ¡n en vocabulario

# Con subword tokenization
vocab = {"cat", "dog", "run", "ning", "s", "er"}
"cats" â†’ ["cat", "s"] âœ…
"running" â†’ ["run", "ning"] âœ…
"runner" â†’ ["run", "er"] âœ…
```

**BPE (Byte-Pair Encoding):**
```python
from tokenizers import Tokenizer
from tokenizers.models import BPE

tokenizer = Tokenizer(BPE())

# Ejemplo de tokens
"unhappiness" â†’ ["un", "happiness"]
"unbelievable" â†’ ["un", "believ", "able"]
```

**WordPiece (BERT):**
```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
tokens = tokenizer.tokenize("unhappiness")
# ["un", "##happi", "##ness"]
# "##" indica continuaciÃ³n de palabra
```

**SentencePiece:**
```python
import sentencepiece as spm

# Usado por modelos como T5, XLNet
sp = spm.SentencePieceProcessor()
sp.load('model.model')
tokens = sp.encode_as_pieces('unhappiness')
# ["â–un", "happiness"]
# "â–" indica inicio de palabra
```

### Comparativa de MÃ©todos

| MÃ©todo | Granularidad | Vocabulario | Uso Principal | Ventajas |
|--------|--------------|-------------|---------------|----------|
| **Word** | Palabras completas | Grande | NLP clÃ¡sico | Interpretable |
| **Character** | Caracteres | PequeÃ±o (~100) | GeneraciÃ³n | Sin OOV |
| **Subword** | Fragmentos | Medio (10k-50k) | Transformers | Balance |

**OOV = Out Of Vocabulary (palabras desconocidas)**

---

## ğŸŒ TokenizaciÃ³n en Diferentes Idiomas {#idiomas}

### InglÃ©s

**CaracterÃ­sticas:**
- âœ… Espacios separan palabras claramente
- âš ï¸ Contracciones: "don't", "I'm", "we'll"
- âš ï¸ Compuestos con guiÃ³n: "state-of-the-art"
- âš ï¸ Abreviaturas: "Dr.", "U.S.A."

**Ejemplo:**
```python
text = "I'm learning state-of-the-art NLP at Dr. Smith's lab."
tokens = word_tokenize(text)
# ["I", "'m", "learning", "state-of-the-art", "NLP", "at", 
#  "Dr.", "Smith", "'s", "lab", "."]
```

### EspaÃ±ol

**CaracterÃ­sticas:**
- âœ… Similar al inglÃ©s (espacios como separadores)
- âš ï¸ Contracciones: "del" (de+el), "al" (a+el)
- âš ï¸ Acentos: "estÃ¡n", "nÃºmero", "dÃ­a"
- âš ï¸ InterrogaciÃ³n/ExclamaciÃ³n: "Â¿CÃ³mo estÃ¡s?"

**Ejemplo:**
```python
import spacy

nlp = spacy.load("es_core_news_sm")
doc = nlp("Â¿CÃ³mo estÃ¡s? Voy al mercado.")
tokens = [token.text for token in doc]
# ["Â¿", "CÃ³mo", "estÃ¡s", "?", "Voy", "al", "mercado", "."]
```

**Nota:** `"al"` puede mantenerse como un token o dividirse en `["a", "el"]` dependiendo del objetivo.

### Chino

**CaracterÃ­sticas:**
- âŒ Sin espacios entre palabras
- âš ï¸ Cada carÃ¡cter puede ser una palabra o parte de una
- âš ï¸ Requiere diccionarios o modelos ML

**Ejemplo:**
```python
import jieba  # Biblioteca popular para chino

text = "æˆ‘çˆ±è‡ªç„¶è¯­è¨€å¤„ç†"
tokens = jieba.cut(text)
# ["æˆ‘", "çˆ±", "è‡ªç„¶è¯­è¨€", "å¤„ç†"]
# "æˆ‘" = yo
# "çˆ±" = amo
# "è‡ªç„¶è¯­è¨€" = lenguaje natural
# "å¤„ç†" = procesamiento
```

### AlemÃ¡n

**CaracterÃ­sticas:**
- âš ï¸ Palabras compuestas largas
- âš ï¸ "Fusswegpulverisierer" = pisador-de-caminos-pulverizador

**Ejemplo:**
```python
text = "Donaudampfschifffahrtsgesellschaft"
# Palabra compuesta: Danubio-vapor-navegaciÃ³n-compaÃ±Ã­a

# TokenizaciÃ³n simple
tokens = [text]  # ["Donaudampfschifffahrtsgesellschaft"]

# TokenizaciÃ³n con descomposiciÃ³n
tokens = ["Donau", "dampf", "schiff", "fahrt", "gesellschaft"]
```

### JaponÃ©s

**CaracterÃ­sticas:**
- âŒ Sin espacios
- âš ï¸ Mezcla de 3 sistemas: Hiragana, Katakana, Kanji

**Ejemplo:**
```python
import fugashi  # Tokenizer japonÃ©s

text = "ç§ã¯æ—¥æœ¬èªã‚’å‹‰å¼·ã—ã¾ã™"
tagger = fugashi.Tagger()
tokens = [word.surface for word in tagger(text)]
# ["ç§", "ã¯", "æ—¥æœ¬èª", "ã‚’", "å‹‰å¼·", "ã—ã¾ã™"]
```

---

## ğŸ› ï¸ Herramientas y Bibliotecas {#herramientas}

### 1. NLTK (Natural Language Toolkit)

**CaracterÃ­sticas:**
- ğŸ“š Educacional y completo
- ğŸ¢ MÃ¡s lento
- ğŸ¯ Bueno para aprendizaje

**Word Tokenization:**
```python
from nltk.tokenize import word_tokenize

text = "Hello, world!"
tokens = word_tokenize(text)
```

**Sentence Tokenization:**
```python
from nltk.tokenize import sent_tokenize

text = "Hello! How are you? I'm fine."
sentences = sent_tokenize(text)
# ["Hello!", "How are you?", "I'm fine."]
```

**Otros Tokenizers:**
```python
from nltk.tokenize import (
    WordPunctTokenizer,
    TweetTokenizer,
    MWETokenizer
)

# WordPunctTokenizer: separa toda puntuaciÃ³n
tokenizer = WordPunctTokenizer()
tokenizer.tokenize("Don't worry!")
# ["Don", "'", "t", "worry", "!"]

# TweetTokenizer: para redes sociales
tokenizer = TweetTokenizer()
tokenizer.tokenize("@user Love #NLP! ğŸ˜Š")
# ["@user", "Love", "#NLP", "!", "ğŸ˜Š"]

# MWETokenizer: multi-word expressions
tokenizer = MWETokenizer([("New", "York"), ("San", "Francisco")])
tokenizer.tokenize(["I", "live", "in", "New", "York"])
# ["I", "live", "in", "New_York"]
```

### 2. spaCy

**CaracterÃ­sticas:**
- âš¡ Muy rÃ¡pido (Cython)
- ğŸ­ Orientado a producciÃ³n
- ğŸ§  Incluye modelos pre-entrenados

**TokenizaciÃ³n BÃ¡sica:**
```python
import spacy

nlp = spacy.load("en_core_web_sm")
doc = nlp("Apple is looking at buying U.K. startup for $1 billion")

for token in doc:
    print(token.text, token.lemma_, token.pos_, token.is_stop)
```

**Ventajas:**
```python
# Maneja casos especiales automÃ¡ticamente
doc = nlp("We're here at 9 a.m. in the U.S.A.")
# "We", "'re", "here", "at", "9", "a.m.", "in", "the", "U.S.A.", "."
```

**PersonalizaciÃ³n:**
```python
from spacy.tokenizer import Tokenizer
from spacy.util import compile_infix_regex

# AÃ±adir reglas personalizadas
def custom_tokenizer(nlp):
    inf = list(nlp.Defaults.infixes)
    inf.remove(r"(?<=[0-9])[+\-\*^](?=[0-9-])")
    infix_re = compile_infix_regex(inf)
    return Tokenizer(nlp.vocab, infix_finditer=infix_re.finditer)

nlp.tokenizer = custom_tokenizer(nlp)
```

### 3. Transformers (Hugging Face)

**Para modelos modernos:**
```python
from transformers import AutoTokenizer

# BERT
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
tokens = tokenizer.tokenize("Hello, world!")
# ['hello', ',', 'world', '!']

# GPT-2
tokenizer = AutoTokenizer.from_pretrained("gpt2")
tokens = tokenizer.tokenize("Hello, world!")
# ['Hello', ',', 'Ä world', '!']
# 'Ä ' representa espacio

# Encoding completo (tokens â†’ IDs)
encoded = tokenizer("Hello, world!", return_tensors="pt")
# {'input_ids': tensor([[...]])}
```

### 4. Stanza (Stanford NLP)

**CaracterÃ­sticas:**
- ğŸŒ 70+ idiomas
- ğŸ“ AcadÃ©mico (Stanford)
- ğŸ”¬ Alta precisiÃ³n

```python
import stanza

stanza.download('en')
nlp = stanza.Pipeline('en')
doc = nlp("Barack Obama was born in Hawaii.")

for sentence in doc.sentences:
    for token in sentence.tokens:
        print(token.text)
```

### Comparativa de Performance

| Biblioteca | Velocidad | PrecisiÃ³n | Idiomas | Uso |
|------------|-----------|-----------|---------|-----|
| **NLTK** | ğŸ¢ Lento | â­â­â­ | ~40 | EducaciÃ³n |
| **spaCy** | âš¡âš¡âš¡ RÃ¡pido | â­â­â­â­ | ~60 | ProducciÃ³n |
| **Transformers** | âš¡âš¡ Medio | â­â­â­â­â­ | 100+ | Deep Learning |
| **Stanza** | âš¡ Medio | â­â­â­â­â­ | 70+ | Academia |

---

## ğŸ”§ Casos Especiales {#casos-especiales}

### 1. Contracciones

**InglÃ©s:**
```python
from nltk.tokenize import word_tokenize

contractions = [
    "don't",    # do + n't
    "I'm",      # I + 'm (am)
    "we'll",    # we + 'll (will)
    "wouldn't", # would + n't
    "it's"      # it + 's (is/has)
]

for word in contractions:
    print(word, "â†’", word_tokenize(word))

# don't â†’ ['do', "n't"]
# I'm â†’ ['I', "'m"]
# we'll â†’ ['we', "'ll"]
```

**EspaÃ±ol:**
```python
# "del" = de + el
# "al" = a + el
text = "Voy al mercado del pueblo"
# OpciÃ³n 1: mantener como tokens
# ["Voy", "al", "mercado", "del", "pueblo"]
# OpciÃ³n 2: expandir
# ["Voy", "a", "el", "mercado", "de", "el", "pueblo"]
```

### 2. NÃºmeros y Fechas

```python
examples = [
    "3.14",           # nÃºmero decimal
    "1,000",          # mil con coma
    "01/15/2024",     # fecha
    "$100.50",        # dinero
    "10:30",          # hora
    "+1-555-1234",    # telÃ©fono
]

for ex in examples:
    tokens = word_tokenize(ex)
    print(f"{ex} â†’ {tokens}")

# 3.14 â†’ ['3.14']
# 1,000 â†’ ['1,000']
# 01/15/2024 â†’ ['01/15/2024']
# $100.50 â†’ ['$', '100.50']
```

### 3. URLs y Emails

```python
from nltk.tokenize import TweetTokenizer

tokenizer = TweetTokenizer()

text = "Visit https://example.com or email user@example.com"
tokens = tokenizer.tokenize(text)
# ['Visit', 'https://example.com', 'or', 'email', 'user@example.com']
```

### 4. Hashtags y Mentions

```python
from nltk.tokenize import TweetTokenizer

tokenizer = TweetTokenizer()

text = "@user1 Check out #NLP and #DeepLearning! ğŸš€"
tokens = tokenizer.tokenize(text)
# ['@user1', 'Check', 'out', '#NLP', 'and', '#DeepLearning', '!', 'ğŸš€']
```

### 5. Emojis

```python
import emoji

text = "I love Python! ğŸ˜ğŸ"

# OpciÃ³n 1: Mantener emojis
tokens = word_tokenize(text)
# ['I', 'love', 'Python', '!', 'ğŸ˜', 'ğŸ']

# OpciÃ³n 2: Convertir a texto
text_with_emoji = emoji.demojize(text)
# "I love Python! :smiling_face_with_heart-eyes::snake:"
```

### 6. Abreviaturas

```python
text = "Dr. Smith works at NASA, U.S.A."

# NLTK maneja bien
tokens = word_tokenize(text)
# ['Dr.', 'Smith', 'works', 'at', 'NASA', ',', 'U.S.A', '.']
```

---

## ğŸš€ TokenizaciÃ³n Moderna {#moderna}

### Subword Tokenization en Transformers

**Â¿Por quÃ© Subword?**

```python
# Problema: Vocabulario infinito
palabras_posibles = infinitas  # "run", "running", "runner", "runs", ...

# SoluciÃ³n: Subword units
subwords = {"run", "ning", "er", "s"}
"running" â†’ ["run", "ning"]
"runner" â†’ ["run", "er"]
"runs" â†’ ["run", "s"]
```

**Ventajas:**
1. âœ… Vocabulario finito pero flexible
2. âœ… Maneja palabras desconocidas (OOV)
3. âœ… Captura morfologÃ­a

### BPE (Byte-Pair Encoding)

Usado por: GPT-2, GPT-3, RoBERTa

**Algoritmo:**
```
1. Empezar con caracteres individuales
2. Encontrar el par mÃ¡s frecuente
3. Fusionar ese par en un nuevo sÃ­mbolo
4. Repetir hasta llegar al tamaÃ±o de vocabulario deseado
```

**Ejemplo:**
```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

text = "unhappiness"
tokens = tokenizer.tokenize(text)
# ['un', 'happiness']

text = "unbelievable"
tokens = tokenizer.tokenize(text)
# ['un', 'bel', 'iev', 'able']
```

### WordPiece

Usado por: BERT, DistilBERT

**Diferencia con BPE:**
- BPE: fusiona el par mÃ¡s frecuente
- WordPiece: fusiona el par que maximiza likelihood del corpus

**Ejemplo:**
```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

text = "unhappiness"
tokens = tokenizer.tokenize(text)
# ['un', '##hap', '##pi', '##ness']
# "##" indica continuaciÃ³n de palabra

text = "playing"
tokens = tokenizer.tokenize(text)
# ['playing']  # En vocabulario como palabra completa
```

### SentencePiece

Usado por: T5, ALBERT, XLNet

**CaracterÃ­sticas:**
- âœ… No requiere pre-tokenizaciÃ³n
- âœ… Funciona directamente en texto raw
- âœ… Ideal para idiomas sin espacios

**Ejemplo:**
```python
from transformers import T5Tokenizer

tokenizer = T5Tokenizer.from_pretrained("t5-small")

text = "Hello world"
tokens = tokenizer.tokenize(text)
# ['â–Hello', 'â–world']
# "â–" representa espacio
```

### Tokens Especiales

```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

# Tokens especiales
print(tokenizer.cls_token)      # [CLS] - clasificaciÃ³n
print(tokenizer.sep_token)      # [SEP] - separador
print(tokenizer.pad_token)      # [PAD] - padding
print(tokenizer.mask_token)     # [MASK] - masked LM
print(tokenizer.unk_token)      # [UNK] - unknown

# Encoding con tokens especiales
encoded = tokenizer("Hello", "World")
print(tokenizer.convert_ids_to_tokens(encoded['input_ids']))
# ['[CLS]', 'hello', '[SEP]', 'world', '[SEP]']
```

---

## ğŸ’¼ Casos de Uso {#casos-uso}

### 1. AnÃ¡lisis de Sentimientos

```python
# TokenizaciÃ³n â†’ AnÃ¡lisis
text = "I absolutely love this product! It's amazing!"
tokens = word_tokenize(text)

# Contar palabras positivas
positive_words = {"love", "amazing", "excellent"}
sentiment_score = sum(1 for token in tokens if token.lower() in positive_words)
```

### 2. BÃºsqueda y RecuperaciÃ³n de InformaciÃ³n

```python
# Tokenizar documentos y queries
documents = [
    "Python is a programming language",
    "Natural Language Processing with Python",
]

query = "Python programming"

# Tokenizar todo
doc_tokens = [word_tokenize(doc.lower()) for doc in documents]
query_tokens = word_tokenize(query.lower())

# Calcular similitud (simplificado)
```

### 3. Machine Translation

```python
# TokenizaciÃ³n bilingÃ¼e
en_text = "Hello world"
es_text = "Hola mundo"

en_tokens = word_tokenize(en_text)  # ['Hello', 'world']
es_tokens = word_tokenize(es_text)  # ['Hola', 'mundo']

# Alineamiento: Hello â†” Hola, world â†” mundo
```

### 4. Text Normalization

```python
import string

text = "Hello, WORLD! This is AMAZING!!!"

# Tokenizar
tokens = word_tokenize(text)

# Normalizar
tokens = [
    token.lower() 
    for token in tokens 
    if token not in string.punctuation
]
# ['hello', 'world', 'this', 'is', 'amazing']
```

### 5. Feature Extraction para ML

```python
from sklearn.feature_extraction.text import CountVectorizer

# El vectorizer usa tokenizaciÃ³n internamente
vectorizer = CountVectorizer(tokenizer=word_tokenize)

corpus = [
    "I love Python",
    "Python is great",
    "I love programming"
]

X = vectorizer.fit_transform(corpus)
# Matriz de features basada en tokens
```

---

## ğŸ“Š Best Practices

### 1. Elegir el Tokenizer Apropiado

```python
# Para NLP clÃ¡sico (anÃ¡lisis, clasificaciÃ³n)
from nltk.tokenize import word_tokenize  # âœ…

# Para producciÃ³n (velocidad importante)
import spacy  # âœ…

# Para modelos Transformer
from transformers import AutoTokenizer  # âœ…

# Para redes sociales
from nltk.tokenize import TweetTokenizer  # âœ…
```

### 2. Consistencia

```python
# âœ… Usar el mismo tokenizer en train y test
tokenizer = word_tokenize

train_tokens = [tokenizer(text) for text in train_data]
test_tokens = [tokenizer(text) for text in test_data]

# âŒ NO mezclar tokenizers
train_tokens = [word_tokenize(text) for text in train_data]
test_tokens = [spacy_tokenize(text) for text in test_data]  # âŒ
```

### 3. NormalizaciÃ³n

```python
def preprocess(text):
    # 1. Lowercase
    text = text.lower()
    
    # 2. Tokenizar
    tokens = word_tokenize(text)
    
    # 3. Remover puntuaciÃ³n (opcional)
    tokens = [t for t in tokens if t.isalnum()]
    
    # 4. Remover stopwords (opcional)
    from nltk.corpus import stopwords
    stops = set(stopwords.words('english'))
    tokens = [t for t in tokens if t not in stops]
    
    return tokens
```

### 4. Manejo de Casos Especiales

```python
# URLs
text = "Visit https://example.com for more info"
# OpciÃ³n 1: Tokenizar normalmente
# OpciÃ³n 2: Reemplazar URLs con token especial
text = text.replace(r'https?://\S+', '[URL]')

# NÃºmeros
text = "I have 123 apples"
# OpciÃ³n 1: Mantener "123"
# OpciÃ³n 2: Reemplazar con "[NUM]"
```

---

## ğŸ“ Resumen

**Conceptos Clave:**
- TokenizaciÃ³n es el primer paso en NLP
- Diferentes niveles: carÃ¡cter, palabra, subpalabra, oraciÃ³n
- Herramientas: NLTK (educaciÃ³n), spaCy (producciÃ³n), Transformers (DL)
- Subword tokenization (BPE, WordPiece) es estÃ¡ndar en modelos modernos

**Decisiones Importantes:**
1. Â¿QuÃ© nivel de granularidad? (palabra, subpalabra, carÃ¡cter)
2. Â¿QuÃ© hacer con puntuaciÃ³n?
3. Â¿CÃ³mo manejar casos especiales? (URLs, emojis, etc.)
4. Â¿Normalizar o no? (lowercase, eliminar acentos)

**PrÃ³ximos Pasos:**
- **Koan 2**: Stemming y Lemmatization (normalizaciÃ³n avanzada)
- **Koan 3**: POS Tagging (anÃ¡lisis gramatical)
- **Koan 7**: Word Embeddings (representaciones vectoriales)

Â¡La tokenizaciÃ³n es la base de todo en NLP! ğŸš€
