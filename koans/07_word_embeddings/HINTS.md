# üí° Pistas para Koan 07: Word Embeddings

## üéØ Objetivo del Koan

Aprender sobre **representaciones vectoriales de palabras**:
- Vectores densos capturan significado sem√°ntico
- Palabras similares tienen vectores similares
- Operaciones vectoriales revelan relaciones

---

## üìù Funci√≥n 1: `get_word_vector_spacy()`

### Nivel 1: Concepto
spaCy proporciona word vectors pre-entrenados con modelos medianos/grandes.

### Nivel 2: Implementaci√≥n
```python
import spacy
import numpy as np
nlp = spacy.load("en_core_web_md")  # Necesita modelo con vectores
doc = nlp(word)
return doc.vector  # Vector numpy array
```

### Nivel 3: Importante
‚ö†Ô∏è Los modelos `sm` (small) NO tienen vectores. Necesitas `md` o `lg`.

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def get_word_vector_spacy(word: str, lang: str = "en") -> np.ndarray:
    import spacy
    import numpy as np
    
    # Modelos medianos tienen vectores (300 dimensiones)
    model = "en_core_web_md" if lang == "en" else "es_core_news_md"
    nlp = spacy.load(model)
    
    doc = nlp(word)
    return doc.vector
```
</details>

---

## üìù Funci√≥n 2: `cosine_similarity()`

### Nivel 1: Concepto
Mide similitud entre vectores: 1 = id√©nticos, 0 = no relacionados, -1 = opuestos.

### Nivel 2: F√≥rmula
```
similarity = (A ¬∑ B) / (||A|| √ó ||B||)
```

### Nivel 3: Implementaci√≥n
```python
import numpy as np
dot_product = np.dot(vec1, vec2)
norm1 = np.linalg.norm(vec1)
norm2 = np.linalg.norm(vec2)
return dot_product / (norm1 * norm2)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:
    import numpy as np
    
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    
    if norm_vec1 == 0 or norm_vec2 == 0:
        return 0.0
    
    return float(dot_product / (norm_vec1 * norm_vec2))
```
</details>

---

## üìù Funci√≥n 3: `word_similarity()`

### Nivel 1: Concepto
Calcula similitud sem√°ntica entre dos palabras usando sus vectores.

### Nivel 2: Pasos
1. Obt√©n vectores de ambas palabras
2. Calcula cosine_similarity entre ellos

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def word_similarity(word1: str, word2: str, lang: str = "en") -> float:
    vec1 = get_word_vector_spacy(word1, lang)
    vec2 = get_word_vector_spacy(word2, lang)
    return cosine_similarity(vec1, vec2)
```
</details>

---

## üìù Funci√≥n 4: `most_similar_words()`

### Nivel 1: Concepto
Encuentra las palabras m√°s similares a una palabra dada en un vocabulario.

### Nivel 2: Pasos
1. Obt√©n vector de la palabra objetivo
2. Calcula similitud con cada palabra del vocabulario
3. Ordena por similitud y retorna top_n

### Nivel 3: Casi la soluci√≥n
```python
target_vec = get_word_vector_spacy(word, lang)
similarities = []
for vocab_word in vocabulary:
    if vocab_word != word:
        vec = get_word_vector_spacy(vocab_word, lang)
        sim = cosine_similarity(target_vec, vec)
        similarities.append((vocab_word, sim))

similarities.sort(key=lambda x: x[1], reverse=True)
return similarities[:top_n]
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def most_similar_words(word: str, vocabulary: List[str], 
                       lang: str = "en", top_n: int = 5) -> List[Tuple[str, float]]:
    target_vector = get_word_vector_spacy(word, lang)
    
    similarities = []
    for vocab_word in vocabulary:
        if vocab_word.lower() != word.lower():
            vocab_vector = get_word_vector_spacy(vocab_word, lang)
            similarity = cosine_similarity(target_vector, vocab_vector)
            similarities.append((vocab_word, similarity))
    
    # Ordenar por similitud descendente
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    return similarities[:top_n]
```
</details>

---

## üìù Funci√≥n 5: `load_word2vec_model()`

### Nivel 1: Concepto
Carga modelos Word2Vec pre-entrenados de Google o propios.

### Nivel 2: Implementaci√≥n
```python
from gensim.models import KeyedVectors
# Para formato binario de Google
model = KeyedVectors.load_word2vec_format(model_path, binary=True)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def load_word2vec_model(model_path: str):
    from gensim.models import KeyedVectors
    # Carga modelo pre-entrenado (ej: GoogleNews-vectors-negative300.bin)
    model = KeyedVectors.load_word2vec_format(model_path, binary=True)
    return model
```
</details>

---

## üìù Funci√≥n 6: `word_analogy()`

### Nivel 1: Concepto
Resuelve analog√≠as: "rey es a reina como hombre es a ?"

**Respuesta**: mujer (usando aritm√©tica vectorial)

### Nivel 2: F√≥rmula
```
resultado = vector(rey) - vector(hombre) + vector(mujer)
# Encuentra palabra m√°s cercana a resultado
```

### Nivel 3: Implementaci√≥n con gensim
```python
# word2vec_model tiene m√©todo most_similar que acepta positive/negative
result = word2vec_model.most_similar(
    positive=[word_a, word_c],  # "rey", "mujer"
    negative=[word_b],           # "hombre"
    topn=1
)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def word_analogy(word2vec_model, word_a: str, word_b: str, word_c: str) -> str:
    """
    Resuelve: word_a es a word_b como word_c es a ?
    Ejemplo: rey (a) es a hombre (b) como reina (c) es a mujer (?)
    """
    try:
        # Aritm√©tica vectorial: resultado = a - b + c
        result = word2vec_model.most_similar(
            positive=[word_a, word_c],
            negative=[word_b],
            topn=1
        )
        return result[0][0]  # Retorna la palabra m√°s similar
    except KeyError as e:
        return f"Word not in vocabulary: {e}"
```
</details>

---

## üìù Funci√≥n 7: `get_sentence_embedding()`

### Nivel 1: Concepto
Representa una oraci√≥n completa como un solo vector promediando word vectors.

### Nivel 2: Pasos
1. Tokeniza la oraci√≥n
2. Obt√©n vector de cada palabra
3. Calcula promedio de todos los vectores

### Nivel 3: Casi la soluci√≥n
```python
import spacy
import numpy as np
nlp = spacy.load("en_core_web_md")
doc = nlp(sentence)
# doc.vector ya hace esto autom√°ticamente!
return doc.vector
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def get_sentence_embedding(sentence: str, lang: str = "en") -> np.ndarray:
    import spacy
    import numpy as np
    
    model = "en_core_web_md" if lang == "en" else "es_core_news_md"
    nlp = spacy.load(model)
    
    doc = nlp(sentence)
    # spaCy promedia autom√°ticamente los vectores de tokens
    return doc.vector
```
</details>

---

## üéØ Conceptos Clave

### ¬øQu√© son Word Embeddings?

**Representaci√≥n densa** de palabras como vectores de n√∫meros reales:

```
"king"   ‚Üí [0.32, -0.45, 0.18, ..., 0.67]  (300 dimensiones)
"queen"  ‚Üí [0.35, -0.42, 0.21, ..., 0.69]  (similar a king)
"car"    ‚Üí [-0.12, 0.73, -0.56, ..., 0.05] (muy diferente)
```

### Propiedades Importantes

1. **Similitud Sem√°ntica**:
   - Palabras similares ‚Üí vectores cercanos
   - "perro" y "gato" m√°s cercanos que "perro" y "avi√≥n"

2. **Aritm√©tica Vectorial**:
   ```
   vector("rey") - vector("hombre") + vector("mujer") ‚âà vector("reina")
   vector("Madrid") - vector("Espa√±a") + vector("Francia") ‚âà vector("Par√≠s")
   ```

3. **Capturan Relaciones**:
   - G√©nero: hombre/mujer, rey/reina
   - Geograf√≠a: pa√≠s/capital
   - Tiempo: presente/pasado

### T√©cnicas Principales

| M√©todo | A√±o | Caracter√≠sticas |
|--------|-----|-----------------|
| **Word2Vec** | 2013 | R√°pido, eficiente (CBOW, Skip-gram) |
| **GloVe** | 2014 | Basado en estad√≠sticas de co-ocurrencia |
| **FastText** | 2016 | Maneja palabras fuera de vocabulario |
| **spaCy vectors** | - | Pre-entrenados, f√°cil de usar |

## üí° Tips Pr√°cticos

### 1. Necesitas modelos con vectores
```python
# ‚ùå NO funciona
nlp = spacy.load("en_core_web_sm")  # Sin vectores
doc = nlp("hello")
print(doc.vector)  # Vector de ceros

# ‚úÖ Funciona
nlp = spacy.load("en_core_web_md")  # Con vectores
doc = nlp("hello")
print(doc.vector)  # Vector real [300 dims]
```

### 2. Descarga modelos grandes
```bash
# Ingl√©s con vectores (300D)
python -m spacy download en_core_web_md

# Espa√±ol con vectores (300D)
python -m spacy download es_core_news_md
```

### 3. Normaliza palabras
```python
# Min√∫sculas para mejor matching
word_similarity("King", "king")  # Misma palabra
```

### 4. Maneja palabras fuera de vocabulario
```python
try:
    vec = model["palabra_rara_xyz"]
except KeyError:
    print("Palabra no en vocabulario")
    vec = np.zeros(300)  # Vector de ceros como fallback
```

## üöÄ Casos de Uso

### B√∫squeda sem√°ntica
```python
query = "feliz"
docs = ["alegre", "contento", "triste", "carro"]
similarities = [(doc, word_similarity(query, doc, "es")) for doc in docs]
similarities.sort(key=lambda x: x[1], reverse=True)
# [("alegre", 0.85), ("contento", 0.78), ...]
```

### Detecci√≥n de duplicados
```python
sentence1 = "The cat sits on the mat"
sentence2 = "A cat is sitting on a rug"
emb1 = get_sentence_embedding(sentence1)
emb2 = get_sentence_embedding(sentence2)
similarity = cosine_similarity(emb1, emb2)
# High similarity ‚Üí likely duplicates
```

### Recomendaci√≥n de contenido
```python
user_liked = ["machine learning", "AI", "neural networks"]
all_topics = ["deep learning", "cooking", "sports", "NLP"]

for topic in all_topics:
    avg_sim = np.mean([word_similarity(topic, liked) for liked in user_liked])
    print(f"{topic}: {avg_sim}")
```

### Clustering de palabras
```python
from sklearn.cluster import KMeans
words = ["dog", "cat", "car", "truck", "bird"]
vectors = [get_word_vector_spacy(w) for w in words]
kmeans = KMeans(n_clusters=2).fit(vectors)
# Clusters: [animales], [veh√≠culos]
```

## üîß Troubleshooting

### Problema: `doc.vector` es todo ceros
**Soluci√≥n**: Usa modelo con vectores (`md` o `lg`, no `sm`)

### Problema: KeyError al buscar palabra
**Soluci√≥n**: Palabra no est√° en vocabulario
```python
if word in model.vocab:
    vec = model[word]
else:
    print("Palabra no encontrada")
```

### Problema: Similitud siempre baja
**Soluci√≥n**: 
- Verifica que palabras est√©n en vocabulario
- Normaliza texto (min√∫sculas, lemmatizaci√≥n)

### Problema: Memoria insuficiente
**Soluci√≥n**: Modelos grandes (GoogleNews 3.6GB)
```python
# Carga solo las palabras que necesitas
model = KeyedVectors.load_word2vec_format(path, binary=True, limit=100000)
```

## üöÄ Siguiente Paso

Una vez completo, ve al **Koan 08: Transformers**!
