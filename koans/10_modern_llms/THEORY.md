> ** Translation Note**: This file is currently in Spanish. English translation coming soon!
> For now, you can use a translator or refer to the code examples which are language-agnostic.
> Want to help translate? See [CONTRIBUTING.md](../../CONTRIBUTING.md)

---

# TeorÃ­a: Modern LLMs & APIs

## ğŸ“š Tabla de Contenidos
1. [IntroducciÃ³n a los LLMs Modernos](#introducciÃ³n)
2. [Arquitectura de Transformers](#arquitectura)
3. [Principales Proveedores y Modelos](#proveedores)
4. [AnatomÃ­a de una Llamada a API](#llamadas-api)
5. [Streaming de Respuestas](#streaming)
6. [Function Calling](#function-calling)
7. [Tokens y Costos](#tokens-costos)
8. [Mejores PrÃ¡cticas](#mejores-prÃ¡cticas)

---

## ğŸŒŸ IntroducciÃ³n a los LLMs Modernos {#introducciÃ³n}

### Â¿QuÃ© es un LLM?

Un **Large Language Model** (Modelo de Lenguaje Grande) es una red neuronal entrenada con billones de palabras para:
- Entender lenguaje natural
- Generar texto coherente
- Seguir instrucciones complejas
- Razonar sobre informaciÃ³n
- Traducir, resumir, programar, y mÃ¡s

### EvoluciÃ³n de los LLMs

```
2018: GPT-1 (117M parÃ¡metros)
  â†“
2019: GPT-2 (1.5B parÃ¡metros)
  â†“
2020: GPT-3 (175B parÃ¡metros) â† Primera API comercial
  â†“
2022: ChatGPT (basado en GPT-3.5)
  â†“
2023: GPT-4 (rumores de 1.7T parÃ¡metros)
      Claude 2 & 3
      Gemini
      Llama 2
  â†“
2024: GPT-4o, Claude 3.5, Gemini 1.5/2.0
      Modelos mÃ¡s rÃ¡pidos y multimodales
```

### Â¿Por quÃ© usar APIs de LLMs?

**Ventajas:**
- âœ… No necesitas infraestructura de GPU
- âœ… Modelos de Ãºltima generaciÃ³n listos para usar
- âœ… Escalado automÃ¡tico
- âœ… Actualizaciones constantes
- âœ… Pago por uso (no costos fijos)

**Desventajas:**
- âŒ Costo por token
- âŒ Latencia de red
- âŒ Dependencia de terceros
- âŒ Limitaciones de privacidad (tus datos van al proveedor)

---

## ğŸ—ï¸ Arquitectura de Transformers {#arquitectura}

### El Transformer

Todos los LLMs modernos estÃ¡n basados en la arquitectura **Transformer** (Vaswani et al., 2017):

```
Input Text â†’ Tokenization â†’ Embeddings â†’ Transformer Layers â†’ Output Logits â†’ Text
```

#### Componentes Clave:

**1. Self-Attention**
- Permite al modelo "prestar atenciÃ³n" a diferentes partes del texto
- Captura relaciones entre palabras distantes
- Ejemplo: En "El gato que MarÃ­a alimentÃ³ estaba contento", "estaba" se relaciona con "gato"

**2. Multi-Head Attention**
- MÃºltiples mecanismos de atenciÃ³n en paralelo
- Cada "cabeza" aprende diferentes tipos de relaciones
- TÃ­picamente 8-96 cabezas por capa

**3. Feed-Forward Networks**
- Redes neuronales densas despuÃ©s de cada capa de atenciÃ³n
- Procesan la informaciÃ³n agregada

**4. Layer Normalization & Residual Connections**
- Estabilizan el entrenamiento
- Permiten entrenar modelos muy profundos (hasta 100+ capas)

### Tipos de Modelos

**Decoder-Only (como GPT)**
- Solo predicen el siguiente token
- Unidireccionales (solo ven el pasado)
- Ã“ptimos para generaciÃ³n de texto
- Ejemplos: GPT-4, Claude, Gemini

**Encoder-Only (como BERT)**
- Procesamiento bidireccional
- Ã“ptimos para clasificaciÃ³n y comprensiÃ³n
- Ejemplos: BERT, RoBERTa

**Encoder-Decoder (como T5)**
- Combinan ambos
- Ã“ptimos para traducciÃ³n y resumen
- Ejemplos: T5, BART

---

## ğŸ¢ Principales Proveedores y Modelos {#proveedores}

### OpenAI

**Historia:**
- Fundada en 2015 por Sam Altman, Elon Musk, y otros
- PopularizÃ³ los LLMs con ChatGPT (Nov 2022)
- LÃ­der del mercado en LLMs comerciales

**Modelos Actuales (Nov 2024):**

| Modelo | ParÃ¡metros | Velocidad | Costo | Mejor Para |
|--------|------------|-----------|-------|------------|
| **GPT-4o** | ~1.7T | âš¡âš¡âš¡ | ğŸ’°ğŸ’° | Balance velocidad/calidad |
| **GPT-4o-mini** | ~? | âš¡âš¡âš¡âš¡ | ğŸ’° | Tareas simples, bajo costo |
| **o1-preview** | ? | âš¡ | ğŸ’°ğŸ’°ğŸ’°ğŸ’° | Razonamiento complejo |
| **o1-mini** | ? | âš¡âš¡ | ğŸ’°ğŸ’° | Razonamiento + velocidad |

**CaracterÃ­sticas Ãšnicas:**
- ğŸ¯ Function calling avanzado
- ğŸ–¼ï¸ VisiÃ³n (anÃ¡lisis de imÃ¡genes)
- ğŸ™ï¸ Audio (Whisper para transcripciÃ³n)
- ğŸ¨ DALL-E para generaciÃ³n de imÃ¡genes
- ğŸ“Š AnÃ¡lisis de datos con Code Interpreter

**LÃ­mites:**
- Contexto: 128K tokens (GPT-4o)
- Rate limits: VarÃ­an por plan (RPM, RPD, TPM)

### Anthropic

**Historia:**
- Fundada en 2021 por ex-empleados de OpenAI
- Enfocados en "AI segura y confiable"
- Conocidos por Claude

**Modelos Claude:**

| Modelo | Contexto | Velocidad | Costo | Mejor Para |
|--------|----------|-----------|-------|------------|
| **Claude 3.5 Sonnet** | 200K | âš¡âš¡âš¡ | ğŸ’°ğŸ’° | Balance Ã³ptimo |
| **Claude 3 Opus** | 200K | âš¡âš¡ | ğŸ’°ğŸ’°ğŸ’° | MÃ¡xima calidad |
| **Claude 3 Haiku** | 200K | âš¡âš¡âš¡âš¡ | ğŸ’° | Velocidad |

**CaracterÃ­sticas Ãšnicas:**
- ğŸ“– Ventana de contexto masiva (200K tokens = ~150K palabras)
- ğŸ¯ Excelente siguiendo instrucciones complejas
- ğŸ”’ Ã‰nfasis en seguridad y honestidad
- ğŸ“š Mejor para anÃ¡lisis de documentos largos

**LÃ­mites:**
- Rate limits mÃ¡s estrictos que OpenAI
- Menos integraciones de terceros

### Google (Gemini)

**Historia:**
- Google ha estado en AI/ML desde siempre (TensorFlow, BERT, T5)
- Gemini lanzado en 2023 como respuesta a GPT-4
- Integrado con todo el ecosistema Google

**Modelos Gemini:**

| Modelo | Contexto | Velocidad | Costo | Mejor Para |
|--------|----------|-----------|-------|------------|
| **Gemini 1.5 Pro** | 2M | âš¡âš¡ | ğŸ’°ğŸ’° | Contexto masivo |
| **Gemini 1.5 Flash** | 1M | âš¡âš¡âš¡âš¡ | ğŸ’° | Velocidad + contexto |
| **Gemini 2.0 Flash** | 1M | âš¡âš¡âš¡âš¡âš¡ | ğŸ’° | Ãšltima generaciÃ³n |

**CaracterÃ­sticas Ãšnicas:**
- ğŸš€ Ventanas de contexto MASIVAS (hasta 2M tokens)
- ğŸ¥ Multimodal nativo (texto, imagen, video, audio)
- ğŸ†“ Tier gratuito generoso
- ğŸ”— IntegraciÃ³n con Google Workspace

**LÃ­mites:**
- API menos madura que OpenAI
- DocumentaciÃ³n a veces confusa

### Comparativa RÃ¡pida

| CaracterÃ­stica | OpenAI | Anthropic | Google |
|----------------|--------|-----------|--------|
| **Calidad General** | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ |
| **Velocidad** | â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **Costo** | ğŸ’°ğŸ’° | ğŸ’°ğŸ’°ğŸ’° | ğŸ’° |
| **Contexto** | 128K | 200K | 2M |
| **DocumentaciÃ³n** | â­â­â­â­â­ | â­â­â­â­ | â­â­â­ |
| **Ecosistema** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­ |

---

## ğŸ“¡ AnatomÃ­a de una Llamada a API {#llamadas-api}

### OpenAI Chat Completions

**Request Structure:**

```python
{
    "model": "gpt-4o-mini",
    "messages": [
        {
            "role": "system",
            "content": "Eres un asistente Ãºtil."
        },
        {
            "role": "user",
            "content": "Â¿QuÃ© es Python?"
        }
    ],
    "temperature": 0.7,
    "max_tokens": 1000,
    "top_p": 1.0,
    "frequency_penalty": 0.0,
    "presence_penalty": 0.0
}
```

**Response Structure:**

```python
{
    "id": "chatcmpl-123",
    "object": "chat.completion",
    "created": 1677652288,
    "model": "gpt-4o-mini",
    "choices": [{
        "index": 0,
        "message": {
            "role": "assistant",
            "content": "Python es un lenguaje..."
        },
        "finish_reason": "stop"
    }],
    "usage": {
        "prompt_tokens": 20,
        "completion_tokens": 150,
        "total_tokens": 170
    }
}
```

### Roles en Mensajes

**system**: Configura el comportamiento del asistente
```python
{"role": "system", "content": "Eres un experto en Python que responde concisamente."}
```

**user**: Mensajes del usuario
```python
{"role": "user", "content": "Â¿CÃ³mo funciona una lista?"}
```

**assistant**: Respuestas previas del modelo (para contexto)
```python
{"role": "assistant", "content": "Una lista es una estructura..."}
```

### ParÃ¡metros Importantes

**temperature** (0.0 - 2.0):
- `0.0`: Determinista, siempre la respuesta mÃ¡s probable
- `0.7`: Balance (recomendado por defecto)
- `1.5+`: Muy creativo/aleatorio

**max_tokens**:
- LÃ­mite de tokens en la respuesta
- No confundir con el lÃ­mite del modelo (contexto)

**top_p** (0.0 - 1.0):
- Nucleus sampling
- `1.0`: Considera todos los tokens posibles
- `0.9`: Solo considera el 90% mÃ¡s probable
- Alternativa a temperature

**frequency_penalty** (-2.0 - 2.0):
- Penaliza repetir tokens ya usados
- Positivo = menos repeticiÃ³n
- Negativo = mÃ¡s repeticiÃ³n

**presence_penalty** (-2.0 - 2.0):
- Similar a frequency pero no acumula
- Ãštil para fomentar nuevos temas

---

## ğŸŒŠ Streaming de Respuestas {#streaming}

### Â¿Por quÃ© Streaming?

Sin streaming:
```
Usuario espera... â³
Usuario espera... â³
Usuario espera... â³
[5 segundos despuÃ©s]
Â¡Respuesta completa aparece!
```

Con streaming:
```
"Python"
"Python es"
"Python es un"
"Python es un lenguaje..."
[Aparece palabra por palabra]
```

### ImplementaciÃ³n

**OpenAI:**
```python
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages,
    stream=True  # â† Activar streaming
)

for chunk in response:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

**Anthropic:**
```python
with client.messages.stream(
    model="claude-3-5-sonnet-20241022",
    messages=messages,
    max_tokens=1000
) as stream:
    for text in stream.text_stream:
        print(text, end="", flush=True)
```

### Ventajas del Streaming

âœ… Mejor UX (respuesta instantÃ¡nea)
âœ… Menor tiempo percibido de espera
âœ… Permite cancelar generaciones largas
âœ… Ideal para chatbots en tiempo real

### Desventajas

âŒ MÃ¡s complejo de implementar
âŒ Dificulta el manejo de errores
âŒ No puedes procesar la respuesta completa hasta el final

---

## ğŸ› ï¸ Function Calling {#function-calling}

### Concepto

Function calling permite que el LLM:
1. Detecte cuÃ¡ndo necesita informaciÃ³n externa
2. Decida quÃ© funciÃ³n llamar
3. Extraiga los parÃ¡metros necesarios
4. Te los devuelva en formato estructurado

**El LLM NO ejecuta la funciÃ³n**, solo te dice quÃ© llamar y cÃ³mo.

### Flujo Completo

```
1. Usuario: "Â¿QuÃ© tiempo hace en Madrid?"
   â†“
2. LLM analiza y decide: "Necesito get_weather(city='Madrid')"
   â†“
3. Tu cÃ³digo recibe: {"name": "get_weather", "arguments": '{"city": "Madrid"}'}
   â†“
4. Tu cÃ³digo ejecuta: weather = get_weather("Madrid")
   â†“
5. EnvÃ­as resultado de vuelta al LLM con el contexto
   â†“
6. LLM genera respuesta: "En Madrid hace 22Â°C y estÃ¡ soleado."
```

### Definir Funciones

Las funciones se definen usando **JSON Schema**:

```python
functions = [
    {
        "name": "get_weather",
        "description": "Obtiene el clima actual de una ciudad",
        "parameters": {
            "type": "object",
            "properties": {
                "city": {
                    "type": "string",
                    "description": "Nombre de la ciudad"
                },
                "unit": {
                    "type": "string",
                    "enum": ["celsius", "fahrenheit"],
                    "description": "Unidad de temperatura"
                }
            },
            "required": ["city"]
        }
    }
]
```

### Enviar a la API

```python
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages,
    tools=[
        {"type": "function", "function": f}
        for f in functions
    ]
)
```

### Procesar Respuesta

```python
message = response.choices[0].message

if message.tool_calls:
    tool_call = message.tool_calls[0]
    function_name = tool_call.function.name
    arguments = json.loads(tool_call.function.arguments)
    
    # Ejecutar tu funciÃ³n
    result = execute_function(function_name, arguments)
    
    # Enviar resultado de vuelta
    messages.append({
        "role": "function",
        "name": function_name,
        "content": str(result)
    })
    
    # Nueva llamada con el resultado
    final_response = client.chat.completions.create(...)
```

### Casos de Uso

- ğŸŒ Llamar APIs externas (clima, noticias, bÃºsqueda)
- ğŸ’¾ Consultar bases de datos
- ğŸ“§ Enviar emails o notificaciones
- ğŸ”§ Ejecutar cÃ³digo o comandos
- ğŸ¤– Crear agentes autÃ³nomos

---

## ğŸª™ Tokens y Costos {#tokens-costos}

### Â¿QuÃ© es un Token?

Un token es una unidad de texto. Aproximadamente:
- 1 token â‰ˆ 4 caracteres en inglÃ©s
- 1 token â‰ˆ 0.75 palabras en inglÃ©s
- 1 palabra en espaÃ±ol â‰ˆ 1-2 tokens

**Ejemplos:**

| Texto | Tokens |
|-------|--------|
| "Hello" | 1 |
| "Hello, world!" | 4 |
| "Hola, Â¿cÃ³mo estÃ¡s?" | 7 |
| "artificial intelligence" | 2 |
| "inteligencia artificial" | 4 |

### TokenizaciÃ³n

Los modelos usan tokenizadores especÃ­ficos:

**GPT (BPE - Byte Pair Encoding):**
```python
import tiktoken

encoder = tiktoken.encoding_for_model("gpt-4o-mini")
tokens = encoder.encode("Hola, Â¿cÃ³mo estÃ¡s?")
print(len(tokens))  # 7
```

**Claude (sentencepiece):**
Similar pero con vocabulario diferente

### Estructura de Costos

```
Costo Total = (Input Tokens Ã— Precio Input) + (Output Tokens Ã— Precio Output)
```

**Precios (Nov 2024) por 1M tokens:**

| Modelo | Input | Output | 1K tokens input | 1K tokens output |
|--------|-------|--------|-----------------|------------------|
| gpt-4o | $2.50 | $10.00 | $0.0025 | $0.010 |
| gpt-4o-mini | $0.15 | $0.60 | $0.00015 | $0.0006 |
| claude-3-5-sonnet | $3.00 | $15.00 | $0.003 | $0.015 |
| gemini-1.5-flash | $0.075 | $0.30 | $0.000075 | $0.0003 |

### Ejemplos de Costo

**ConversaciÃ³n Simple** (100 tokens input, 200 tokens output):
- GPT-4o: $0.00025 + $0.002 = **$0.00225**
- GPT-4o-mini: $0.000015 + $0.00012 = **$0.000135**
- Gemini Flash: $0.0000075 + $0.00006 = **$0.0000675**

**AnÃ¡lisis de Documento** (10K tokens input, 1K tokens output):
- GPT-4o: $0.025 + $0.01 = **$0.035**
- Claude Sonnet: $0.03 + $0.015 = **$0.045**

**Uso Diario** (100K tokens/dÃ­a):
- GPT-4o-mini: ~$15/mes
- GPT-4o: ~$250/mes
- Gemini Flash: ~$7.5/mes

### Optimizar Costos

**1. Usa el modelo mÃ¡s barato que funcione**
```python
# âŒ Usar GPT-4o para todo
response = call_gpt4o("Hola")

# âœ… Usar GPT-4o-mini para tareas simples
response = call_gpt4o_mini("Hola")
```

**2. Limita max_tokens**
```python
# âœ… Si solo necesitas respuestas cortas
response = client.chat.completions.create(
    ...,
    max_tokens=100  # LÃ­mita el output
)
```

**3. Cachea respuestas**
```python
cache = {}

def cached_llm_call(prompt):
    if prompt in cache:
        return cache[prompt]
    response = llm_call(prompt)
    cache[prompt] = response
    return response
```

**4. Usa prompts concisos**
```python
# âŒ Prompt verboso
prompt = "Te voy a dar un texto y me gustarÃ­a que por favor lo resumas de la manera mÃ¡s concisa posible, intentando capturar las ideas principales..."

# âœ… Prompt conciso
prompt = "Resume este texto:"
```

**5. Batch processing**
```python
# âœ… Procesa mÃºltiples items en una llamada
prompt = "Resume cada uno:\n1. Texto1\n2. Texto2\n3. Texto3"
```

---

## âš¡ Mejores PrÃ¡cticas {#mejores-prÃ¡cticas}

### 1. Manejo de Errores

**Errores Comunes:**

- **401 Unauthorized**: API key invÃ¡lida
- **429 Too Many Requests**: Rate limit excedido
- **500/503 Server Error**: Problemas del servidor
- **Timeout**: Respuesta demasiado lenta

**ImplementaciÃ³n Robusta:**

```python
import time
from openai import OpenAI, OpenAIError

def robust_llm_call(prompt, max_retries=3):
    client = OpenAI()
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                timeout=30
            )
            return response.choices[0].message.content
            
        except OpenAIError as e:
            print(f"Intento {attempt + 1} fallÃ³: {e}")
            
            if attempt < max_retries - 1:
                # Backoff exponencial: 1s, 2s, 4s
                wait_time = 2 ** attempt
                print(f"Reintentando en {wait_time}s...")
                time.sleep(wait_time)
            else:
                print("Todos los intentos fallaron")
                return None
```

### 2. Rate Limiting

**Implementa tu propio rate limiter:**

```python
import time
from collections import deque

class RateLimiter:
    def __init__(self, max_calls, time_window):
        self.max_calls = max_calls
        self.time_window = time_window
        self.calls = deque()
    
    def wait_if_needed(self):
        now = time.time()
        
        # Eliminar llamadas antiguas
        while self.calls and self.calls[0] < now - self.time_window:
            self.calls.popleft()
        
        # Si alcanzamos el lÃ­mite, esperar
        if len(self.calls) >= self.max_calls:
            sleep_time = self.calls[0] + self.time_window - now
            time.sleep(sleep_time)
        
        self.calls.append(now)

# Uso: 10 llamadas por minuto
limiter = RateLimiter(max_calls=10, time_window=60)

for prompt in prompts:
    limiter.wait_if_needed()
    response = llm_call(prompt)
```

### 3. Logging y Monitoring

```python
import logging
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def monitored_llm_call(prompt, model="gpt-4o-mini"):
    start_time = datetime.now()
    
    try:
        logger.info(f"Llamando a {model}")
        logger.debug(f"Prompt: {prompt[:100]}...")
        
        response = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        duration = (datetime.now() - start_time).total_seconds()
        tokens = response.usage.total_tokens
        cost = calculate_cost(response.usage, model)
        
        logger.info(f"âœ“ Completado en {duration:.2f}s | "
                   f"Tokens: {tokens} | Costo: ${cost:.4f}")
        
        return response.choices[0].message.content
        
    except Exception as e:
        duration = (datetime.now() - start_time).total_seconds()
        logger.error(f"âœ— Error despuÃ©s de {duration:.2f}s: {e}")
        raise
```

### 4. Seguridad

**Variables de Entorno para API Keys:**

```python
# âœ… CORRECTO
import os
api_key = os.getenv("OPENAI_API_KEY")

# âŒ NUNCA hagas esto
api_key = "sk-proj-abc123..."  # Hardcoded en el cÃ³digo
```

**Archivo .env:**
```bash
# .env (NO subas esto a Git)
OPENAI_API_KEY=sk-proj-...
ANTHROPIC_API_KEY=sk-ant-...
GOOGLE_API_KEY=AIza...
```

**Cargar con python-dotenv:**
```python
from dotenv import load_dotenv
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
```

### 5. Testing

**Mock LLM calls en tests:**

```python
import pytest
from unittest.mock import patch

def test_llm_function():
    with patch('openai.OpenAI') as mock_client:
        # Mock de respuesta
        mock_client.return_value.chat.completions.create.return_value = {
            "choices": [{
                "message": {
                    "content": "Respuesta mockeada"
                }
            }]
        }
        
        result = my_llm_function("test prompt")
        assert result == "Respuesta mockeada"
```

### 6. Prompt Engineering

**TÃ©cnicas Efectivas:**

**a) Instrucciones Claras y EspecÃ­ficas:**
```python
# âŒ Vago
prompt = "Dame informaciÃ³n sobre Python"

# âœ… EspecÃ­fico
prompt = "Lista 5 caracterÃ­sticas clave de Python que lo hacen popular, en mÃ¡ximo 2 lÃ­neas cada una."
```

**b) Few-Shot Learning:**
```python
prompt = """Clasifica el sentimiento de estos tweets:

Tweet: "Â¡Me encanta este producto!"
Sentimiento: Positivo

Tweet: "No funciona, muy decepcionado"
Sentimiento: Negativo

Tweet: "Acabo de comprar el nuevo iPhone"
Sentimiento: """
```

**c) Chain of Thought:**
```python
prompt = """Resuelve paso a paso:
Juan tiene 3 manzanas. MarÃ­a le da 5 mÃ¡s. Juan come 2.
Â¿CuÃ¡ntas manzanas tiene Juan?

Razonamiento paso a paso:"""
```

**d) Roles y Contexto:**
```python
system_message = """Eres un profesor de programaciÃ³n experto 
que explica conceptos complejos de forma simple, usando analogÃ­as 
y ejemplos prÃ¡cticos."""
```

---

## ğŸ“š Recursos Adicionales

### DocumentaciÃ³n Oficial

- [OpenAI API Docs](https://platform.openai.com/docs)
- [Anthropic Claude Docs](https://docs.anthropic.com/)
- [Google Gemini API](https://ai.google.dev/docs)

### Papers Importantes

- [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - Transformer original
- [GPT-3 Paper](https://arxiv.org/abs/2005.14165)
- [Constitutional AI](https://arxiv.org/abs/2212.08073) - Claude's approach

### Herramientas

- [tiktoken](https://github.com/openai/tiktoken) - Tokenizador de OpenAI
- [OpenAI Tokenizer](https://platform.openai.com/tokenizer) - Web tool
- [LangChain](https://python.langchain.com/) - Framework para aplicaciones con LLMs

### Comunidades

- [OpenAI Community Forum](https://community.openai.com/)
- [r/LocalLLaMA](https://reddit.com/r/LocalLLaMA)
- [Hugging Face Discord](https://hf.co/join/discord)

---

## ğŸ“ PrÃ³ximos Pasos

DespuÃ©s de dominar este koan, continÃºa con:

- **Koan 11: AI Agents** - Construye agentes autÃ³nomos con LangChain
- **Koan 12: Semantic Search** - Embeddings y bÃºsqueda vectorial
- **Koan 13: RAG** - Retrieval-Augmented Generation

Â¡Buena suerte! ğŸš€
