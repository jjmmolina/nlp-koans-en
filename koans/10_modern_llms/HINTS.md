# üí° Pistas para Koan 10: Modern LLMs & APIs

## üéØ Objetivo del Koan

Aprender a usar los **LLMs m√°s modernos** a trav√©s de sus APIs:
- OpenAI (GPT-4, GPT-4o)
- Anthropic (Claude)
- Google (Gemini)
- Function calling
- Streaming
- Gesti√≥n de costos

---

## üìù Funci√≥n 1: `call_openai_chat()`

### Nivel 1: Concepto
La API de OpenAI usa el formato de "chat completions" con mensajes de sistema, usuario y asistente.

### Nivel 2: Implementaci√≥n
```python
from openai import OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

response = client.chat.completions.create(
    model=model,
    messages=messages,
    temperature=temperature,
    max_tokens=max_tokens
)
return response.choices[0].message.content
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def call_openai_chat(
    messages: List[Dict[str, str]],
    model: str = "gpt-4o-mini",
    temperature: float = 0.7,
    max_tokens: int = 1000
) -> str:
    from openai import OpenAI
    
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens
    )
    
    return response.choices[0].message.content
```
</details>

---

## üìù Funci√≥n 2: `call_openai_streaming()`

### Nivel 1: Concepto
Streaming permite recibir la respuesta en tiempo real, token por token, mejorando UX.

### Nivel 2: Implementaci√≥n
```python
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

stream = client.chat.completions.create(
    model=model,
    messages=messages,
    stream=True  # ¬°Clave!
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        yield chunk.choices[0].delta.content
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def call_openai_streaming(
    messages: List[Dict[str, str]],
    model: str = "gpt-4o-mini"
) -> Generator[str, None, None]:
    from openai import OpenAI
    
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    stream = client.chat.completions.create(
        model=model,
        messages=messages,
        stream=True
    )
    
    for chunk in stream:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content
```
</details>

---

## üìù Funci√≥n 3: `call_anthropic_claude()`

### Nivel 1: Concepto
Claude tiene una API similar pero con algunas diferencias (no hay rol "system" separado en versiones antiguas).

### Nivel 2: Implementaci√≥n
```python
import anthropic

client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))

response = client.messages.create(
    model=model,
    max_tokens=max_tokens,
    messages=messages
)

return response.content[0].text
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def call_anthropic_claude(
    messages: List[Dict[str, str]],
    model: str = "claude-3-5-sonnet-20241022",
    max_tokens: int = 1000
) -> str:
    import anthropic
    
    client = anthropic.Anthropic(api_key=os.getenv("ANTHROPIC_API_KEY"))
    
    response = client.messages.create(
        model=model,
        max_tokens=max_tokens,
        messages=messages
    )
    
    return response.content[0].text
```
</details>

---

## üìù Funci√≥n 4: `call_google_gemini()`

### Nivel 1: Concepto
Gemini de Google tiene una API m√°s simple para casos b√°sicos.

### Nivel 2: Implementaci√≥n
```python
import google.generativeai as genai

genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
model = genai.GenerativeModel(model)
response = model.generate_content(prompt)
return response.text
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def call_google_gemini(
    prompt: str,
    model: str = "gemini-1.5-flash"
) -> str:
    import google.generativeai as genai
    
    genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
    
    model_instance = genai.GenerativeModel(model)
    response = model_instance.generate_content(prompt)
    
    return response.text
```
</details>

---

## üìù Funci√≥n 5: `openai_function_calling()`

### Nivel 1: Concepto
Function calling permite que el LLM decida llamar funciones con par√°metros espec√≠ficos.

### Nivel 2: Formato de tools
```python
tools = [
    {
        "type": "function",
        "function": {
            "name": "function_name",
            "description": "What it does",
            "parameters": {...}
        }
    }
]
```

### Nivel 3: Implementaci√≥n
```python
from openai import OpenAI
import json

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

tools = [{"type": "function", "function": f} for f in functions]

response = client.chat.completions.create(
    model=model,
    messages=messages,
    tools=tools
)

tool_call = response.choices[0].message.tool_calls[0]
return {
    "name": tool_call.function.name,
    "arguments": tool_call.function.arguments
}
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def openai_function_calling(
    messages: List[Dict[str, str]],
    functions: List[Dict[str, Any]],
    model: str = "gpt-4o-mini"
) -> Dict[str, Any]:
    from openai import OpenAI
    
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    # Convertir functions a formato tools
    tools = [{"type": "function", "function": f} for f in functions]
    
    response = client.chat.completions.create(
        model=model,
        messages=messages,
        tools=tools
    )
    
    # Extraer la primera tool call
    message = response.choices[0].message
    if message.tool_calls:
        tool_call = message.tool_calls[0]
        return {
            "name": tool_call.function.name,
            "arguments": tool_call.function.arguments
        }
    
    return {}
```
</details>

---

## üìù Funci√≥n 6: `calculate_token_cost()`

### Nivel 1: Concepto
Cada modelo tiene diferentes precios por token de entrada (prompt) y salida (completion).

### Nivel 2: Precios (Nov 2024)
```python
PRICES = {
    "gpt-4o": {"input": 2.50, "output": 10.00},
    "gpt-4o-mini": {"input": 0.15, "output": 0.60},
    "claude-3-5-sonnet-20241022": {"input": 3.00, "output": 15.00},
    "gemini-1.5-flash": {"input": 0.075, "output": 0.30}
}
# Precios por 1M tokens
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def calculate_token_cost(
    prompt_tokens: int,
    completion_tokens: int,
    model: str
) -> float:
    # Precios por 1M tokens (Nov 2024)
    PRICES = {
        "gpt-4o": {"input": 2.50, "output": 10.00},
        "gpt-4o-mini": {"input": 0.15, "output": 0.60},
        "gpt-4": {"input": 30.00, "output": 60.00},
        "claude-3-5-sonnet-20241022": {"input": 3.00, "output": 15.00},
        "claude-3-opus-20240229": {"input": 15.00, "output": 75.00},
        "gemini-1.5-pro": {"input": 1.25, "output": 5.00},
        "gemini-1.5-flash": {"input": 0.075, "output": 0.30}
    }
    
    if model not in PRICES:
        return 0.0
    
    input_cost = (prompt_tokens / 1_000_000) * PRICES[model]["input"]
    output_cost = (completion_tokens / 1_000_000) * PRICES[model]["output"]
    
    return input_cost + output_cost
```
</details>

---

## üìù Funci√≥n 7: `compare_llm_outputs()`

### Nivel 1: Concepto
Comparar respuestas de diferentes modelos ayuda a elegir el mejor para tu caso de uso.

### Nivel 2: Manejo de APIs
```python
results = {}
messages = [{"role": "user", "content": prompt}]

if "gpt" in model:
    results[model] = call_openai_chat(messages, model=model)
elif "claude" in model:
    results[model] = call_anthropic_claude(messages, model=model)
elif "gemini" in model:
    results[model] = call_google_gemini(prompt, model=model)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def compare_llm_outputs(
    prompt: str,
    models: List[str] = ["gpt-4o-mini", "claude-3-5-sonnet-20241022", "gemini-1.5-flash"]
) -> Dict[str, str]:
    results = {}
    messages = [{"role": "user", "content": prompt}]
    
    for model in models:
        try:
            if "gpt" in model or "o1" in model:
                results[model] = call_openai_chat(messages, model=model, max_tokens=500)
            elif "claude" in model:
                results[model] = call_anthropic_claude(messages, model=model, max_tokens=500)
            elif "gemini" in model:
                results[model] = call_google_gemini(prompt, model=model)
            else:
                results[model] = f"Unknown model type: {model}"
        except Exception as e:
            results[model] = f"Error: {str(e)}"
    
    return results
```
</details>

---

## üìù Funci√≥n 8: `safe_llm_call()`

### Nivel 1: Concepto
Las llamadas a APIs pueden fallar (rate limits, timeouts, errores de red). Necesitamos manejo robusto.

### Nivel 2: Exponential Backoff
```python
import time

for attempt in range(max_retries):
    try:
        # Intentar llamada
        return response
    except Exception as e:
        if attempt == max_retries - 1:
            return None
        wait_time = 2 ** attempt  # 1s, 2s, 4s...
        time.sleep(wait_time)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def safe_llm_call(
    prompt: str,
    model: str = "gpt-4o-mini",
    max_retries: int = 3
) -> Optional[str]:
    import time
    
    messages = [{"role": "user", "content": prompt}]
    
    for attempt in range(max_retries):
        try:
            if "gpt" in model or "o1" in model:
                return call_openai_chat(messages, model=model)
            elif "claude" in model:
                return call_anthropic_claude(messages, model=model)
            elif "gemini" in model:
                return call_google_gemini(prompt, model=model)
        except Exception as e:
            print(f"Attempt {attempt + 1} failed: {e}")
            
            if attempt == max_retries - 1:
                return None
            
            # Exponential backoff
            wait_time = 2 ** attempt
            time.sleep(wait_time)
    
    return None
```
</details>

---

## üéØ Conceptos Clave

### Modelos Principales (Nov 2024)

| Proveedor | Modelo | Contexto | Mejor para |
|-----------|--------|----------|------------|
| **OpenAI** | gpt-4o | 128K | Balance precio/calidad |
| | gpt-4o-mini | 128K | Tareas simples, econ√≥mico |
| | o1-preview | 128K | Razonamiento complejo |
| **Anthropic** | claude-3-5-sonnet | 200K | C√≥digo, an√°lisis largo |
| | claude-3-opus | 200K | Tareas complejas |
| **Google** | gemini-1.5-pro | 2M | Contexto ultra-largo |
| | gemini-2.0-flash-exp | 1M | R√°pido, multimodal |

### Par√°metros Importantes

**Temperature** (0.0-2.0):
- `0.0-0.3`: Determinista, factual
- `0.7-1.0`: Balanceado (default)
- `1.5-2.0`: Creativo, aleatorio

**Max Tokens**:
- Controla longitud de respuesta
- No confundir con window de contexto

**Top-p** (Nucleus Sampling):
- Alternativa a temperature
- 0.9 = considera tokens hasta 90% prob acumulada

### Function Calling

Permite que el LLM:
1. **Detecte** cu√°ndo necesita una herramienta
2. **Extraiga** par√°metros del contexto
3. **Devuelva** JSON con la llamada a funci√≥n

**Ejemplo**:
```python
User: "¬øQu√© tiempo hace en Madrid?"
LLM: {"name": "get_weather", "arguments": {"city": "Madrid"}}
```

## üí° Tips Pr√°cticos

### 1. Optimiza Costos

```python
# Usa modelos mini para tareas simples
model = "gpt-4o-mini"  # vs "gpt-4o"

# Limita tokens de salida
max_tokens = 100  # vs 4000

# Usa caching de prompts (Claude, Gemini)
```

### 2. Maneja Rate Limits

```python
import time
from functools import wraps

def rate_limit(calls_per_minute=60):
    interval = 60.0 / calls_per_minute
    
    def decorator(func):
        last_called = [0.0]
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            elapsed = time.time() - last_called[0]
            wait_time = interval - elapsed
            if wait_time > 0:
                time.sleep(wait_time)
            
            result = func(*args, **kwargs)
            last_called[0] = time.time()
            return result
        
        return wrapper
    return decorator

@rate_limit(calls_per_minute=30)
def my_llm_call():
    ...
```

### 3. System Prompts Efectivos

```python
# Malo: vago
"Eres un asistente √∫til"

# Bueno: espec√≠fico
"""Eres un experto en Python. 
Proporciona c√≥digo limpio, bien documentado.
Explica decisiones de dise√±o.
Usa type hints siempre."""
```

### 4. Structured Outputs

```python
# OpenAI response_format (beta)
response = client.chat.completions.create(
    model="gpt-4o-mini",
    messages=messages,
    response_format={"type": "json_object"}
)
```

## üöÄ Casos de Uso

### Chatbot Conversacional
```python
conversation_history = []

while True:
    user_input = input("User: ")
    conversation_history.append({"role": "user", "content": user_input})
    
    response = call_openai_chat(conversation_history)
    print(f"Bot: {response}")
    
    conversation_history.append({"role": "assistant", "content": response})
```

### Extracci√≥n de Informaci√≥n con Function Calling
```python
functions = [{
    "name": "save_contact",
    "parameters": {
        "type": "object",
        "properties": {
            "name": {"type": "string"},
            "email": {"type": "string"},
            "phone": {"type": "string"}
        }
    }
}]

text = "Contacta con Juan P√©rez al juan@example.com o al 555-1234"
result = openai_function_calling([{"role": "user", "content": text}], functions)
# Extrae autom√°ticamente nombre, email, tel√©fono
```

### Comparaci√≥n de Modelos
```python
prompt = "Explica qu√© es un transformer en 3 l√≠neas"
results = compare_llm_outputs(prompt)

for model, response in results.items():
    print(f"\n{model}:")
    print(response)
    print(f"Cost: ${calculate_token_cost(100, 50, model):.4f}")
```

## üîß Troubleshooting

### Problema: API Key inv√°lida
**Soluci√≥n**:
```bash
# Linux/Mac
export OPENAI_API_KEY="sk-..."

# Windows PowerShell
$env:OPENAI_API_KEY="sk-..."

# Python
import os
os.environ["OPENAI_API_KEY"] = "sk-..."
```

### Problema: Rate limit exceeded
**Soluci√≥n**:
- Implementa exponential backoff
- Usa tier m√°s alto (paga m√°s)
- Distribuye requests en el tiempo

### Problema: Respuestas inconsistentes
**Soluci√≥n**:
- Baja temperature (0.0-0.3)
- Usa seed parameter (OpenAI)
- Mejora system prompt

### Problema: Costo muy alto
**Soluci√≥n**:
- Usa modelos mini
- Reduce max_tokens
- Cachea respuestas comunes
- Implementa presupuesto por usuario

## üìö Recursos

- **OpenAI Docs**: https://platform.openai.com/docs
- **Anthropic Docs**: https://docs.anthropic.com
- **Google AI Docs**: https://ai.google.dev/docs
- **LLM Pricing**: https://artificialanalysis.ai/models

## üöÄ Siguiente Paso

Una vez completo, ve al **Koan 11: AI Agents**!
