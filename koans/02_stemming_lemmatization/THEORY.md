> ** Translation Note**: This file is currently in Spanish. English translation coming soon!
> For now, you can use a translator or refer to the code examples which are language-agnostic.
> Want to help translate? See [CONTRIBUTING.md](../../CONTRIBUTING.md)

---

# Teor√≠a: Stemming & Lemmatization

## üìö Tabla de Contenidos
1. [Introducci√≥n a la Normalizaci√≥n de Texto](#introducci√≥n)
2. [Stemming](#stemming)
3. [Lemmatization](#lemmatization)
4. [Comparaci√≥n Stemming vs Lemmatization](#comparaci√≥n)
5. [Algoritmos y T√©cnicas](#algoritmos)
6. [Herramientas](#herramientas)
7. [Casos de Uso](#casos-uso)

---

## üéØ Introducci√≥n a la Normalizaci√≥n de Texto {#introducci√≥n}

### ¬øPor qu√© Normalizar?

**Problema:**
```python
palabras = ["run", "runs", "running", "ran", "runner"]
# ¬øSon todas diferentes? Para una computadora, S√ç.
# Para un humano, todas se relacionan con "correr"
```

**Soluci√≥n: Reducir a una forma can√≥nica**
```python
# Despu√©s de normalizaci√≥n
todas ‚Üí "run"
```

### Variaciones Morfol√≥gicas

**Inflexi√≥n** (cambios gramaticales):
```
Verbos: walk ‚Üí walks, walked, walking
Sustantivos: cat ‚Üí cats
Adjetivos: good ‚Üí better, best
```

**Derivaci√≥n** (nuevas palabras):
```
happy ‚Üí happiness, unhappy, happily
nation ‚Üí national, nationality, nationalize
```

### Beneficios

**1. Reducci√≥n de Vocabulario:**
```python
# Antes
vocab = {"run", "runs", "running", "ran", "runner"}  # 5 palabras

# Despu√©s
vocab = {"run"}  # 1 palabra
```

**2. Mejora en B√∫squeda:**
```python
query = "running shoes"
documento = "Best shoes for runners"

# Sin normalizaci√≥n: NO match ‚ùå
# Con normalizaci√≥n: "run" match "run" ‚úÖ
```

**3. Mejora en ML:**
```python
# Menos features = modelo m√°s simple y robusto
# "running" y "run" ahora son el mismo feature
```

---

## ‚úÇÔ∏è Stemming {#stemming}

### Concepto

**Stemming** es el proceso de reducir palabras a su ra√≠z (stem) mediante reglas heur√≠sticas, generalmente cortando sufijos.

```
Palabra ‚Üí Stem (ra√≠z aproximada)

running ‚Üí run
happiness ‚Üí happi
studies ‚Üí studi
```

**Caracter√≠sticas:**
- ‚ö° R√°pido (basado en reglas)
- ‚ö†Ô∏è No siempre produce palabras reales
- üéØ Objetivo: velocidad sobre precisi√≥n

### Algoritmos de Stemming

#### 1. Porter Stemmer (1980)

El m√°s popular y usado.

**Funcionamiento:**
```
Aplica 5 fases de reglas:
Fase 1: Plurales y -ed, -ing
Fase 2: -ational ‚Üí -ate, -ization ‚Üí -ize
Fase 3: -icate ‚Üí -ic, -ative ‚Üí [nada]
Fase 4: -al, -ance, -ence, -er, -ic, -able, -ible, -ant, -ment
Fase 5: -e, -ll ‚Üí -l
```

**Ejemplos:**
```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

palabras = [
    "running",     # ‚Üí run
    "runner",      # ‚Üí runner (¬°no cambia!)
    "easily",      # ‚Üí easili
    "happiness",   # ‚Üí happi
    "connection",  # ‚Üí connect
    "conditional", # ‚Üí condit
]

for palabra in palabras:
    print(f"{palabra:15} ‚Üí {stemmer.stem(palabra)}")
```

**Resultados:**
```
running         ‚Üí run
runner          ‚Üí runner  # ‚ö†Ô∏è no reduce a "run"
easily          ‚Üí easili  # ‚ö†Ô∏è no es palabra real
happiness       ‚Üí happi   # ‚ö†Ô∏è no es palabra real
connection      ‚Üí connect ‚úÖ
conditional     ‚Üí condit  # ‚ö†Ô∏è no es palabra real
```

#### 2. Lancaster Stemmer (Paice-Husk, 1990)

M√°s agresivo que Porter.

**Ejemplos:**
```python
from nltk.stem import LancasterStemmer

stemmer = LancasterStemmer()

palabras = [
    "running",     # ‚Üí run
    "runner",      # ‚Üí run
    "easily",      # ‚Üí easy
    "happiness",   # ‚Üí happy
    "connection",  # ‚Üí connect
    "maximum",     # ‚Üí maxim
]

for palabra in palabras:
    print(f"{palabra:15} ‚Üí {stemmer.stem(palabra)}")
```

**Resultados:**
```
running         ‚Üí run
runner          ‚Üí run     # ‚úÖ m√°s agresivo
easily          ‚Üí easy    # ‚úÖ mejor que Porter
happiness       ‚Üí happy   # ‚úÖ reconoce la ra√≠z
connection      ‚Üí connect
maximum         ‚Üí maxim
```

**Caracter√≠sticas:**
- ‚úÖ M√°s agresivo
- ‚úÖ Reduce m√°s variaciones
- ‚ö†Ô∏è Mayor riesgo de sobre-stemming

#### 3. Snowball Stemmer (Porter2, 2001)

Mejora de Porter, con soporte multiling√ºe.

**Ejemplos:**
```python
from nltk.stem import SnowballStemmer

# Ingl√©s
stemmer = SnowballStemmer("english")

palabras = [
    "running",
    "easily", 
    "happiness",
    "generously"
]

for palabra in palabras:
    print(f"{palabra:15} ‚Üí {stemmer.stem(palabra)}")
```

**Espa√±ol:**
```python
stemmer_es = SnowballStemmer("spanish")

palabras_es = [
    "corriendo",   # ‚Üí corr
    "corredor",    # ‚Üí corr
    "felizmente",  # ‚Üí feliz
    "cantando",    # ‚Üí cant
]

for palabra in palabras_es:
    print(f"{palabra:15} ‚Üí {stemmer_es.stem(palabra)}")
```

### Problemas del Stemming

#### 1. Over-stemming

Reduce demasiado, conflando palabras no relacionadas.

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

# Ejemplo 1: "universal" y "university"
print(stemmer.stem("universal"))   # ‚Üí univers
print(stemmer.stem("university"))  # ‚Üí univers
# ‚ö†Ô∏è Palabras diferentes reducidas a lo mismo

# Ejemplo 2: "organization" y "organ"
print(stemmer.stem("organization")) # ‚Üí organ
print(stemmer.stem("organ"))        # ‚Üí organ
# ‚ö†Ô∏è Significados muy diferentes
```

#### 2. Under-stemming

No reduce suficiente, dejando variaciones separadas.

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

print(stemmer.stem("data"))        # ‚Üí data
print(stemmer.stem("datum"))       # ‚Üí datum
# ‚ö†Ô∏è Misma palabra (data es plural de datum) pero stems diferentes

print(stemmer.stem("aluminum"))    # ‚Üí aluminum
print(stemmer.stem("aluminium"))   # ‚Üí aluminium
# ‚ö†Ô∏è Misma palabra, ortograf√≠as diferentes
```

#### 3. No Produce Palabras Reales

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

palabras = ["happiness", "easily", "conditional"]

for palabra in palabras:
    stem = stemmer.stem(palabra)
    print(f"{palabra:15} ‚Üí {stem:10} {'‚ùå No es palabra real' if stem not in ['happy', 'easy', 'condition'] else ''}")

# happiness       ‚Üí happi      ‚ùå No es palabra real
# easily          ‚Üí easili     ‚ùå No es palabra real  
# conditional     ‚Üí condit     ‚ùå No es palabra real
```

---

## üìñ Lemmatization {#lemmatization}

### Concepto

**Lemmatization** reduce palabras a su forma base (lema) usando an√°lisis morfol√≥gico y diccionarios.

```
Palabra ‚Üí Lemma (forma base real en diccionario)

running ‚Üí run
better ‚Üí good
am/is/are/was/were ‚Üí be
mice ‚Üí mouse
```

**Caracter√≠sticas:**
- üê¢ M√°s lento (usa diccionarios y reglas)
- ‚úÖ Siempre produce palabras reales
- üéØ Objetivo: precisi√≥n sobre velocidad

### WordNet Lemmatizer (NLTK)

```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

palabras = [
    "running",
    "ran",
    "better",
    "mice",
    "geese",
    "cacti"
]

for palabra in palabras:
    lemma = lemmatizer.lemmatize(palabra)
    print(f"{palabra:15} ‚Üí {lemma}")
```

**Resultados:**
```
running         ‚Üí running  # ‚ö†Ô∏è Necesita POS tag
ran             ‚Üí ran      # ‚ö†Ô∏è Necesita POS tag
better          ‚Üí better   # ‚ö†Ô∏è Necesita POS tag
mice            ‚Üí mouse    ‚úÖ
geese           ‚Üí goose    ‚úÖ
cacti           ‚Üí cactus   ‚úÖ
```

### Part-of-Speech (POS) Tags

**Problema:**
```python
lemmatizer.lemmatize("running")  # ‚Üí running (sin cambio)
```

**Soluci√≥n:** Especificar la categor√≠a gramatical

```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

# Sin POS tag (asume sustantivo por defecto)
print(lemmatizer.lemmatize("running"))  # ‚Üí running

# Con POS tag: verbo
print(lemmatizer.lemmatize("running", pos='v'))  # ‚Üí run

# Con POS tag: adjetivo
print(lemmatizer.lemmatize("better", pos='a'))  # ‚Üí good

# Con POS tag: verbo
print(lemmatizer.lemmatize("was", pos='v'))  # ‚Üí be
```

**POS Tags en WordNet:**
```python
# 'n' = noun (sustantivo)
# 'v' = verb (verbo)
# 'a' = adjective (adjetivo)
# 'r' = adverb (adverbio)
```

### Lemmatization con POS Tagging Autom√°tico

```python
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

def get_wordnet_pos(treebank_tag):
    """Convierte Penn Treebank tags a WordNet POS tags"""
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Default

def lemmatize_sentence(sentence):
    lemmatizer = WordNetLemmatizer()
    
    # Tokenizar y POS tag
    tokens = nltk.word_tokenize(sentence)
    pos_tags = nltk.pos_tag(tokens)
    
    # Lemmatizar con POS correcto
    lemmas = []
    for word, tag in pos_tags:
        wn_pos = get_wordnet_pos(tag)
        lemma = lemmatizer.lemmatize(word, pos=wn_pos)
        lemmas.append(lemma)
    
    return lemmas

# Ejemplo
sentence = "The striped bats are hanging on their feet for best"
print(lemmatize_sentence(sentence))
# ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'good']
```

### spaCy Lemmatization

spaCy hace lemmatization autom√°ticamente con POS tagging integrado.

```python
import spacy

nlp = spacy.load("en_core_web_sm")

doc = nlp("The striped bats are hanging on their feet for best")

for token in doc:
    print(f"{token.text:15} ‚Üí {token.lemma_:15} ({token.pos_})")
```

**Resultado:**
```
The             ‚Üí the             (DET)
striped         ‚Üí strip           (VERB)
bats            ‚Üí bat             (NOUN)
are             ‚Üí be              (AUX)
hanging         ‚Üí hang            (VERB)
on              ‚Üí on              (ADP)
their           ‚Üí their           (PRON)
feet            ‚Üí foot            (NOUN)
for             ‚Üí for             (ADP)
best            ‚Üí good            (ADJ)
```

---

## ‚öñÔ∏è Comparaci√≥n Stemming vs Lemmatization {#comparaci√≥n}

### Comparativa Directa

```python
from nltk.stem import PorterStemmer, WordNetLemmatizer
import spacy

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
nlp = spacy.load("en_core_web_sm")

palabras = [
    ("running", "v"),
    ("better", "a"),
    ("studies", "n"),
    ("feet", "n"),
    ("geese", "n"),
    ("easily", "r"),
]

print(f"{'Word':<15} {'Stem':<15} {'NLTK Lemma':<15} {'spaCy Lemma':<15}")
print("-" * 60)

for word, pos in palabras:
    stem = stemmer.stem(word)
    lemma_nltk = lemmatizer.lemmatize(word, pos=pos)
    lemma_spacy = nlp(word)[0].lemma_
    
    print(f"{word:<15} {stem:<15} {lemma_nltk:<15} {lemma_spacy:<15}")
```

**Resultado:**
```
Word            Stem            NLTK Lemma      spaCy Lemma    
------------------------------------------------------------
running         run             run             run            
better          better          good            well           
studies         studi           study           study          
feet            feet            foot            foot           
geese           gees            goose           goose          
easily          easili          easily          easily         
```

### Tabla Comparativa

| Aspecto | Stemming | Lemmatization |
|---------|----------|---------------|
| **Velocidad** | ‚ö°‚ö°‚ö° Muy r√°pido | üê¢ M√°s lento |
| **Precisi√≥n** | ‚ö†Ô∏è Aproximada | ‚úÖ Alta |
| **Resultado** | Ra√≠z (puede no ser palabra real) | Lema (palabra v√°lida) |
| **M√©todo** | Reglas heur√≠sticas | An√°lisis morfol√≥gico + diccionario |
| **Requiere POS** | ‚ùå No | ‚úÖ S√≠ (para mejor resultado) |
| **Ejemplos** | running ‚Üí run<br>easily ‚Üí easili | running ‚Üí run<br>easily ‚Üí easy |
| **Uso T√≠pico** | B√∫squeda de texto<br>IR simple | NLP avanzado<br>An√°lisis sem√°ntico |

### Cu√°ndo Usar Cada Uno

**Usar Stemming cuando:**
- ‚ö° Velocidad es cr√≠tica
- üìä Trabajas con grandes vol√∫menes
- üîç B√∫squeda y recuperaci√≥n de informaci√≥n
- üìà Features para ML donde precisi√≥n no es cr√≠tica

**Usar Lemmatization cuando:**
- üéØ Precisi√≥n es importante
- üìñ An√°lisis sem√°ntico
- üó£Ô∏è Sistemas de di√°logo
- üî¨ Investigaci√≥n ling√º√≠stica
- üéì Aplicaciones educativas

---

## üîß Algoritmos y T√©cnicas {#algoritmos}

### Porter Stemmer en Detalle

**5 Fases de Reglas:**

**Fase 1: Sufijos comunes**
```
SSES ‚Üí SS          caresses ‚Üí caress
IES  ‚Üí I           ponies ‚Üí poni
SS   ‚Üí SS          caress ‚Üí caress
S    ‚Üí             cats ‚Üí cat
```

**Fase 2: Sufijos derivacionales**
```
(m>0) ATIONAL ‚Üí ATE    relational ‚Üí relate
(m>0) TIONAL  ‚Üí TION   conditional ‚Üí condition
(m>0) ENCI    ‚Üí ENCE   valenci ‚Üí valence
```

**Fase 3: M√°s derivacionales**
```
(m>0) ICATE ‚Üí IC       triplicate ‚Üí triplic
(m>0) ATIVE ‚Üí          formative ‚Üí form
```

**Fase 4: Sufijos m√°s comunes**
```
(m>1) AL    ‚Üí          revival ‚Üí reviv
(m>1) ANCE  ‚Üí          allowance ‚Üí allow
(m>1) ENCE  ‚Üí          inference ‚Üí infer
```

**Fase 5: Limpieza final**
```
(m>1) E     ‚Üí          probate ‚Üí probat
(m=1 and not *o) E ‚Üí   rate ‚Üí rate
```

**M√©trica m (measure):**
```
m = n√∫mero de secuencias [consonante(s)][vocal(es)]

tree:     (VC)  ‚Üí m=1
trees:    (VC)s ‚Üí m=1
trouble:  (VC)(VC) ‚Üí m=2
```

### Lancaster Stemmer en Detalle

**Caracter√≠sticas:**
- Usa tabla de ~120 reglas
- M√°s agresivo que Porter
- Aplica reglas en orden de especificidad

**Ejemplos de reglas:**
```
SSES    ‚Üí SS    (como Porter)
IES     ‚Üí Y     (m√°s agresivo que Porter)
ATIONAL ‚Üí ATE   
TIONAL  ‚Üí TION  
```

### Snowball (Porter2)

**Mejoras sobre Porter:**
- ‚úÖ Mejor manejo de excepciones
- ‚úÖ Soporte multiling√ºe (15+ idiomas)
- ‚úÖ M√°s eficiente
- ‚úÖ Mejor documentaci√≥n

**Idiomas soportados:**
```python
from nltk.stem import SnowballStemmer

# Ver idiomas disponibles
print(SnowballStemmer.languages)
# ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 
#  'german', 'hungarian', 'italian', 'norwegian', 'porter', 
#  'portuguese', 'romanian', 'russian', 'spanish', 'swedish')
```

---

## üõ†Ô∏è Herramientas {#herramientas}

### NLTK

**Stemmers:**
```python
from nltk.stem import (
    PorterStemmer,
    LancasterStemmer,
    SnowballStemmer,
    RegexpStemmer
)

# Porter
porter = PorterStemmer()
porter.stem("running")  # ‚Üí run

# Lancaster
lancaster = LancasterStemmer()
lancaster.stem("running")  # ‚Üí run

# Snowball (multiling√ºe)
snowball = SnowballStemmer("english")
snowball.stem("running")  # ‚Üí run

# Custom Regexp Stemmer
regexp = RegexpStemmer('ing$|s$|e$', min=4)
regexp.stem("running")  # ‚Üí runn
```

**Lemmatizers:**
```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

# Con POS tag
lemmatizer.lemmatize("running", pos='v')  # ‚Üí run
lemmatizer.lemmatize("better", pos='a')   # ‚Üí good
```

### spaCy

**Lemmatization Autom√°tica:**
```python
import spacy

nlp = spacy.load("en_core_web_sm")

doc = nlp("running dogs are better")

for token in doc:
    print(token.text, "‚Üí", token.lemma_)

# running ‚Üí run
# dogs ‚Üí dog
# are ‚Üí be
# better ‚Üí well
```

**Ventajas de spaCy:**
- ‚úÖ POS tagging autom√°tico
- ‚úÖ Muy r√°pido
- ‚úÖ Preciso
- ‚úÖ Multiling√ºe

### Stanza (Stanford NLP)

```python
import stanza

nlp = stanza.Pipeline('en', processors='tokenize,lemma')

doc = nlp("The quick brown foxes are jumping")

for sentence in doc.sentences:
    for word in sentence.words:
        print(f"{word.text} ‚Üí {word.lemma}")

# The ‚Üí the
# quick ‚Üí quick
# brown ‚Üí brown
# foxes ‚Üí fox
# are ‚Üí be
# jumping ‚Üí jump
```

### Comparativa de Performance

| Herramienta | Velocidad | Precisi√≥n | Facilidad |
|-------------|-----------|-----------|-----------|
| **Porter (NLTK)** | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Lancaster (NLTK)** | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **NLTK Lemmatizer** | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |
| **spaCy** | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| **Stanza** | ‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

---

## üíº Casos de Uso {#casos-uso}

### 1. B√∫squeda de Informaci√≥n

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

# Query del usuario
query = "running shoes"
query_stems = [stemmer.stem(word) for word in query.split()]
# ["run", "shoe"]

# Documentos
docs = [
    "Best shoes for runners",
    "Running shoe reviews",
    "Marathon running tips"
]

# Buscar matches
for doc in docs:
    doc_stems = [stemmer.stem(word) for word in doc.lower().split()]
    if any(stem in doc_stems for stem in query_stems):
        print(f"‚úÖ Match: {doc}")

# ‚úÖ Match: Best shoes for runners (shoe, run)
# ‚úÖ Match: Running shoe reviews (run, shoe)
# ‚úÖ Match: Marathon running tips (run)
```

### 2. An√°lisis de Sentimientos

```python
import spacy

nlp = spacy.load("en_core_web_sm")

# Reviews con variaciones
reviews = [
    "I love this product! It's amazing!",
    "Loved it! Amazingly good!",
    "Loving every moment. Amazing quality."
]

# Palabras positivas (en lema)
positive_lemmas = {"love", "amazing", "good", "excellent"}

for review in reviews:
    doc = nlp(review.lower())
    lemmas = [token.lemma_ for token in doc if token.is_alpha]
    
    sentiment_score = sum(1 for lemma in lemmas if lemma in positive_lemmas)
    print(f"{review[:30]:30} ‚Üí Score: {sentiment_score}")

# I love this product! It's am ‚Üí Score: 2 (love, amazing)
# Loved it! Amazingly good!    ‚Üí Score: 3 (love, amazing, good)
# Loving every moment. Amazi  ‚Üí Score: 2 (love, amazing)
```

### 3. Text Classification

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

def stem_tokenizer(text):
    tokens = text.lower().split()
    return [stemmer.stem(token) for token in tokens]

# Vectorizer con stemming
vectorizer = TfidfVectorizer(tokenizer=stem_tokenizer)

corpus = [
    "Python programming tutorial",
    "Programming in Python for beginners",
    "Learn to program with Python"
]

X = vectorizer.fit_transform(corpus)

# Features son stems: ["python", "program", "tutori", "begin", "learn"]
# "programming", "programmer", "programs" ‚Üí todos son "program"
```

### 4. Chatbots

```python
import spacy

nlp = spacy.load("en_core_web_sm")

# Base de conocimiento con variaciones
faq = {
    "reset password": "To reset your password, go to...",
    "change password": "To change your password, go to...",
    "update password": "To update your password, go to...",
}

def find_answer(user_query):
    # Lemmatizar query del usuario
    doc_query = nlp(user_query.lower())
    query_lemmas = {token.lemma_ for token in doc_query if not token.is_stop}
    
    # Buscar mejor match
    best_match = None
    best_score = 0
    
    for faq_key, answer in faq.items():
        doc_faq = nlp(faq_key)
        faq_lemmas = {token.lemma_ for token in doc_faq}
        
        # Similitud simple: intersecci√≥n de lemmas
        score = len(query_lemmas & faq_lemmas)
        
        if score > best_score:
            best_score = score
            best_match = answer
    
    return best_match if best_score > 0 else "I don't understand"

# Usuario puede preguntar de diferentes formas
print(find_answer("How do I reset my password?"))
# ‚Üí "To reset your password, go to..."

print(find_answer("I want to change my password"))
# ‚Üí "To change your password, go to..."

print(find_answer("updating password"))
# ‚Üí "To update your password, go to..."
```

### 5. Reducci√≥n de Features para ML

```python
from collections import Counter
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

corpus = [
    "machine learning is great",
    "learning machines are smart",
    "I love machine learning"
]

# Sin stemming
words_no_stem = []
for doc in corpus:
    words_no_stem.extend(doc.lower().split())

vocab_no_stem = Counter(words_no_stem)
print(f"Vocabulario sin stemming: {len(vocab_no_stem)} palabras")
print(vocab_no_stem)
# Vocabulario sin stemming: 9 palabras
# {'machine': 2, 'learning': 3, 'is': 1, 'great': 1, 
#  'machines': 1, 'are': 1, 'smart': 1, 'i': 1, 'love': 1}

# Con stemming
words_stem = []
for doc in corpus:
    tokens = doc.lower().split()
    words_stem.extend([stemmer.stem(t) for t in tokens])

vocab_stem = Counter(words_stem)
print(f"\nVocabulario con stemming: {len(vocab_stem)} palabras")
print(vocab_stem)
# Vocabulario con stemming: 7 palabras
# {'machin': 3, 'learn': 3, 'is': 1, 'great': 1, 
#  'are': 1, 'smart': 1, 'i': 1, 'love': 1}
```

---

## üìä Best Practices

### 1. Elegir la Herramienta Correcta

```python
# Para b√∫squeda simple y velocidad
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()  # ‚úÖ

# Para an√°lisis sem√°ntico preciso
import spacy
nlp = spacy.load("en_core_web_sm")  # ‚úÖ

# Para multiling√ºe
from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer("spanish")  # ‚úÖ
```

### 2. Consistencia

```python
# ‚úÖ Usar la misma t√©cnica en todo el pipeline
def preprocess(text, method='lemma'):
    if method == 'stem':
        stemmer = PorterStemmer()
        tokens = text.split()
        return [stemmer.stem(t) for t in tokens]
    elif method == 'lemma':
        doc = nlp(text)
        return [token.lemma_ for token in doc]

# Aplicar consistentemente
train_processed = [preprocess(text, 'lemma') for text in train]
test_processed = [preprocess(text, 'lemma') for text in test]
```

### 3. Combinar con Otras T√©cnicas

```python
import spacy
from nltk.corpus import stopwords

nlp = spacy.load("en_core_web_sm")
stop_words = set(stopwords.words('english'))

def advanced_preprocess(text):
    # 1. Lowercase
    text = text.lower()
    
    # 2. Tokenizar y lemmatizar
    doc = nlp(text)
    
    # 3. Filtrar
    tokens = [
        token.lemma_ 
        for token in doc 
        if token.is_alpha  # Solo palabras
        and not token.is_stop  # Sin stopwords
        and len(token) > 2  # Longitud m√≠nima
    ]
    
    return tokens

text = "The running dogs are jumping over the fence"
print(advanced_preprocess(text))
# ['run', 'dog', 'jump', 'fence']
```

### 4. Manejo de Excepciones

```python
import spacy

nlp = spacy.load("en_core_web_sm")

# Palabras especiales que no deben lemmatizarse
special_words = {"COVID-19", "iPhone", "NASA"}

def lemmatize_with_exceptions(text, exceptions=special_words):
    doc = nlp(text)
    
    lemmas = []
    for token in doc:
        if token.text in exceptions:
            lemmas.append(token.text)  # Mantener original
        else:
            lemmas.append(token.lemma_)
    
    return lemmas

text = "NASA announced iPhone support for COVID-19 tracking"
print(lemmatize_with_exceptions(text))
# ['NASA', 'announce', 'iPhone', 'support', 'for', 'COVID-19', 'track']
```

---

## üéì Resumen

**Conceptos Clave:**
- **Stemming**: Reducci√≥n a ra√≠z mediante reglas (r√°pido, aproximado)
- **Lemmatization**: Reducci√≥n a lema mediante an√°lisis (preciso, lento)
- Stemming para velocidad, lemmatization para precisi√≥n
- spaCy es la mejor opci√≥n para producci√≥n

**Algoritmos Principales:**
- **Porter**: Balance, m√°s usado
- **Lancaster**: M√°s agresivo
- **Snowball**: Porter mejorado, multiling√ºe
- **WordNet**: Lemmatization con diccionario

**Decisiones Importantes:**
1. ¬øVelocidad o precisi√≥n?
2. ¬øPalabras reales importan?
3. ¬øMultiling√ºe?
4. ¬øIntegrar con POS tagging?

**Pr√≥ximos Pasos:**
- **Koan 3**: POS Tagging (necesario para lemmatization √≥ptima)
- **Koan 5**: Text Classification (usando normalizaci√≥n)
- **Koan 6**: Sentiment Analysis (benefici√°ndose de normalizaci√≥n)

¬°La normalizaci√≥n mejora todos los pipelines de NLP! üöÄ
