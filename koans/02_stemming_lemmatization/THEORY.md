# TeorÃ­a: Stemming & Lemmatization

## ğŸ“š Tabla de Contenidos
1. [IntroducciÃ³n a la NormalizaciÃ³n de Texto](#introducciÃ³n)
2. [Stemming](#stemming)
3. [Lemmatization](#lemmatization)
4. [ComparaciÃ³n Stemming vs Lemmatization](#comparaciÃ³n)
5. [Algoritmos y TÃ©cnicas](#algoritmos)
6. [Herramientas](#herramientas)
7. [Casos de Uso](#casos-uso)

---

## ğŸ¯ IntroducciÃ³n a la NormalizaciÃ³n de Texto {#introducciÃ³n}

### Â¿Por quÃ© Normalizar?

**Problema:**
```python
palabras = ["run", "runs", "running", "ran", "runner"]
# Â¿Son todas diferentes? Para una computadora, SÃ.
# Para un humano, todas se relacionan con "correr"
```

**SoluciÃ³n: Reducir a una forma canÃ³nica**
```python
# DespuÃ©s de normalizaciÃ³n
todas â†’ "run"
```

### Variaciones MorfolÃ³gicas

**InflexiÃ³n** (cambios gramaticales):
```
Verbos: walk â†’ walks, walked, walking
Sustantivos: cat â†’ cats
Adjetivos: good â†’ better, best
```

**DerivaciÃ³n** (nuevas palabras):
```
happy â†’ happiness, unhappy, happily
nation â†’ national, nationality, nationalize
```

### Beneficios

**1. ReducciÃ³n de Vocabulario:**
```python
# Antes
vocab = {"run", "runs", "running", "ran", "runner"}  # 5 palabras

# DespuÃ©s
vocab = {"run"}  # 1 palabra
```

**2. Mejora en BÃºsqueda:**
```python
query = "running shoes"
documento = "Best shoes for runners"

# Sin normalizaciÃ³n: NO match âŒ
# Con normalizaciÃ³n: "run" match "run" âœ…
```

**3. Mejora en ML:**
```python
# Menos features = modelo mÃ¡s simple y robusto
# "running" y "run" ahora son el mismo feature
```

---

## âœ‚ï¸ Stemming {#stemming}

### Concepto

**Stemming** es el proceso de reducir palabras a su raÃ­z (stem) mediante reglas heurÃ­sticas, generalmente cortando sufijos.

```
Palabra â†’ Stem (raÃ­z aproximada)

running â†’ run
happiness â†’ happi
studies â†’ studi
```

**CaracterÃ­sticas:**
- âš¡ RÃ¡pido (basado en reglas)
- âš ï¸ No siempre produce palabras reales
- ğŸ¯ Objetivo: velocidad sobre precisiÃ³n

### Algoritmos de Stemming

#### 1. Porter Stemmer (1980)

El mÃ¡s popular y usado.

**Funcionamiento:**
```
Aplica 5 fases de reglas:
Fase 1: Plurales y -ed, -ing
Fase 2: -ational â†’ -ate, -ization â†’ -ize
Fase 3: -icate â†’ -ic, -ative â†’ [nada]
Fase 4: -al, -ance, -ence, -er, -ic, -able, -ible, -ant, -ment
Fase 5: -e, -ll â†’ -l
```

**Ejemplos:**
```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

palabras = [
    "running",     # â†’ run
    "runner",      # â†’ runner (Â¡no cambia!)
    "easily",      # â†’ easili
    "happiness",   # â†’ happi
    "connection",  # â†’ connect
    "conditional", # â†’ condit
]

for palabra in palabras:
    print(f"{palabra:15} â†’ {stemmer.stem(palabra)}")
```

**Resultados:**
```
running         â†’ run
runner          â†’ runner  # âš ï¸ no reduce a "run"
easily          â†’ easili  # âš ï¸ no es palabra real
happiness       â†’ happi   # âš ï¸ no es palabra real
connection      â†’ connect âœ…
conditional     â†’ condit  # âš ï¸ no es palabra real
```

#### 2. Lancaster Stemmer (Paice-Husk, 1990)

MÃ¡s agresivo que Porter.

**Ejemplos:**
```python
from nltk.stem import LancasterStemmer

stemmer = LancasterStemmer()

palabras = [
    "running",     # â†’ run
    "runner",      # â†’ run
    "easily",      # â†’ easy
    "happiness",   # â†’ happy
    "connection",  # â†’ connect
    "maximum",     # â†’ maxim
]

for palabra in palabras:
    print(f"{palabra:15} â†’ {stemmer.stem(palabra)}")
```

**Resultados:**
```
running         â†’ run
runner          â†’ run     # âœ… mÃ¡s agresivo
easily          â†’ easy    # âœ… mejor que Porter
happiness       â†’ happy   # âœ… reconoce la raÃ­z
connection      â†’ connect
maximum         â†’ maxim
```

**CaracterÃ­sticas:**
- âœ… MÃ¡s agresivo
- âœ… Reduce mÃ¡s variaciones
- âš ï¸ Mayor riesgo de sobre-stemming

#### 3. Snowball Stemmer (Porter2, 2001)

Mejora de Porter, con soporte multilingÃ¼e.

**Ejemplos:**
```python
from nltk.stem import SnowballStemmer

# InglÃ©s
stemmer = SnowballStemmer("english")

palabras = [
    "running",
    "easily", 
    "happiness",
    "generously"
]

for palabra in palabras:
    print(f"{palabra:15} â†’ {stemmer.stem(palabra)}")
```

**EspaÃ±ol:**
```python
stemmer_es = SnowballStemmer("spanish")

palabras_es = [
    "corriendo",   # â†’ corr
    "corredor",    # â†’ corr
    "felizmente",  # â†’ feliz
    "cantando",    # â†’ cant
]

for palabra in palabras_es:
    print(f"{palabra:15} â†’ {stemmer_es.stem(palabra)}")
```

### Problemas del Stemming

#### 1. Over-stemming

Reduce demasiado, conflando palabras no relacionadas.

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

# Ejemplo 1: "universal" y "university"
print(stemmer.stem("universal"))   # â†’ univers
print(stemmer.stem("university"))  # â†’ univers
# âš ï¸ Palabras diferentes reducidas a lo mismo

# Ejemplo 2: "organization" y "organ"
print(stemmer.stem("organization")) # â†’ organ
print(stemmer.stem("organ"))        # â†’ organ
# âš ï¸ Significados muy diferentes
```

#### 2. Under-stemming

No reduce suficiente, dejando variaciones separadas.

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

print(stemmer.stem("data"))        # â†’ data
print(stemmer.stem("datum"))       # â†’ datum
# âš ï¸ Misma palabra (data es plural de datum) pero stems diferentes

print(stemmer.stem("aluminum"))    # â†’ aluminum
print(stemmer.stem("aluminium"))   # â†’ aluminium
# âš ï¸ Misma palabra, ortografÃ­as diferentes
```

#### 3. No Produce Palabras Reales

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

palabras = ["happiness", "easily", "conditional"]

for palabra in palabras:
    stem = stemmer.stem(palabra)
    print(f"{palabra:15} â†’ {stem:10} {'âŒ No es palabra real' if stem not in ['happy', 'easy', 'condition'] else ''}")

# happiness       â†’ happi      âŒ No es palabra real
# easily          â†’ easili     âŒ No es palabra real  
# conditional     â†’ condit     âŒ No es palabra real
```

---

## ğŸ“– Lemmatization {#lemmatization}

### Concepto

**Lemmatization** reduce palabras a su forma base (lema) usando anÃ¡lisis morfolÃ³gico y diccionarios.

```
Palabra â†’ Lemma (forma base real en diccionario)

running â†’ run
better â†’ good
am/is/are/was/were â†’ be
mice â†’ mouse
```

**CaracterÃ­sticas:**
- ğŸ¢ MÃ¡s lento (usa diccionarios y reglas)
- âœ… Siempre produce palabras reales
- ğŸ¯ Objetivo: precisiÃ³n sobre velocidad

### WordNet Lemmatizer (NLTK)

```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

palabras = [
    "running",
    "ran",
    "better",
    "mice",
    "geese",
    "cacti"
]

for palabra in palabras:
    lemma = lemmatizer.lemmatize(palabra)
    print(f"{palabra:15} â†’ {lemma}")
```

**Resultados:**
```
running         â†’ running  # âš ï¸ Necesita POS tag
ran             â†’ ran      # âš ï¸ Necesita POS tag
better          â†’ better   # âš ï¸ Necesita POS tag
mice            â†’ mouse    âœ…
geese           â†’ goose    âœ…
cacti           â†’ cactus   âœ…
```

### Part-of-Speech (POS) Tags

**Problema:**
```python
lemmatizer.lemmatize("running")  # â†’ running (sin cambio)
```

**SoluciÃ³n:** Especificar la categorÃ­a gramatical

```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

# Sin POS tag (asume sustantivo por defecto)
print(lemmatizer.lemmatize("running"))  # â†’ running

# Con POS tag: verbo
print(lemmatizer.lemmatize("running", pos='v'))  # â†’ run

# Con POS tag: adjetivo
print(lemmatizer.lemmatize("better", pos='a'))  # â†’ good

# Con POS tag: verbo
print(lemmatizer.lemmatize("was", pos='v'))  # â†’ be
```

**POS Tags en WordNet:**
```python
# 'n' = noun (sustantivo)
# 'v' = verb (verbo)
# 'a' = adjective (adjetivo)
# 'r' = adverb (adverbio)
```

### Lemmatization con POS Tagging AutomÃ¡tico

```python
import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet

def get_wordnet_pos(treebank_tag):
    """Convierte Penn Treebank tags a WordNet POS tags"""
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Default

def lemmatize_sentence(sentence):
    lemmatizer = WordNetLemmatizer()
    
    # Tokenizar y POS tag
    tokens = nltk.word_tokenize(sentence)
    pos_tags = nltk.pos_tag(tokens)
    
    # Lemmatizar con POS correcto
    lemmas = []
    for word, tag in pos_tags:
        wn_pos = get_wordnet_pos(tag)
        lemma = lemmatizer.lemmatize(word, pos=wn_pos)
        lemmas.append(lemma)
    
    return lemmas

# Ejemplo
sentence = "The striped bats are hanging on their feet for best"
print(lemmatize_sentence(sentence))
# ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'good']
```

### spaCy Lemmatization

spaCy hace lemmatization automÃ¡ticamente con POS tagging integrado.

```python
import spacy

nlp = spacy.load("en_core_web_sm")

doc = nlp("The striped bats are hanging on their feet for best")

for token in doc:
    print(f"{token.text:15} â†’ {token.lemma_:15} ({token.pos_})")
```

**Resultado:**
```
The             â†’ the             (DET)
striped         â†’ strip           (VERB)
bats            â†’ bat             (NOUN)
are             â†’ be              (AUX)
hanging         â†’ hang            (VERB)
on              â†’ on              (ADP)
their           â†’ their           (PRON)
feet            â†’ foot            (NOUN)
for             â†’ for             (ADP)
best            â†’ good            (ADJ)
```

---

## âš–ï¸ ComparaciÃ³n Stemming vs Lemmatization {#comparaciÃ³n}

### Comparativa Directa

```python
from nltk.stem import PorterStemmer, WordNetLemmatizer
import spacy

stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()
nlp = spacy.load("en_core_web_sm")

palabras = [
    ("running", "v"),
    ("better", "a"),
    ("studies", "n"),
    ("feet", "n"),
    ("geese", "n"),
    ("easily", "r"),
]

print(f"{'Word':<15} {'Stem':<15} {'NLTK Lemma':<15} {'spaCy Lemma':<15}")
print("-" * 60)

for word, pos in palabras:
    stem = stemmer.stem(word)
    lemma_nltk = lemmatizer.lemmatize(word, pos=pos)
    lemma_spacy = nlp(word)[0].lemma_
    
    print(f"{word:<15} {stem:<15} {lemma_nltk:<15} {lemma_spacy:<15}")
```

**Resultado:**
```
Word            Stem            NLTK Lemma      spaCy Lemma    
------------------------------------------------------------
running         run             run             run            
better          better          good            well           
studies         studi           study           study          
feet            feet            foot            foot           
geese           gees            goose           goose          
easily          easili          easily          easily         
```

### Tabla Comparativa

| Aspecto | Stemming | Lemmatization |
|---------|----------|---------------|
| **Velocidad** | âš¡âš¡âš¡ Muy rÃ¡pido | ğŸ¢ MÃ¡s lento |
| **PrecisiÃ³n** | âš ï¸ Aproximada | âœ… Alta |
| **Resultado** | RaÃ­z (puede no ser palabra real) | Lema (palabra vÃ¡lida) |
| **MÃ©todo** | Reglas heurÃ­sticas | AnÃ¡lisis morfolÃ³gico + diccionario |
| **Requiere POS** | âŒ No | âœ… SÃ­ (para mejor resultado) |
| **Ejemplos** | running â†’ run<br>easily â†’ easili | running â†’ run<br>easily â†’ easy |
| **Uso TÃ­pico** | BÃºsqueda de texto<br>IR simple | NLP avanzado<br>AnÃ¡lisis semÃ¡ntico |

### CuÃ¡ndo Usar Cada Uno

**Usar Stemming cuando:**
- âš¡ Velocidad es crÃ­tica
- ğŸ“Š Trabajas con grandes volÃºmenes
- ğŸ” BÃºsqueda y recuperaciÃ³n de informaciÃ³n
- ğŸ“ˆ Features para ML donde precisiÃ³n no es crÃ­tica

**Usar Lemmatization cuando:**
- ğŸ¯ PrecisiÃ³n es importante
- ğŸ“– AnÃ¡lisis semÃ¡ntico
- ğŸ—£ï¸ Sistemas de diÃ¡logo
- ğŸ”¬ InvestigaciÃ³n lingÃ¼Ã­stica
- ğŸ“ Aplicaciones educativas

---

## ğŸ”§ Algoritmos y TÃ©cnicas {#algoritmos}

### Porter Stemmer en Detalle

**5 Fases de Reglas:**

**Fase 1: Sufijos comunes**
```
SSES â†’ SS          caresses â†’ caress
IES  â†’ I           ponies â†’ poni
SS   â†’ SS          caress â†’ caress
S    â†’             cats â†’ cat
```

**Fase 2: Sufijos derivacionales**
```
(m>0) ATIONAL â†’ ATE    relational â†’ relate
(m>0) TIONAL  â†’ TION   conditional â†’ condition
(m>0) ENCI    â†’ ENCE   valenci â†’ valence
```

**Fase 3: MÃ¡s derivacionales**
```
(m>0) ICATE â†’ IC       triplicate â†’ triplic
(m>0) ATIVE â†’          formative â†’ form
```

**Fase 4: Sufijos mÃ¡s comunes**
```
(m>1) AL    â†’          revival â†’ reviv
(m>1) ANCE  â†’          allowance â†’ allow
(m>1) ENCE  â†’          inference â†’ infer
```

**Fase 5: Limpieza final**
```
(m>1) E     â†’          probate â†’ probat
(m=1 and not *o) E â†’   rate â†’ rate
```

**MÃ©trica m (measure):**
```
m = nÃºmero de secuencias [consonante(s)][vocal(es)]

tree:     (VC)  â†’ m=1
trees:    (VC)s â†’ m=1
trouble:  (VC)(VC) â†’ m=2
```

### Lancaster Stemmer en Detalle

**CaracterÃ­sticas:**
- Usa tabla de ~120 reglas
- MÃ¡s agresivo que Porter
- Aplica reglas en orden de especificidad

**Ejemplos de reglas:**
```
SSES    â†’ SS    (como Porter)
IES     â†’ Y     (mÃ¡s agresivo que Porter)
ATIONAL â†’ ATE   
TIONAL  â†’ TION  
```

### Snowball (Porter2)

**Mejoras sobre Porter:**
- âœ… Mejor manejo de excepciones
- âœ… Soporte multilingÃ¼e (15+ idiomas)
- âœ… MÃ¡s eficiente
- âœ… Mejor documentaciÃ³n

**Idiomas soportados:**
```python
from nltk.stem import SnowballStemmer

# Ver idiomas disponibles
print(SnowballStemmer.languages)
# ('arabic', 'danish', 'dutch', 'english', 'finnish', 'french', 
#  'german', 'hungarian', 'italian', 'norwegian', 'porter', 
#  'portuguese', 'romanian', 'russian', 'spanish', 'swedish')
```

---

## ğŸ› ï¸ Herramientas {#herramientas}

### NLTK

**Stemmers:**
```python
from nltk.stem import (
    PorterStemmer,
    LancasterStemmer,
    SnowballStemmer,
    RegexpStemmer
)

# Porter
porter = PorterStemmer()
porter.stem("running")  # â†’ run

# Lancaster
lancaster = LancasterStemmer()
lancaster.stem("running")  # â†’ run

# Snowball (multilingÃ¼e)
snowball = SnowballStemmer("english")
snowball.stem("running")  # â†’ run

# Custom Regexp Stemmer
regexp = RegexpStemmer('ing$|s$|e$', min=4)
regexp.stem("running")  # â†’ runn
```

**Lemmatizers:**
```python
from nltk.stem import WordNetLemmatizer

lemmatizer = WordNetLemmatizer()

# Con POS tag
lemmatizer.lemmatize("running", pos='v')  # â†’ run
lemmatizer.lemmatize("better", pos='a')   # â†’ good
```

### spaCy

**Lemmatization AutomÃ¡tica:**
```python
import spacy

nlp = spacy.load("en_core_web_sm")

doc = nlp("running dogs are better")

for token in doc:
    print(token.text, "â†’", token.lemma_)

# running â†’ run
# dogs â†’ dog
# are â†’ be
# better â†’ well
```

**Ventajas de spaCy:**
- âœ… POS tagging automÃ¡tico
- âœ… Muy rÃ¡pido
- âœ… Preciso
- âœ… MultilingÃ¼e

### Stanza (Stanford NLP)

```python
import stanza

nlp = stanza.Pipeline('en', processors='tokenize,lemma')

doc = nlp("The quick brown foxes are jumping")

for sentence in doc.sentences:
    for word in sentence.words:
        print(f"{word.text} â†’ {word.lemma}")

# The â†’ the
# quick â†’ quick
# brown â†’ brown
# foxes â†’ fox
# are â†’ be
# jumping â†’ jump
```

### Comparativa de Performance

| Herramienta | Velocidad | PrecisiÃ³n | Facilidad |
|-------------|-----------|-----------|-----------|
| **Porter (NLTK)** | âš¡âš¡âš¡âš¡ | â­â­ | â­â­â­â­â­ |
| **Lancaster (NLTK)** | âš¡âš¡âš¡âš¡ | â­â­ | â­â­â­â­â­ |
| **NLTK Lemmatizer** | âš¡âš¡ | â­â­â­ | â­â­â­ |
| **spaCy** | âš¡âš¡âš¡âš¡ | â­â­â­â­â­ | â­â­â­â­â­ |
| **Stanza** | âš¡âš¡ | â­â­â­â­â­ | â­â­â­â­ |

---

## ğŸ’¼ Casos de Uso {#casos-uso}

### 1. BÃºsqueda de InformaciÃ³n

```python
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

# Query del usuario
query = "running shoes"
query_stems = [stemmer.stem(word) for word in query.split()]
# ["run", "shoe"]

# Documentos
docs = [
    "Best shoes for runners",
    "Running shoe reviews",
    "Marathon running tips"
]

# Buscar matches
for doc in docs:
    doc_stems = [stemmer.stem(word) for word in doc.lower().split()]
    if any(stem in doc_stems for stem in query_stems):
        print(f"âœ… Match: {doc}")

# âœ… Match: Best shoes for runners (shoe, run)
# âœ… Match: Running shoe reviews (run, shoe)
# âœ… Match: Marathon running tips (run)
```

### 2. AnÃ¡lisis de Sentimientos

```python
import spacy

nlp = spacy.load("en_core_web_sm")

# Reviews con variaciones
reviews = [
    "I love this product! It's amazing!",
    "Loved it! Amazingly good!",
    "Loving every moment. Amazing quality."
]

# Palabras positivas (en lema)
positive_lemmas = {"love", "amazing", "good", "excellent"}

for review in reviews:
    doc = nlp(review.lower())
    lemmas = [token.lemma_ for token in doc if token.is_alpha]
    
    sentiment_score = sum(1 for lemma in lemmas if lemma in positive_lemmas)
    print(f"{review[:30]:30} â†’ Score: {sentiment_score}")

# I love this product! It's am â†’ Score: 2 (love, amazing)
# Loved it! Amazingly good!    â†’ Score: 3 (love, amazing, good)
# Loving every moment. Amazi  â†’ Score: 2 (love, amazing)
```

### 3. Text Classification

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

def stem_tokenizer(text):
    tokens = text.lower().split()
    return [stemmer.stem(token) for token in tokens]

# Vectorizer con stemming
vectorizer = TfidfVectorizer(tokenizer=stem_tokenizer)

corpus = [
    "Python programming tutorial",
    "Programming in Python for beginners",
    "Learn to program with Python"
]

X = vectorizer.fit_transform(corpus)

# Features son stems: ["python", "program", "tutori", "begin", "learn"]
# "programming", "programmer", "programs" â†’ todos son "program"
```

### 4. Chatbots

```python
import spacy

nlp = spacy.load("en_core_web_sm")

# Base de conocimiento con variaciones
faq = {
    "reset password": "To reset your password, go to...",
    "change password": "To change your password, go to...",
    "update password": "To update your password, go to...",
}

def find_answer(user_query):
    # Lemmatizar query del usuario
    doc_query = nlp(user_query.lower())
    query_lemmas = {token.lemma_ for token in doc_query if not token.is_stop}
    
    # Buscar mejor match
    best_match = None
    best_score = 0
    
    for faq_key, answer in faq.items():
        doc_faq = nlp(faq_key)
        faq_lemmas = {token.lemma_ for token in doc_faq}
        
        # Similitud simple: intersecciÃ³n de lemmas
        score = len(query_lemmas & faq_lemmas)
        
        if score > best_score:
            best_score = score
            best_match = answer
    
    return best_match if best_score > 0 else "I don't understand"

# Usuario puede preguntar de diferentes formas
print(find_answer("How do I reset my password?"))
# â†’ "To reset your password, go to..."

print(find_answer("I want to change my password"))
# â†’ "To change your password, go to..."

print(find_answer("updating password"))
# â†’ "To update your password, go to..."
```

### 5. ReducciÃ³n de Features para ML

```python
from collections import Counter
from nltk.stem import PorterStemmer

stemmer = PorterStemmer()

corpus = [
    "machine learning is great",
    "learning machines are smart",
    "I love machine learning"
]

# Sin stemming
words_no_stem = []
for doc in corpus:
    words_no_stem.extend(doc.lower().split())

vocab_no_stem = Counter(words_no_stem)
print(f"Vocabulario sin stemming: {len(vocab_no_stem)} palabras")
print(vocab_no_stem)
# Vocabulario sin stemming: 9 palabras
# {'machine': 2, 'learning': 3, 'is': 1, 'great': 1, 
#  'machines': 1, 'are': 1, 'smart': 1, 'i': 1, 'love': 1}

# Con stemming
words_stem = []
for doc in corpus:
    tokens = doc.lower().split()
    words_stem.extend([stemmer.stem(t) for t in tokens])

vocab_stem = Counter(words_stem)
print(f"\nVocabulario con stemming: {len(vocab_stem)} palabras")
print(vocab_stem)
# Vocabulario con stemming: 7 palabras
# {'machin': 3, 'learn': 3, 'is': 1, 'great': 1, 
#  'are': 1, 'smart': 1, 'i': 1, 'love': 1}
```

---

## ğŸ“Š Best Practices

### 1. Elegir la Herramienta Correcta

```python
# Para bÃºsqueda simple y velocidad
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()  # âœ…

# Para anÃ¡lisis semÃ¡ntico preciso
import spacy
nlp = spacy.load("en_core_web_sm")  # âœ…

# Para multilingÃ¼e
from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer("spanish")  # âœ…
```

### 2. Consistencia

```python
# âœ… Usar la misma tÃ©cnica en todo el pipeline
def preprocess(text, method='lemma'):
    if method == 'stem':
        stemmer = PorterStemmer()
        tokens = text.split()
        return [stemmer.stem(t) for t in tokens]
    elif method == 'lemma':
        doc = nlp(text)
        return [token.lemma_ for token in doc]

# Aplicar consistentemente
train_processed = [preprocess(text, 'lemma') for text in train]
test_processed = [preprocess(text, 'lemma') for text in test]
```

### 3. Combinar con Otras TÃ©cnicas

```python
import spacy
from nltk.corpus import stopwords

nlp = spacy.load("en_core_web_sm")
stop_words = set(stopwords.words('english'))

def advanced_preprocess(text):
    # 1. Lowercase
    text = text.lower()
    
    # 2. Tokenizar y lemmatizar
    doc = nlp(text)
    
    # 3. Filtrar
    tokens = [
        token.lemma_ 
        for token in doc 
        if token.is_alpha  # Solo palabras
        and not token.is_stop  # Sin stopwords
        and len(token) > 2  # Longitud mÃ­nima
    ]
    
    return tokens

text = "The running dogs are jumping over the fence"
print(advanced_preprocess(text))
# ['run', 'dog', 'jump', 'fence']
```

### 4. Manejo de Excepciones

```python
import spacy

nlp = spacy.load("en_core_web_sm")

# Palabras especiales que no deben lemmatizarse
special_words = {"COVID-19", "iPhone", "NASA"}

def lemmatize_with_exceptions(text, exceptions=special_words):
    doc = nlp(text)
    
    lemmas = []
    for token in doc:
        if token.text in exceptions:
            lemmas.append(token.text)  # Mantener original
        else:
            lemmas.append(token.lemma_)
    
    return lemmas

text = "NASA announced iPhone support for COVID-19 tracking"
print(lemmatize_with_exceptions(text))
# ['NASA', 'announce', 'iPhone', 'support', 'for', 'COVID-19', 'track']
```

---

## ğŸ“ Resumen

**Conceptos Clave:**
- **Stemming**: ReducciÃ³n a raÃ­z mediante reglas (rÃ¡pido, aproximado)
- **Lemmatization**: ReducciÃ³n a lema mediante anÃ¡lisis (preciso, lento)
- Stemming para velocidad, lemmatization para precisiÃ³n
- spaCy es la mejor opciÃ³n para producciÃ³n

**Algoritmos Principales:**
- **Porter**: Balance, mÃ¡s usado
- **Lancaster**: MÃ¡s agresivo
- **Snowball**: Porter mejorado, multilingÃ¼e
- **WordNet**: Lemmatization con diccionario

**Decisiones Importantes:**
1. Â¿Velocidad o precisiÃ³n?
2. Â¿Palabras reales importan?
3. Â¿MultilingÃ¼e?
4. Â¿Integrar con POS tagging?

**PrÃ³ximos Pasos:**
- **Koan 3**: POS Tagging (necesario para lemmatization Ã³ptima)
- **Koan 5**: Text Classification (usando normalizaciÃ³n)
- **Koan 6**: Sentiment Analysis (beneficiÃ¡ndose de normalizaciÃ³n)

Â¡La normalizaciÃ³n mejora todos los pipelines de NLP! ğŸš€
