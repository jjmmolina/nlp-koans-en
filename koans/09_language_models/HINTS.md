# üí° Pistas para Koan 09: Language Models

## üéØ Objetivo del Koan

Aprender sobre **modelos de lenguaje modernos**:
- Generaci√≥n de texto con control
- Prompting efectivo
- Evaluaci√≥n de modelos
- Uso de LLMs para tareas complejas

---

## üìù Funci√≥n 1: `calculate_perplexity()`

### Nivel 1: Concepto
Perplexity mide qu√© tan "sorprendido" est√° un modelo por una secuencia. Menor perplexity = mejor modelo.

### Nivel 2: F√≥rmula
```
Perplexity = exp(average_negative_log_likelihood)
```

### Nivel 3: Implementaci√≥n
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import numpy as np

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

inputs = tokenizer(text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs, labels=inputs["input_ids"])
    loss = outputs.loss

perplexity = torch.exp(loss).item()
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def calculate_perplexity(text: str, model_name: str = "gpt2") -> float:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # Tokeniza
    inputs = tokenizer(text, return_tensors="pt")
    
    # Calcula loss (cross-entropy)
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
    
    # Perplexity = exp(loss)
    perplexity = torch.exp(loss).item()
    
    return perplexity
```
</details>

---

## üìù Funci√≥n 2: `generate_with_parameters()`

### Nivel 1: Concepto
Controla la generaci√≥n de texto con par√°metros como temperatura, top_k, top_p.

### Nivel 2: Par√°metros clave
```python
temperature: 0.1-2.0 (m√°s alto = m√°s creativo/aleatorio)
top_k: Considera solo top K tokens (ej: 50)
top_p: Nucleus sampling, considera tokens hasta probabilidad p (ej: 0.9)
num_return_sequences: Cu√°ntas versiones generar
```

### Nivel 3: Implementaci√≥n
```python
from transformers import pipeline

generator = pipeline("text-generation", model=model_name)
results = generator(
    prompt,
    max_length=max_length,
    temperature=temperature,
    top_k=top_k,
    top_p=top_p,
    num_return_sequences=num_sequences
)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def generate_with_parameters(prompt: str,
                              model_name: str = "gpt2",
                              max_length: int = 50,
                              temperature: float = 1.0,
                              top_k: int = 50,
                              top_p: float = 0.95,
                              num_sequences: int = 1) -> List[str]:
    from transformers import pipeline
    
    generator = pipeline("text-generation", model=model_name)
    
    results = generator(
        prompt,
        max_length=max_length,
        temperature=temperature,
        top_k=top_k,
        top_p=top_p,
        num_return_sequences=num_sequences,
        do_sample=True
    )
    
    return [result["generated_text"] for result in results]
```
</details>

---

## üìù Funci√≥n 3: `prompt_engineering()`

### Nivel 1: Concepto
Dise√±a prompts efectivos para obtener respuestas espec√≠ficas de LLMs.

### Nivel 2: Estrategias
```python
# Few-shot learning: dar ejemplos
prompt = """
Traduce al espa√±ol:
English: Hello
Spanish: Hola
English: Thank you
Spanish: Gracias
English: Goodbye
Spanish:"""

# Zero-shot: instrucci√≥n directa
prompt = "Traduce 'Goodbye' al espa√±ol:"
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def prompt_engineering(task: str, 
                       examples: List[Tuple[str, str]] = None,
                       query: str = "",
                       model_name: str = "gpt2") -> str:
    from transformers import pipeline
    
    # Construye prompt
    if examples:
        # Few-shot learning
        prompt = f"{task}:\n\n"
        for input_ex, output_ex in examples:
            prompt += f"Input: {input_ex}\nOutput: {output_ex}\n\n"
        prompt += f"Input: {query}\nOutput:"
    else:
        # Zero-shot
        prompt = f"{task}: {query}"
    
    # Genera respuesta
    generator = pipeline("text-generation", model=model_name)
    result = generator(prompt, max_length=len(prompt.split()) + 50)[0]["generated_text"]
    
    # Extrae solo la respuesta nueva
    response = result[len(prompt):].strip()
    
    return response
```
</details>

---

## üìù Funci√≥n 4: `get_token_probabilities()`

### Nivel 1: Concepto
Obtiene las probabilidades de los siguientes tokens seg√∫n el modelo.

### Nivel 2: Uso
```python
text = "The capital of France is"
# Modelo da alta probabilidad a "Paris", "Paris.", etc.
```

### Nivel 3: Implementaci√≥n
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

inputs = tokenizer(text, return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits[0, -1, :]  # √öltimo token
    probs = torch.softmax(logits, dim=-1)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def get_token_probabilities(text: str, 
                             model_name: str = "gpt2",
                             top_k: int = 5) -> List[Tuple[str, float]]:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # Tokeniza
    inputs = tokenizer(text, return_tensors="pt")
    
    # Obtiene logits
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits[0, -1, :]  # Logits del √∫ltimo token
    
    # Convierte a probabilidades
    probs = torch.softmax(logits, dim=-1)
    
    # Obtiene top_k tokens
    top_probs, top_indices = torch.topk(probs, top_k)
    
    # Convierte √≠ndices a tokens
    results = []
    for prob, idx in zip(top_probs, top_indices):
        token = tokenizer.decode([idx])
        results.append((token, prob.item()))
    
    return results
```
</details>

---

## üìù Funci√≥n 5: `conditional_generation()`

### Nivel 1: Concepto
Genera texto que cumple ciertas condiciones (longitud, estilo, keywords).

### Nivel 2: Estrategias
```python
# Control de longitud
min_length, max_length

# Control de contenido
# Penaliza repeticiones con repetition_penalty
# Evita tokens con bad_words_ids
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def conditional_generation(prompt: str,
                           model_name: str = "gpt2",
                           min_length: int = 20,
                           max_length: int = 100,
                           no_repeat_ngram_size: int = 3,
                           repetition_penalty: float = 1.2) -> str:
    from transformers import AutoTokenizer, AutoModelForCausalLM
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    # Tokeniza prompt
    inputs = tokenizer(prompt, return_tensors="pt")
    
    # Genera con restricciones
    outputs = model.generate(
        **inputs,
        min_length=min_length,
        max_length=max_length,
        no_repeat_ngram_size=no_repeat_ngram_size,
        repetition_penalty=repetition_penalty,
        do_sample=True,
        top_p=0.92,
        top_k=50
    )
    
    # Decodifica
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text
```
</details>

---

## üìù Funci√≥n 6: `compare_model_outputs()`

### Nivel 1: Concepto
Compara c√≥mo diferentes modelos responden al mismo prompt.

### Nivel 2: Modelos a comparar
```python
models = [
    "gpt2",           # 117M params
    "gpt2-medium",    # 345M params
    "gpt2-large",     # 774M params
    "distilgpt2"      # 82M params (m√°s r√°pido)
]
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def compare_model_outputs(prompt: str, 
                          model_names: List[str],
                          max_length: int = 50) -> dict:
    from transformers import pipeline
    
    results = {}
    
    for model_name in model_names:
        try:
            generator = pipeline("text-generation", model=model_name)
            output = generator(
                prompt,
                max_length=max_length,
                num_return_sequences=1,
                do_sample=True
            )[0]["generated_text"]
            
            results[model_name] = output
        except Exception as e:
            results[model_name] = f"Error: {str(e)}"
    
    return results
```
</details>

---

## üìù Funci√≥n 7: `evaluate_generation_quality()`

### Nivel 1: Concepto
Eval√∫a la calidad del texto generado usando m√©tricas autom√°ticas.

### Nivel 2: M√©tricas
```python
# Perplexity: qu√© tan "natural" es el texto
# Diversity: cu√°ntos tokens √∫nicos
# Repetition: cu√°ntas n-gramas repetidas
# Coherence: similitud sem√°ntica entre oraciones
```

### Nivel 3: Implementaci√≥n b√°sica
```python
import numpy as np

tokens = text.split()
unique_tokens = len(set(tokens))
total_tokens = len(tokens)

diversity = unique_tokens / total_tokens
perplexity = calculate_perplexity(text, model_name)
```

### ‚úÖ Soluci√≥n
<details>
<summary>Click para ver</summary>

```python
def evaluate_generation_quality(text: str, 
                                 model_name: str = "gpt2") -> dict:
    import numpy as np
    from collections import Counter
    
    # Tokeniza
    tokens = text.split()
    
    # Diversity: ratio de tokens √∫nicos
    unique_tokens = len(set(tokens))
    total_tokens = len(tokens)
    diversity = unique_tokens / total_tokens if total_tokens > 0 else 0
    
    # Repetition: bigrams repetidos
    bigrams = [tuple(tokens[i:i+2]) for i in range(len(tokens)-1)]
    bigram_counts = Counter(bigrams)
    repeated_bigrams = sum(1 for count in bigram_counts.values() if count > 1)
    repetition_ratio = repeated_bigrams / len(bigrams) if bigrams else 0
    
    # Perplexity
    try:
        perplexity = calculate_perplexity(text, model_name)
    except:
        perplexity = None
    
    return {
        "total_tokens": total_tokens,
        "unique_tokens": unique_tokens,
        "diversity": diversity,
        "repetition_ratio": repetition_ratio,
        "perplexity": perplexity
    }
```
</details>

---

## üéØ Conceptos Clave

### Language Models (LMs)

**Definici√≥n**: Modelos que predicen la probabilidad de secuencias de palabras.

```
P("The cat sat on the mat")
P("on mat sat The cat the") # Baja probabilidad
```

### Tipos de Language Models

| Tipo | Arquitectura | Ejemplo | Uso |
|------|--------------|---------|-----|
| **Causal LM** | Decoder | GPT-2/3 | Generaci√≥n de texto |
| **Masked LM** | Encoder | BERT | Clasificaci√≥n, NER |
| **Seq2Seq** | Enc-Dec | T5, BART | Traducci√≥n, resumen |

### Par√°metros de Generaci√≥n

#### Temperature (0.1 - 2.0)
```python
temperature = 0.1  # Conservador, predecible
temperature = 1.0  # Balanceado (default)
temperature = 2.0  # Creativo, aleatorio
```

#### Top-k Sampling
```python
top_k = 1   # Greedy (siempre el m√°s probable)
top_k = 50  # Considera 50 tokens m√°s probables
```

#### Top-p (Nucleus Sampling)
```python
top_p = 0.9  # Considera tokens hasta 90% probabilidad acumulada
top_p = 0.5  # M√°s conservador
```

### Prompt Engineering

**Zero-Shot**:
```python
"Traduce al espa√±ol: Hello"
```

**Few-Shot**:
```python
"""
Traduce al espa√±ol:
English: Hello ‚Üí Spanish: Hola
English: Bye ‚Üí Spanish: Adi√≥s
English: Thanks ‚Üí Spanish:
"""
```

**Chain-of-Thought**:
```python
"""
P: ¬øCu√°nto es 15 + 27?
R: Primero sumo 15 + 20 = 35
   Luego sumo 35 + 7 = 42
   Respuesta: 42
"""
```

## üí° Tips Pr√°cticos

### 1. Ajusta temperatura seg√∫n necesidad

```python
# Factual/Preciso (c√≥digo, matem√°ticas)
temperature = 0.3

# Creativo (historias, poes√≠a)
temperature = 1.5
```

### 2. Combina top_k y top_p

```python
# Buena configuraci√≥n general
generate_with_parameters(
    prompt,
    temperature=0.8,
    top_k=50,
    top_p=0.95
)
```

### 3. Evita repeticiones

```python
conditional_generation(
    prompt,
    no_repeat_ngram_size=3,      # No repite 3-gramas
    repetition_penalty=1.2       # Penaliza tokens repetidos
)
```

### 4. Controla longitud efectivamente

```python
# M√≠nimo garantizado
min_length = 50

# M√°ximo absoluto
max_length = 200

# O usa early stopping
generate(..., early_stopping=True)
```

## üöÄ Casos de Uso

### Autocompletar c√≥digo
```python
prompt = "def fibonacci(n):\n    if n <= 1:\n        return n\n    else:"
code = generate_with_parameters(
    prompt, 
    model_name="gpt2",
    temperature=0.3,  # Conservador para c√≥digo
    max_length=100
)
```

### Generaci√≥n creativa
```python
prompt = "Once upon a time in a distant galaxy"
stories = generate_with_parameters(
    prompt,
    temperature=1.2,  # Creativo
    num_sequences=3   # 3 versiones
)
```

### Clasificaci√≥n con prompts
```python
prompt = """
Clasifica el sentimiento:
"Me encanta este producto" -> Positivo
"Es horrible" -> Negativo
"No est√° mal" ->"""

result = prompt_engineering(
    task="Sentiment classification",
    examples=[
        ("Me encanta", "Positivo"),
        ("Es horrible", "Negativo")
    ],
    query="No est√° mal"
)
```

### Generaci√≥n de res√∫menes
```python
article = "Long article text here..."
prompt = f"Resumir en una oraci√≥n:\n{article}\n\nResumen:"

summary = conditional_generation(
    prompt,
    min_length=10,
    max_length=50
)
```

## üîß Troubleshooting

### Problema: Texto muy repetitivo
**Soluci√≥n**:
```python
conditional_generation(
    prompt,
    no_repeat_ngram_size=3,
    repetition_penalty=1.5
)
```

### Problema: Generaci√≥n sin sentido
**Soluci√≥n**:
```python
# Reduce temperatura
generate_with_parameters(prompt, temperature=0.7)

# O usa top_p m√°s bajo
generate_with_parameters(prompt, top_p=0.8)
```

### Problema: Muy lento
**Soluci√≥n**:
```python
# Usa modelo m√°s peque√±o
model_name = "distilgpt2"  # vs "gpt2-large"

# O reduce max_length
max_length = 50  # vs 200
```

### Problema: Perplexity muy alto
**Soluci√≥n**: 
- El texto no es natural
- Est√° en idioma diferente al del modelo
- Contiene caracteres especiales/raros

## üìö Recursos Adicionales

### Modelos recomendados

**Para espa√±ol**:
```python
"mrm8488/GPT-2-finetuned-SQUAD-spanish"
"DeepESP/gpt2-spanish"
```

**Para ingl√©s**:
```python
"gpt2"           # Base (117M)
"gpt2-medium"    # Mediano (345M)
"gpt2-large"     # Grande (774M)
"distilgpt2"     # R√°pido (82M)
```

### Papers importantes
- "Attention Is All You Need" (Transformers)
- "Language Models are Few-Shot Learners" (GPT-3)
- "BERT: Pre-training of Deep Bidirectional Transformers"

## üéâ ¬°Felicidades!

Has completado todos los **9 Koans de NLP**! üéä

### Lo que has aprendido:
‚úÖ Tokenizaci√≥n (NLTK, spaCy)
‚úÖ Stemming y Lemmatization
‚úÖ POS Tagging
‚úÖ Named Entity Recognition
‚úÖ Text Classification (TF-IDF, ML)
‚úÖ Sentiment Analysis (TextBlob, Transformers)
‚úÖ Word Embeddings (Word2Vec, spaCy)
‚úÖ Transformers (BERT, GPT, pipelines)
‚úÖ Language Models (generaci√≥n, prompting)

### Pr√≥ximos pasos:
1. üîß **Proyectos propios**: Aplica lo aprendido
2. üìö **Papers**: Lee investigaci√≥n reciente
3. üöÄ **Fine-tuning**: Entrena modelos con tus datos
4. ü§ù **Contribuye**: Comparte en GitHub
5. üéì **Especial√≠zate**: Elige un √°rea (NER, QA, etc.)

### Recursos para continuar:
- Hugging Face Course: https://huggingface.co/course
- Fast.ai NLP: https://www.fast.ai/
- Papers With Code: https://paperswithcode.com/area/natural-language-processing
- r/LanguageTechnology en Reddit

**¬°Mucha suerte en tu viaje de NLP!** üöÄ
