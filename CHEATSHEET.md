# ðŸ“š NLP Koans - Cheat Sheet

Referencia rÃ¡pida de conceptos y cÃ³digo aprendido en cada koan.

---

## ðŸ”¹ Koan 01: TokenizaciÃ³n

### Conceptos Clave
- **TokenizaciÃ³n**: Dividir texto en unidades (tokens)
- **Tipos**: Palabras, oraciones, caracteres
- **Herramientas**: NLTK (clÃ¡sico), spaCy (industrial)

### CÃ³digo Esencial

```python
# NLTK - TokenizaciÃ³n de palabras
from nltk.tokenize import word_tokenize
tokens = word_tokenize("Hola, Â¿cÃ³mo estÃ¡s?")
# ['Hola', ',', 'Â¿cÃ³mo', 'estÃ¡s', '?']

# NLTK - TokenizaciÃ³n de oraciones
from nltk.tokenize import sent_tokenize
sentences = sent_tokenize("Hola. Â¿CÃ³mo estÃ¡s? Bien.")
# ['Hola.', 'Â¿CÃ³mo estÃ¡s?', 'Bien.']

# spaCy - TokenizaciÃ³n avanzada
import spacy
nlp = spacy.load("es_core_news_sm")
doc = nlp("El Dr. GarcÃ­a ganÃ³ $1,000.")
tokens = [token.text for token in doc]

# TokenizaciÃ³n personalizada
tokens = text.split("-")  # Delimitador personalizado

# Contar frecuencias
from collections import Counter
tokens = word_tokenize(text.lower())
frecuencias = Counter(tokens)

# Eliminar puntuaciÃ³n
import string
tokens_limpios = [t for t in tokens if t not in string.punctuation]
```

### ðŸ’¡ Tips
- spaCy maneja mejor abreviaturas y nÃºmeros
- NLTK es mÃ¡s rÃ¡pido para tareas simples
- Siempre normaliza (lowercase) antes de contar

---

## ðŸ”¹ Koan 02: Stemming y Lemmatization

### Conceptos Clave
- **Stemming**: Corta sufijos (rÃ¡pido, aproximado)
- **Lemmatization**: Forma canÃ³nica (preciso, usa diccionario)
- **CuÃ¡ndo usar**: Stemming para IR, Lemmatization para anÃ¡lisis

### CÃ³digo Esencial

```python
# Porter Stemmer (solo inglÃ©s)
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()
stem = stemmer.stem("running")  # "run"

# Snowball Stemmer (multiidioma)
from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer("spanish")
stem = stemmer.stem("corriendo")  # "corr"

# WordNet Lemmatizer (inglÃ©s)
from nltk.stem import WordNetLemmatizer
lemmatizer = WordNetLemmatizer()
lemma = lemmatizer.lemmatize("running", pos="v")  # "run"

# spaCy Lemmatization (automÃ¡tico)
import spacy
nlp = spacy.load("es_core_news_sm")
doc = nlp("Los gatos corrÃ­an")
lemmas = [token.lemma_ for token in doc]
# ['el', 'gato', 'correr']
```

### ComparaciÃ³n

| Aspecto | Stemming | Lemmatization |
|---------|----------|---------------|
| Velocidad | RÃ¡pido âš¡ | Lento ðŸ¢ |
| PrecisiÃ³n | Aproximado | Preciso âœ“ |
| Resultado | Puede no ser palabra | Siempre palabra vÃ¡lida |
| Ejemplo | "corriendo" â†’ "corr" | "corriendo" â†’ "correr" |

### ðŸ’¡ Tips
- Para espaÃ±ol: usa Snowball o spaCy
- POS tag mejora lemmatization (n, v, a, r)
- spaCy hace lemmatization gratis al procesar

---

## ðŸ”¹ Koan 03: POS Tagging

### Conceptos Clave
- **POS**: Part of Speech (categorÃ­a gramatical)
- **Universal Dependencies**: 17 etiquetas estÃ¡ndar
- **Uso**: Filtrar palabras, anÃ¡lisis sintÃ¡ctico

### CÃ³digo Esencial

```python
# NLTK POS Tagging (inglÃ©s, Penn Treebank)
from nltk import pos_tag, word_tokenize
tokens = word_tokenize("The cat sits")
tags = pos_tag(tokens)
# [('The', 'DT'), ('cat', 'NN'), ('sits', 'VBZ')]

# spaCy POS Tagging (Universal Dependencies)
import spacy
nlp = spacy.load("es_core_news_sm")
doc = nlp("El gato grande")
for token in doc:
    print(f"{token.text}: {token.pos_} ({token.tag_})")

# Extraer por categorÃ­a
sustantivos = [token.text for token in doc if token.pos_ == "NOUN"]
verbos = [token.text for token in doc if token.pos_ == "VERB"]
adjetivos = [token.text for token in doc if token.pos_ == "ADJ"]

# Contar categorÃ­as
from collections import Counter
pos_counts = Counter([token.pos_ for token in doc])
```

### Etiquetas Universales Principales

| Etiqueta | Tipo | Ejemplo |
|----------|------|---------|
| NOUN | Sustantivo | casa, perro |
| VERB | Verbo | correr, comer |
| ADJ | Adjetivo | grande, azul |
| ADV | Adverbio | rÃ¡pidamente |
| PRON | Pronombre | Ã©l, ella |
| DET | Determinante | el, la, un |
| ADP | PreposiciÃ³n | de, en, por |
| PUNCT | PuntuaciÃ³n | . , ; |

### ðŸ’¡ Tips
- spaCy es mejor para producciÃ³n
- Usa `.pos_` para universal, `.tag_` para especÃ­fico
- Filtra por mÃºltiples POS para anÃ¡lisis complejo

---

## ðŸ”¹ Koan 04: Named Entity Recognition (NER)

### Conceptos Clave
- **NER**: Identificar entidades nombradas
- **Tipos**: Personas, organizaciones, lugares, fechas
- **Uso**: ExtracciÃ³n de informaciÃ³n, anonimizaciÃ³n

### CÃ³digo Esencial

```python
# spaCy NER (mejor opciÃ³n)
import spacy
nlp = spacy.load("es_core_news_sm")
doc = nlp("MarÃ­a GarcÃ­a trabaja en Google en Madrid")

# Extraer todas las entidades
entities = [(ent.text, ent.label_) for ent in doc.ents]
# [('MarÃ­a GarcÃ­a', 'PER'), ('Google', 'ORG'), ('Madrid', 'LOC')]

# Filtrar por tipo
personas = [ent.text for ent in doc.ents if ent.label_ == "PER"]
lugares = [ent.text for ent in doc.ents if ent.label_ == "LOC"]
organizaciones = [ent.text for ent in doc.ents if ent.label_ == "ORG"]

# Con contexto (posiciÃ³n)
for ent in doc.ents:
    print(f"{ent.text} ({ent.label_}) en posiciÃ³n {ent.start_char}-{ent.end_char}")

# Contar tipos
from collections import Counter
entity_counts = Counter([ent.label_ for ent in doc.ents])
```

### Tipos de Entidades

**EspaÃ±ol (spaCy)**:
- `PER`: Persona
- `LOC`: Lugar
- `ORG`: OrganizaciÃ³n
- `MISC`: MiscelÃ¡neo

**InglÃ©s (spaCy)**:
- `PERSON`: Persona
- `GPE`: Entidad geopolÃ­tica
- `ORG`: OrganizaciÃ³n
- `DATE`: Fecha
- `MONEY`: Dinero

### ðŸ’¡ Tips
- MayÃºsculas mejoran detecciÃ³n
- Modelos grandes (md, lg) son mÃ¡s precisos
- Usa contexto para desambiguar

---

## ðŸ”¹ Koan 05: Text Classification

### Conceptos Clave
- **Pipeline**: VectorizaciÃ³n â†’ Modelo ML â†’ PredicciÃ³n
- **TF-IDF**: Frecuencia de tÃ©rminos con peso por rareza
- **Clasificadores**: Naive Bayes, Logistic Regression, SVM

### CÃ³digo Esencial

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# 1. VectorizaciÃ³n TF-IDF
vectorizer = TfidfVectorizer(max_features=1000)
X_train = vectorizer.fit_transform(train_texts)
X_test = vectorizer.transform(test_texts)

# 2. Entrenar clasificador
classifier = MultinomialNB()
classifier.fit(X_train, y_train)

# 3. Predecir
predictions = classifier.predict(X_test)

# 4. Evaluar
accuracy = accuracy_score(y_test, predictions)
report = classification_report(y_test, predictions)

# Pipeline (recomendado)
from sklearn.pipeline import Pipeline

pipeline = Pipeline([
    ('vectorizer', TfidfVectorizer(max_features=1000)),
    ('classifier', MultinomialNB())
])
pipeline.fit(train_texts, y_train)
predictions = pipeline.predict(test_texts)

# Feature importance
feature_names = vectorizer.get_feature_names_out()
coef = classifier.coef_[0]
top_features = sorted(zip(feature_names, coef), key=lambda x: x[1], reverse=True)[:10]
```

### MÃ©tricas de EvaluaciÃ³n

| MÃ©trica | Significado |
|---------|-------------|
| **Accuracy** | % predicciones correctas |
| **Precision** | De los positivos predichos, % correctos |
| **Recall** | De los positivos reales, % encontrados |
| **F1-Score** | Media armÃ³nica de precision y recall |

### ðŸ’¡ Tips
- Usa Pipeline para evitar errores
- MÃ¡s features â‰  mejor (prueba 100-10000)
- Logistic Regression es interpretable

---

## ðŸ”¹ Koan 06: Sentiment Analysis

### Conceptos Clave
- **MÃ©todos**: LÃ©xico (TextBlob), ML, Transformers
- **Polaridad**: -1 (negativo) a +1 (positivo)
- **Subjetividad**: 0 (objetivo) a 1 (subjetivo)

### CÃ³digo Esencial

```python
# TextBlob (reglas, rÃ¡pido)
from textblob import TextBlob
blob = TextBlob("This is amazing!")
polarity = blob.sentiment.polarity  # 0.0 a 1.0
subjectivity = blob.sentiment.subjectivity

# Transformers (mejor precisiÃ³n)
from transformers import pipeline

# InglÃ©s
classifier = pipeline("sentiment-analysis", 
    model="distilbert-base-uncased-finetuned-sst-2-english")
result = classifier("I love this product!")
# [{'label': 'POSITIVE', 'score': 0.9998}]

# EspaÃ±ol
classifier = pipeline("sentiment-analysis",
    model="pysentimiento/robertuito-sentiment-analysis")
result = classifier("Me encanta este producto!")

# Batch processing (mÃ¡s eficiente)
results = classifier(["Text 1", "Text 2", "Text 3"])

# Clasificar polaridad
def classify_sentiment(polarity, threshold=0.1):
    if polarity > threshold:
        return "positive"
    elif polarity < -threshold:
        return "negative"
    return "neutral"
```

### Modelos Recomendados

**InglÃ©s**:
- `distilbert-base-uncased-finetuned-sst-2-english` (rÃ¡pido)
- `cardiffnlp/twitter-roberta-base-sentiment-latest` (tweets)

**EspaÃ±ol**:
- `pysentimiento/robertuito-sentiment-analysis`
- `cardiffnlp/twitter-xlm-roberta-base-sentiment`

### ðŸ’¡ Tips
- TextBlob para prototipos rÃ¡pidos
- Transformers para producciÃ³n
- Batch processing es 10x mÃ¡s rÃ¡pido
- Sarcasmo es difÃ­cil de detectar

---

## ðŸ”¹ Koan 07: Word Embeddings

### Conceptos Clave
- **Embeddings**: RepresentaciÃ³n densa de palabras como vectores
- **Similitud**: Palabras similares â†’ vectores cercanos
- **AritmÃ©tica**: vector("rey") - vector("hombre") + vector("mujer") â‰ˆ vector("reina")

### CÃ³digo Esencial

```python
# spaCy Word Vectors (necesita md o lg)
import spacy
import numpy as np

nlp = spacy.load("en_core_web_md")  # o es_core_news_md
doc = nlp("cat")
vector = doc.vector  # numpy array (300 dims)

# Similitud coseno
def cosine_similarity(vec1, vec2):
    dot = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot / (norm1 * norm2)

# Similitud entre palabras
doc1 = nlp("cat")
doc2 = nlp("dog")
similarity = doc1.similarity(doc2)  # 0.8

# Word2Vec (gensim)
from gensim.models import KeyedVectors

# Cargar modelo pre-entrenado
model = KeyedVectors.load_word2vec_format('GoogleNews-vectors.bin', binary=True)

# Similitud
similarity = model.similarity('king', 'queen')

# Palabras mÃ¡s similares
similar = model.most_similar('king', topn=5)

# AnalogÃ­as
result = model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)
# ['queen']

# Sentence embedding (promedio)
doc = nlp("The cat sits on the mat")
sentence_vector = doc.vector  # spaCy promedia automÃ¡ticamente
```

### Propiedades Importantes

1. **Similitud SemÃ¡ntica**: Palabras relacionadas tienen vectores cercanos
2. **AritmÃ©tica Vectorial**: Captura relaciones (gÃ©nero, geografÃ­a, tiempo)
3. **Contextual vs EstÃ¡tico**:
   - Word2Vec: mismo vector siempre
   - BERT: vector cambia segÃºn contexto

### ðŸ’¡ Tips
- Modelos `sm` NO tienen vectores
- Usa `md` o `lg` para embeddings
- Normaliza palabras (lowercase)
- Word2Vec es estÃ¡tico, BERT es contextual

---

## ðŸ”¹ Koan 08: Transformers

### Conceptos Clave
- **Transformers**: Arquitectura con self-attention
- **Modelos**: BERT (encoder), GPT (decoder), T5 (seq2seq)
- **Pipelines**: API simple de Hugging Face

### CÃ³digo Esencial

```python
from transformers import pipeline

# 1. GeneraciÃ³n de texto (GPT-2)
generator = pipeline("text-generation", model="gpt2")
text = generator("Once upon a time", max_length=50)[0]["generated_text"]

# 2. Fill-mask (BERT)
unmasker = pipeline("fill-mask", model="bert-base-uncased")
results = unmasker("Paris is the [MASK] of France.")
# [{'token_str': 'capital', 'score': 0.999}]

# 3. Question Answering
qa = pipeline("question-answering", model="distilbert-base-cased-distilled-squad")
result = qa(
    question="What is the capital of France?",
    context="Paris is the capital of France. It has 2.2M inhabitants."
)
# {'answer': 'Paris', 'score': 0.98}

# 4. Summarization
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
summary = summarizer(long_article, max_length=130, min_length=30)[0]["summary_text"]

# 5. Embeddings contextuales (BERT)
from transformers import AutoTokenizer, AutoModel
import torch

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModel.from_pretrained("bert-base-uncased")

inputs = tokenizer("Hello world", return_tensors="pt")
with torch.no_grad():
    outputs = model(**inputs)
embeddings = outputs.last_hidden_state.mean(dim=1)

# Control de generaciÃ³n
generator = pipeline("text-generation", model="gpt2")
text = generator(
    "The future of AI",
    max_length=100,
    temperature=0.7,    # Creatividad
    top_p=0.9,          # Nucleus sampling
    top_k=50,           # Top-K sampling
    do_sample=True
)
```

### Modelos Principales

| Modelo | Tipo | Mejor para |
|--------|------|------------|
| BERT | Encoder | ClasificaciÃ³n, NER, Q&A |
| GPT-2/3 | Decoder | GeneraciÃ³n de texto |
| T5 | Encoder-Decoder | TraducciÃ³n, resumen |
| BART | Encoder-Decoder | Resumen, parÃ¡frasis |
| DistilBERT | Encoder | BERT mÃ¡s rÃ¡pido (60%) |

### ðŸ’¡ Tips
- Usa modelos distilled para velocidad
- GPU acelera 10-100x
- `temperature=0.7` para texto balanceado
- BERT max 512 tokens

---

## ðŸ”¹ Koan 09: Language Models

### Conceptos Clave
- **Perplexity**: QuÃ© tan "sorprendido" estÃ¡ el modelo (menor = mejor)
- **Prompt Engineering**: DiseÃ±ar prompts efectivos
- **Control**: Temperature, top-k, top-p, repetition_penalty

### CÃ³digo Esencial

```python
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
import torch

# 1. Perplexity
def calculate_perplexity(text, model_name="gpt2"):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs, labels=inputs["input_ids"])
        loss = outputs.loss
    
    return torch.exp(loss).item()

# 2. GeneraciÃ³n controlada
generator = pipeline("text-generation", model="gpt2")

text = generator(
    "The future of AI",
    max_length=100,
    min_length=20,
    temperature=0.8,           # 0.1-2.0 (creatividad)
    top_k=50,                  # Top K tokens
    top_p=0.95,                # Nucleus sampling
    repetition_penalty=1.2,    # Penaliza repeticiÃ³n
    no_repeat_ngram_size=3,    # No repite 3-gramas
    num_return_sequences=3     # Genera 3 versiones
)

# 3. Prompt Engineering

# Zero-shot
prompt = "Translate to Spanish: Hello"
result = generator(prompt, max_length=50)

# Few-shot
prompt = """
Translate to Spanish:
English: Hello â†’ Spanish: Hola
English: Goodbye â†’ Spanish: AdiÃ³s
English: Thank you â†’ Spanish:
"""
result = generator(prompt, max_length=len(prompt.split()) + 10)

# 4. Token probabilities
def get_next_token_probs(text, model_name="gpt2", top_k=5):
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    
    inputs = tokenizer(text, return_tensors="pt")
    with torch.no_grad():
        outputs = model(**inputs)
        logits = outputs.logits[0, -1, :]
        probs = torch.softmax(logits, dim=-1)
    
    top_probs, top_indices = torch.topk(probs, top_k)
    results = [(tokenizer.decode([idx]), prob.item()) 
               for prob, idx in zip(top_probs, top_indices)]
    return results

# 5. EvaluaciÃ³n de calidad
def evaluate_generation(text):
    tokens = text.split()
    diversity = len(set(tokens)) / len(tokens)  # Ratio tokens Ãºnicos
    perplexity = calculate_perplexity(text)
    
    return {
        "diversity": diversity,
        "perplexity": perplexity,
        "length": len(tokens)
    }
```

### ParÃ¡metros de GeneraciÃ³n

| ParÃ¡metro | Rango | Efecto |
|-----------|-------|--------|
| `temperature` | 0.1-2.0 | Creatividad (â†‘ mÃ¡s aleatorio) |
| `top_k` | 1-100 | Considera top K tokens |
| `top_p` | 0.1-1.0 | Nucleus sampling |
| `repetition_penalty` | 1.0-2.0 | Penaliza repeticiÃ³n |
| `no_repeat_ngram_size` | 2-5 | Evita n-gramas repetidos |

### Estrategias de Prompting

1. **Zero-shot**: InstrucciÃ³n directa
2. **Few-shot**: Dar ejemplos
3. **Chain-of-thought**: Razonamiento paso a paso
4. **Instruction-following**: Instrucciones explÃ­citas

### ðŸ’¡ Tips
- Temperature bajo (0.3) para cÃ³digo/facts
- Temperature alto (1.5) para creatividad
- Few-shot mejora resultados dramÃ¡ticamente
- Perplexity < 50 = buen texto

---

## ðŸš€ Comandos Ãštiles

```bash
# Ejecutar tests
pytest koans/01_tokenization/test_tokenization.py -v
pytest koans/01_tokenization/test_tokenization.py::TestTokenizationBasics -v

# Verificar progreso
python check_progress.ps1  # Windows
./check_progress.sh        # Linux/Mac

# Instalar modelos spaCy
python -m spacy download es_core_news_sm
python -m spacy download en_core_web_sm
python -m spacy download en_core_web_md

# Descargar recursos NLTK
python -c "import nltk; nltk.download('punkt'); nltk.download('wordnet')"
```

---

## ðŸ“– Recursos Adicionales

- **Hugging Face**: https://huggingface.co/
- **spaCy Docs**: https://spacy.io/usage
- **NLTK Book**: https://www.nltk.org/book/
- **Papers With Code**: https://paperswithcode.com/area/natural-language-processing

---

**Â¡Consulta este cheat sheet siempre que necesites recordar algo rÃ¡pidamente!** ðŸŽ¯
